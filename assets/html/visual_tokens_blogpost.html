<!DOCTYPE html>
<!-- saved from url=(0046)http://127.0.0.1:4000/blog/2026/demystifying1/ -->
<html lang="en" data-theme-setting="dark" class="" data-theme="dark" style="--vsc-domain: &quot;127.0.0.1&quot;;"><!-- Head --><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><style>#back-to-top{background:#000;-webkit-border-radius:50%;-moz-border-radius:50%;border-radius:50%;bottom:20px;-webkit-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);-moz-box-shadow:0 2px 5px 0 rgba(0,0,0,.26);box-shadow:0 2px 5px 0 rgba(0,0,0,.26);color:#fff;cursor:pointer;display:block;height:56px;opacity:1;outline:0;position:fixed;right:20px;-webkit-tap-highlight-color:transparent;-webkit-touch-callout:none;-webkit-transition:bottom .2s,opacity .2s;-o-transition:bottom .2s,opacity .2s;-moz-transition:bottom .2s,opacity .2s;transition:bottom .2s,opacity .2s;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none;width:56px;z-index:1}#back-to-top svg{display:block;fill:currentColor;height:20px;margin:11px auto 0;width:20px}#back-to-top.hidden{bottom:-56px;opacity:0}</style>

    
    <!-- Metadata, OpenGraph and Schema.org -->




<!-- Standard metadata -->

<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<title>
  
  
    
      De-mystifying Multimodal Learning&lt;br&gt;&lt;small&gt;The Hidden Inefficiency in Vision Language Modelling | Matteo Nulli
    
  
&lt;/small&gt;
</title>
<meta name="author" content="Matteo Nulli">
<meta name="description" content="Discussing Vision-Language biggest performance bottleneck">

  <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">










<!-- Bootstrap & MDB -->
<link rel="stylesheet" href="./visual_tokens_blogpost_files/bootstrap.min.css">
<link rel="stylesheet" href="./visual_tokens_blogpost_files/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

<!-- Bootstrap Table -->


<!-- Fonts & Icons -->
<link defer="" rel="stylesheet" href="./visual_tokens_blogpost_files/academicons.min.css">
<link defer="" rel="stylesheet" type="text/css" href="./visual_tokens_blogpost_files/css">

<!-- Code Syntax Highlighting -->
<link defer="" rel="stylesheet" href="./visual_tokens_blogpost_files/jekyll-pygments-themes-github.css" media="none" id="highlight_theme_light">



<!-- Styles -->

<!-- pseudocode -->



  <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%80&lt;/text&gt;&lt;/svg&gt;">

<link rel="stylesheet" href="./visual_tokens_blogpost_files/main.css">
<link rel="canonical" href="http://localhost:4000/blog/2026/demystifying1/">

<!-- Dark Mode -->
<script src="./visual_tokens_blogpost_files/theme.js"></script>

  <link defer="" rel="stylesheet" href="./visual_tokens_blogpost_files/jekyll-pygments-themes-native.css" media="" id="highlight_theme_dark">
  <script>
    initTheme();
  </script>


<!-- GeoJSON support via Leaflet -->


<!-- diff2html -->






  <style type="text/css">/* Chart.js */
@-webkit-keyframes chartjs-render-animation{from{opacity:0.99}to{opacity:1}}@keyframes chartjs-render-animation{from{opacity:0.99}to{opacity:1}}.chartjs-render-monitor{-webkit-animation:chartjs-render-animation 0.001s;animation:chartjs-render-animation 0.001s;}</style><style type="text/css">.CtxtMenu_InfoClose {  top:.2em; right:.2em;}
.CtxtMenu_InfoContent {  overflow:auto; text-align:left; font-size:80%;  padding:.4em .6em; border:1px inset; margin:1em 0px;  max-height:20em; max-width:30em; background-color:#EEEEEE;  white-space:normal;}
.CtxtMenu_Info.CtxtMenu_MousePost {outline:none;}
.CtxtMenu_Info {  position:fixed; left:50%; width:auto; text-align:center;  border:3px outset; padding:1em 2em; background-color:#DDDDDD;  color:black;  cursor:default; font-family:message-box; font-size:120%;  font-style:normal; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 15px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius:15px;               /* Safari and Chrome */  -moz-border-radius:15px;                  /* Firefox */  -khtml-border-radius:15px;                /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */  filter:progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color="gray", Positive="true"); /* IE */}
</style><style type="text/css">.CtxtMenu_MenuClose {  position:absolute;  cursor:pointer;  display:inline-block;  border:2px solid #AAA;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  font-family: "Courier New", Courier;  font-size:24px;  color:#F0F0F0}
.CtxtMenu_MenuClose span {  display:block; background-color:#AAA; border:1.5px solid;  border-radius:18px;  -webkit-border-radius: 18px;             /* Safari and Chrome */  -moz-border-radius: 18px;                /* Firefox */  -khtml-border-radius: 18px;              /* Konqueror */  line-height:0;  padding:8px 0 6px     /* may need to be browser-specific */}
.CtxtMenu_MenuClose:hover {  color:white!important;  border:2px solid #CCC!important}
.CtxtMenu_MenuClose:hover span {  background-color:#CCC!important}
.CtxtMenu_MenuClose:hover:focus {  outline:none}
</style><style type="text/css">.CtxtMenu_Menu {  position:absolute;  background-color:white;  color:black;  width:auto; padding:5px 0px;  border:1px solid #CCCCCC; margin:0; cursor:default;  font: menu; text-align:left; text-indent:0; text-transform:none;  line-height:normal; letter-spacing:normal; word-spacing:normal;  word-wrap:normal; white-space:nowrap; float:none; z-index:201;  border-radius: 5px;                     /* Opera 10.5 and IE9 */  -webkit-border-radius: 5px;             /* Safari and Chrome */  -moz-border-radius: 5px;                /* Firefox */  -khtml-border-radius: 5px;              /* Konqueror */  box-shadow:0px 10px 20px #808080;         /* Opera 10.5 and IE9 */  -webkit-box-shadow:0px 10px 20px #808080; /* Safari 3 & Chrome */  -moz-box-shadow:0px 10px 20px #808080;    /* Forefox 3.5 */  -khtml-box-shadow:0px 10px 20px #808080;  /* Konqueror */}
.CtxtMenu_MenuItem {  padding: 1px 2em;  background:transparent;}
.CtxtMenu_MenuArrow {  position:absolute; right:.5em; padding-top:.25em; color:#666666;  font-family: null; font-size: .75em}
.CtxtMenu_MenuActive .CtxtMenu_MenuArrow {color:white}
.CtxtMenu_MenuArrow.CtxtMenu_RTL {left:.5em; right:auto}
.CtxtMenu_MenuCheck {  position:absolute; left:.7em;  font-family: null}
.CtxtMenu_MenuCheck.CtxtMenu_RTL { right:.7em; left:auto }
.CtxtMenu_MenuRadioCheck {  position:absolute; left: .7em;}
.CtxtMenu_MenuRadioCheck.CtxtMenu_RTL {  right: .7em; left:auto}
.CtxtMenu_MenuInputBox {  padding-left: 1em; right:.5em; color:#666666;  font-family: null;}
.CtxtMenu_MenuInputBox.CtxtMenu_RTL {  left: .1em;}
.CtxtMenu_MenuComboBox {  left:.1em; padding-bottom:.5em;}
.CtxtMenu_MenuSlider {  left: .1em;}
.CtxtMenu_SliderValue {  position:absolute; right:.1em; padding-top:.25em; color:#333333;  font-size: .75em}
.CtxtMenu_SliderBar {  outline: none; background: #d3d3d3}
.CtxtMenu_MenuLabel {  padding: 1px 2em 3px 1.33em;  font-style:italic}
.CtxtMenu_MenuRule {  border-top: 1px solid #DDDDDD;  margin: 4px 3px;}
.CtxtMenu_MenuDisabled {  color:GrayText}
.CtxtMenu_MenuActive {  background-color: #606872;  color: white;}
.CtxtMenu_MenuDisabled:focus {  background-color: #E8E8E8}
.CtxtMenu_MenuLabel:focus {  background-color: #E8E8E8}
.CtxtMenu_ContextMenu:focus {  outline:none}
.CtxtMenu_ContextMenu .CtxtMenu_MenuItem:focus {  outline:none}
.CtxtMenu_SelectionMenu {  position:relative; float:left;  border-bottom: none; -webkit-box-shadow:none; -webkit-border-radius:0px; }
.CtxtMenu_SelectionItem {  padding-right: 1em;}
.CtxtMenu_Selection {  right: 40%; width:50%; }
.CtxtMenu_SelectionBox {  padding: 0em; max-height:20em; max-width: none;  background-color:#FFFFFF;}
.CtxtMenu_SelectionDivider {  clear: both; border-top: 2px solid #000000;}
.CtxtMenu_Menu .CtxtMenu_MenuClose {  top:-10px; left:-10px}
</style><style id="MJX-CHTML-styles">
mjx-container[jax="CHTML"] {
  line-height: 0;
}

mjx-container [space="1"] {
  margin-left: .111em;
}

mjx-container [space="2"] {
  margin-left: .167em;
}

mjx-container [space="3"] {
  margin-left: .222em;
}

mjx-container [space="4"] {
  margin-left: .278em;
}

mjx-container [space="5"] {
  margin-left: .333em;
}

mjx-container [rspace="1"] {
  margin-right: .111em;
}

mjx-container [rspace="2"] {
  margin-right: .167em;
}

mjx-container [rspace="3"] {
  margin-right: .222em;
}

mjx-container [rspace="4"] {
  margin-right: .278em;
}

mjx-container [rspace="5"] {
  margin-right: .333em;
}

mjx-container [size="s"] {
  font-size: 70.7%;
}

mjx-container [size="ss"] {
  font-size: 50%;
}

mjx-container [size="Tn"] {
  font-size: 60%;
}

mjx-container [size="sm"] {
  font-size: 85%;
}

mjx-container [size="lg"] {
  font-size: 120%;
}

mjx-container [size="Lg"] {
  font-size: 144%;
}

mjx-container [size="LG"] {
  font-size: 173%;
}

mjx-container [size="hg"] {
  font-size: 207%;
}

mjx-container [size="HG"] {
  font-size: 249%;
}

mjx-container [width="full"] {
  width: 100%;
}

mjx-box {
  display: inline-block;
}

mjx-block {
  display: block;
}

mjx-itable {
  display: inline-table;
}

mjx-row {
  display: table-row;
}

mjx-row > * {
  display: table-cell;
}

mjx-mtext {
  display: inline-block;
  text-align: left;
}

mjx-mstyle {
  display: inline-block;
}

mjx-merror {
  display: inline-block;
  color: red;
  background-color: yellow;
}

mjx-mphantom {
  visibility: hidden;
}

_::-webkit-full-page-media, _:future, :root mjx-container {
  will-change: opacity;
}

mjx-assistive-mml {
  position: absolute !important;
  top: 0px;
  left: 0px;
  clip: rect(1px, 1px, 1px, 1px);
  padding: 1px 0px 0px 0px !important;
  border: 0px !important;
  display: block !important;
  width: auto !important;
  overflow: hidden !important;
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}

mjx-assistive-mml[display="block"] {
  width: 100% !important;
}

mjx-math {
  display: inline-block;
  text-align: left;
  line-height: 0;
  text-indent: 0;
  font-style: normal;
  font-weight: normal;
  font-size: 100%;
  font-size-adjust: none;
  letter-spacing: normal;
  border-collapse: collapse;
  word-wrap: normal;
  word-spacing: normal;
  white-space: nowrap;
  direction: ltr;
  padding: 1px 0;
}

mjx-container[jax="CHTML"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="CHTML"][display="true"][width="full"] {
  display: flex;
}

mjx-container[jax="CHTML"][display="true"] mjx-math {
  padding: 0;
}

mjx-container[jax="CHTML"][justify="left"] {
  text-align: left;
}

mjx-container[jax="CHTML"][justify="right"] {
  text-align: right;
}

mjx-msub {
  display: inline-block;
  text-align: left;
}

mjx-mi {
  display: inline-block;
  text-align: left;
}

mjx-c {
  display: inline-block;
}

mjx-utext {
  display: inline-block;
  padding: .75em 0 .2em 0;
}

mjx-TeXAtom {
  display: inline-block;
  text-align: left;
}

mjx-mo {
  display: inline-block;
  text-align: left;
}

mjx-stretchy-h {
  display: inline-table;
  width: 100%;
}

mjx-stretchy-h > * {
  display: table-cell;
  width: 0;
}

mjx-stretchy-h > * > mjx-c {
  display: inline-block;
  transform: scalex(1.0000001);
}

mjx-stretchy-h > * > mjx-c::before {
  display: inline-block;
  width: initial;
}

mjx-stretchy-h > mjx-ext {
  /* IE */ overflow: hidden;
  /* others */ overflow: clip visible;
  width: 100%;
}

mjx-stretchy-h > mjx-ext > mjx-c::before {
  transform: scalex(500);
}

mjx-stretchy-h > mjx-ext > mjx-c {
  width: 0;
}

mjx-stretchy-h > mjx-beg > mjx-c {
  margin-right: -.1em;
}

mjx-stretchy-h > mjx-end > mjx-c {
  margin-left: -.1em;
}

mjx-stretchy-v {
  display: inline-block;
}

mjx-stretchy-v > * {
  display: block;
}

mjx-stretchy-v > mjx-beg {
  height: 0;
}

mjx-stretchy-v > mjx-end > mjx-c {
  display: block;
}

mjx-stretchy-v > * > mjx-c {
  transform: scaley(1.0000001);
  transform-origin: left center;
  overflow: hidden;
}

mjx-stretchy-v > mjx-ext {
  display: block;
  height: 100%;
  box-sizing: border-box;
  border: 0px solid transparent;
  /* IE */ overflow: hidden;
  /* others */ overflow: visible clip;
}

mjx-stretchy-v > mjx-ext > mjx-c::before {
  width: initial;
  box-sizing: border-box;
}

mjx-stretchy-v > mjx-ext > mjx-c {
  transform: scaleY(500) translateY(.075em);
  overflow: visible;
}

mjx-mark {
  display: inline-block;
  height: 0px;
}

mjx-msup {
  display: inline-block;
  text-align: left;
}

mjx-mn {
  display: inline-block;
  text-align: left;
}

mjx-munderover {
  display: inline-block;
  text-align: left;
}

mjx-munderover:not([limits="false"]) {
  padding-top: .1em;
}

mjx-munderover:not([limits="false"]) > * {
  display: block;
}

mjx-msubsup {
  display: inline-block;
  text-align: left;
}

mjx-script {
  display: inline-block;
  padding-right: .05em;
  padding-left: .033em;
}

mjx-script > mjx-spacer {
  display: block;
}

mjx-mspace {
  display: inline-block;
  text-align: left;
}

mjx-c::before {
  display: block;
  width: 0;
}

.MJX-TEX {
  font-family: MJXZERO, MJXTEX;
}

.TEX-B {
  font-family: MJXZERO, MJXTEX-B;
}

.TEX-I {
  font-family: MJXZERO, MJXTEX-I;
}

.TEX-MI {
  font-family: MJXZERO, MJXTEX-MI;
}

.TEX-BI {
  font-family: MJXZERO, MJXTEX-BI;
}

.TEX-S1 {
  font-family: MJXZERO, MJXTEX-S1;
}

.TEX-S2 {
  font-family: MJXZERO, MJXTEX-S2;
}

.TEX-S3 {
  font-family: MJXZERO, MJXTEX-S3;
}

.TEX-S4 {
  font-family: MJXZERO, MJXTEX-S4;
}

.TEX-A {
  font-family: MJXZERO, MJXTEX-A;
}

.TEX-C {
  font-family: MJXZERO, MJXTEX-C;
}

.TEX-CB {
  font-family: MJXZERO, MJXTEX-CB;
}

.TEX-FR {
  font-family: MJXZERO, MJXTEX-FR;
}

.TEX-FRB {
  font-family: MJXZERO, MJXTEX-FRB;
}

.TEX-SS {
  font-family: MJXZERO, MJXTEX-SS;
}

.TEX-SSB {
  font-family: MJXZERO, MJXTEX-SSB;
}

.TEX-SSI {
  font-family: MJXZERO, MJXTEX-SSI;
}

.TEX-SC {
  font-family: MJXZERO, MJXTEX-SC;
}

.TEX-T {
  font-family: MJXZERO, MJXTEX-T;
}

.TEX-V {
  font-family: MJXZERO, MJXTEX-V;
}

.TEX-VB {
  font-family: MJXZERO, MJXTEX-VB;
}

mjx-stretchy-v mjx-c, mjx-stretchy-h mjx-c {
  font-family: MJXZERO, MJXTEX-S1, MJXTEX-S4, MJXTEX, MJXTEX-A ! important;
}

@font-face /* 0 */ {
  font-family: MJXZERO;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff");
}

@font-face /* 1 */ {
  font-family: MJXTEX;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff");
}

@font-face /* 2 */ {
  font-family: MJXTEX-B;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff");
}

@font-face /* 3 */ {
  font-family: MJXTEX-I;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff");
}

@font-face /* 4 */ {
  font-family: MJXTEX-MI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff");
}

@font-face /* 5 */ {
  font-family: MJXTEX-BI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff");
}

@font-face /* 6 */ {
  font-family: MJXTEX-S1;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff");
}

@font-face /* 7 */ {
  font-family: MJXTEX-S2;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff");
}

@font-face /* 8 */ {
  font-family: MJXTEX-S3;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff");
}

@font-face /* 9 */ {
  font-family: MJXTEX-S4;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff");
}

@font-face /* 10 */ {
  font-family: MJXTEX-A;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff");
}

@font-face /* 11 */ {
  font-family: MJXTEX-C;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff");
}

@font-face /* 12 */ {
  font-family: MJXTEX-CB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff");
}

@font-face /* 13 */ {
  font-family: MJXTEX-FR;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff");
}

@font-face /* 14 */ {
  font-family: MJXTEX-FRB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff");
}

@font-face /* 15 */ {
  font-family: MJXTEX-SS;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff");
}

@font-face /* 16 */ {
  font-family: MJXTEX-SSB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff");
}

@font-face /* 17 */ {
  font-family: MJXTEX-SSI;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff");
}

@font-face /* 18 */ {
  font-family: MJXTEX-SC;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff");
}

@font-face /* 19 */ {
  font-family: MJXTEX-T;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff");
}

@font-face /* 20 */ {
  font-family: MJXTEX-V;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff");
}

@font-face /* 21 */ {
  font-family: MJXTEX-VB;
  src: url("https://cdn.jsdelivr.net/npm/mathjax@3/es5/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff");
}

mjx-c.mjx-c1D453.TEX-I::before {
  padding: 0.705em 0.55em 0.205em 0;
  content: "f";
}

mjx-c.mjx-c1D463.TEX-I::before {
  padding: 0.443em 0.485em 0.011em 0;
  content: "v";
}

mjx-c.mjx-c1D703.TEX-I::before {
  padding: 0.705em 0.469em 0.01em 0;
  content: "\3B8";
}

mjx-c.mjx-c3A::before {
  padding: 0.43em 0.278em 0 0;
  content: ":";
}

mjx-c.mjx-c211D.TEX-A::before {
  padding: 0.683em 0.722em 0 0;
  content: "R";
}

mjx-c.mjx-c1D436.TEX-I::before {
  padding: 0.705em 0.76em 0.022em 0;
  content: "C";
}

mjx-c.mjx-cD7::before {
  padding: 0.491em 0.778em 0 0;
  content: "\D7";
}

mjx-c.mjx-c1D43B.TEX-I::before {
  padding: 0.683em 0.888em 0 0;
  content: "H";
}

mjx-c.mjx-c1D44A.TEX-I::before {
  padding: 0.683em 1.048em 0.022em 0;
  content: "W";
}

mjx-c.mjx-c2192::before {
  padding: 0.511em 1em 0.011em 0;
  content: "\2192";
}

mjx-c.mjx-c1D449.TEX-I::before {
  padding: 0.683em 0.769em 0.022em 0;
  content: "V";
}

mjx-c.mjx-c1D439.TEX-I::before {
  padding: 0.68em 0.749em 0 0;
  content: "F";
}

mjx-c.mjx-c2C::before {
  padding: 0.121em 0.278em 0.194em 0;
  content: ",";
}

mjx-c.mjx-c1D415.TEX-B::before {
  padding: 0.686em 0.869em 0.007em 0;
  content: "V";
}

mjx-c.mjx-c1D413.TEX-B::before {
  padding: 0.675em 0.8em 0 0;
  content: "T";
}

mjx-c.mjx-c3D::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "=";
}

mjx-c.mjx-c1D45A.TEX-I::before {
  padding: 0.442em 0.878em 0.011em 0;
  content: "m";
}

mjx-c.mjx-c1D6FE.TEX-I::before {
  padding: 0.441em 0.543em 0.216em 0;
  content: "\3B3";
}

mjx-c.mjx-c28::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: "(";
}

mjx-c.mjx-c1D417.TEX-B::before {
  padding: 0.686em 0.869em 0 0;
  content: "X";
}

mjx-c.mjx-c2032::before {
  padding: 0.56em 0.275em 0 0;
  content: "\2032";
}

mjx-c.mjx-c29::before {
  padding: 0.75em 0.389em 0.25em 0;
  content: ")";
}

mjx-c.mjx-c2208::before {
  padding: 0.54em 0.667em 0.04em 0;
  content: "\2208";
}

mjx-c.mjx-c1D437.TEX-I::before {
  padding: 0.683em 0.828em 0 0;
  content: "D";
}

mjx-c.mjx-c6F::before {
  padding: 0.448em 0.5em 0.01em 0;
  content: "o";
}

mjx-c.mjx-c72::before {
  padding: 0.442em 0.392em 0 0;
  content: "r";
}

mjx-c.mjx-c69::before {
  padding: 0.669em 0.278em 0 0;
  content: "i";
}

mjx-c.mjx-c67::before {
  padding: 0.453em 0.5em 0.206em 0;
  content: "g";
}

mjx-c.mjx-c6E::before {
  padding: 0.442em 0.556em 0 0;
  content: "n";
}

mjx-c.mjx-c61::before {
  padding: 0.448em 0.5em 0.011em 0;
  content: "a";
}

mjx-c.mjx-c6C::before {
  padding: 0.694em 0.278em 0 0;
  content: "l";
}

mjx-c.mjx-c2F::before {
  padding: 0.75em 0.5em 0.25em 0;
  content: "/";
}

mjx-c.mjx-c1D443.TEX-I::before {
  padding: 0.683em 0.751em 0 0;
  content: "P";
}

mjx-c.mjx-c1D446.TEX-I::before {
  padding: 0.705em 0.645em 0.022em 0;
  content: "S";
}

mjx-c.mjx-c51::before {
  padding: 0.705em 0.778em 0.193em 0;
  content: "Q";
}

mjx-c.mjx-c77::before {
  padding: 0.431em 0.722em 0.011em 0;
  content: "w";
}

mjx-c.mjx-c65::before {
  padding: 0.448em 0.444em 0.011em 0;
  content: "e";
}

mjx-c.mjx-c33::before {
  padding: 0.665em 0.5em 0.022em 0;
  content: "3";
}

mjx-c.mjx-c22C5::before {
  padding: 0.31em 0.278em 0 0;
  content: "\22C5";
}

mjx-c.mjx-c1D440.TEX-I::before {
  padding: 0.683em 1.051em 0 0;
  content: "M";
}

mjx-c.mjx-c4C::before {
  padding: 0.683em 0.625em 0 0;
  content: "L";
}

mjx-c.mjx-c56::before {
  padding: 0.683em 0.75em 0.022em 0;
  content: "V";
}

mjx-c.mjx-c41::before {
  padding: 0.716em 0.75em 0 0;
  content: "A";
}

mjx-c.mjx-c2D::before {
  padding: 0.252em 0.333em 0 0;
  content: "-";
}

mjx-c.mjx-c4F::before {
  padding: 0.705em 0.778em 0.022em 0;
  content: "O";
}

mjx-c.mjx-c73::before {
  padding: 0.448em 0.394em 0.011em 0;
  content: "s";
}

mjx-c.mjx-c2217::before {
  padding: 0.465em 0.5em 0 0;
  content: "\2217";
}

mjx-c.mjx-c5B::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "[";
}

mjx-c.mjx-c1D458.TEX-I::before {
  padding: 0.694em 0.521em 0.011em 0;
  content: "k";
}

mjx-c.mjx-c2B::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "+";
}

mjx-c.mjx-c31::before {
  padding: 0.666em 0.5em 0 0;
  content: "1";
}

mjx-c.mjx-c5D::before {
  padding: 0.75em 0.278em 0.25em 0;
  content: "]";
}

mjx-c.mjx-c47::before {
  padding: 0.705em 0.785em 0.022em 0;
  content: "G";
}

mjx-c.mjx-c6D::before {
  padding: 0.442em 0.833em 0 0;
  content: "m";
}

mjx-c.mjx-c70::before {
  padding: 0.442em 0.556em 0.194em 0;
  content: "p";
}

mjx-c.mjx-c38::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "8";
}

mjx-c.mjx-c39::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "9";
}

mjx-c.mjx-c36::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "6";
}

mjx-c.mjx-c34::before {
  padding: 0.677em 0.5em 0 0;
  content: "4";
}

mjx-c.mjx-c32::before {
  padding: 0.666em 0.5em 0 0;
  content: "2";
}

mjx-c.mjx-c35::before {
  padding: 0.666em 0.5em 0.022em 0;
  content: "5";
}

mjx-c.mjx-c74::before {
  padding: 0.615em 0.389em 0.01em 0;
  content: "t";
}

mjx-c.mjx-c78::before {
  padding: 0.431em 0.528em 0 0;
  content: "x";
}

mjx-c.mjx-c1D43F.TEX-I::before {
  padding: 0.683em 0.681em 0 0;
  content: "L";
}

mjx-c.mjx-c2212::before {
  padding: 0.583em 0.778em 0.082em 0;
  content: "\2212";
}

mjx-c.mjx-c2211.TEX-S1::before {
  padding: 0.75em 1.056em 0.25em 0;
  content: "\2211";
}

mjx-c.mjx-c1D441.TEX-I::before {
  padding: 0.683em 0.888em 0 0;
  content: "N";
}

mjx-c.mjx-c1D456.TEX-I::before {
  padding: 0.661em 0.345em 0.011em 0;
  content: "i";
}

mjx-c.mjx-c63::before {
  padding: 0.448em 0.444em 0.011em 0;
  content: "c";
}

mjx-c.mjx-c79::before {
  padding: 0.431em 0.528em 0.204em 0;
  content: "y";
}

mjx-c.mjx-c66::before {
  padding: 0.705em 0.372em 0 0;
  content: "f";
}

mjx-c.mjx-c221D::before {
  padding: 0.442em 0.778em 0.011em 0;
  content: "\221D";
}

mjx-c.mjx-c20::before {
  padding: 0 0.25em 0 0;
  content: " ";
}

mjx-c.mjx-c64::before {
  padding: 0.694em 0.556em 0.011em 0;
  content: "d";
}
</style><script id="altmetric-embed-js" src="./visual_tokens_blogpost_files/altmetric_badges-10c8bb92f52500b3055c97099c9c67f45c4cef824fe9b68d82fe653d6db644c0.js"></script><link rel="stylesheet" href="./visual_tokens_blogpost_files/badge.css"><style type="text/css">.medium-zoom-overlay{position:fixed;top:0;right:0;bottom:0;left:0;opacity:0;transition:opacity .3s;will-change:opacity}.medium-zoom--opened .medium-zoom-overlay{cursor:pointer;cursor:zoom-out;opacity:1}.medium-zoom-image{cursor:pointer;cursor:zoom-in;transition:transform .3s cubic-bezier(.2,0,.2,1)!important}.medium-zoom-image--hidden{visibility:hidden}.medium-zoom-image--opened{position:relative;cursor:pointer;cursor:zoom-out;will-change:transform}</style></head>

  <!-- Body -->
  <body class="fixed-top-nav " style="padding-top: 56.6211px;">
    <!-- Header -->
    <header>
  <!-- Nav Bar -->
  <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation">
    <div class="container">
      
        <a class="navbar-brand title font-weight-lighter" href="http://127.0.0.1:4000/">
          
            
              <span class="font-weight-bold">Matteo</span>
            
            
            Nulli
          
        </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>

      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          

          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="http://127.0.0.1:4000/">about
              
            </a>
          </li>

          <!-- Other pages -->
          
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="http://127.0.0.1:4000/publications/">publications
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="http://127.0.0.1:4000/blog/">blog
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="http://127.0.0.1:4000/cv/">cv
                    
                  </a>
                </li>
              
            
          
            
              
                <li class="nav-item ">
                  
                  <a class="nav-link" href="http://127.0.0.1:4000/projects/">more
                    
                  </a>
                </li>
              
            
          
          
            <!-- Search -->
            <li class="nav-item">
              <button id="search-toggle" title="Search" onclick="openSearchModal()">
                <span class="nav-link">‚åò k <i class="ti ti-search"></i></span>
              </button>
            </li>
          
          
            <!-- Toogle theme mode -->
            <li class="toggle-container">
              <button id="light-toggle" title="Change theme">
                <i class="ti ti-sun-moon" id="light-toggle-system"></i>
                <i class="ti ti-moon-filled" id="light-toggle-dark"></i>
                <i class="ti ti-sun-filled" id="light-toggle-light"></i>
              </button>
            </li>
          
        </ul>
      </div>
    </div>
  </nav>
  
    <!-- Scrolling Progress Bar -->
    <progress id="progress" value="0" style="top: 56.6211px;" max="10818">
      <div class="progress-container">
        <span class="progress-bar"></span>
      </div>
    </progress>
  
</header>


    <!-- Content -->
    <div class="container mt-5" role="main">
      
        




  <!-- Page/Post style -->
  <style type="text/css">
    .mermaid svg { 
  max-width: 100%; 
  height: auto; 
}

  </style>



<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']]
    }
  };
</script>
<script defer="" src="./visual_tokens_blogpost_files/tex-chtml.js"></script>




<div class="post">
  <header class="post-header">
    <h1 class="post-title">De-mystifying Multimodal Learning<br><small>The Hidden Inefficiency in Vision Language Modelling</small>
</h1>
    <p class="post-meta">
      Created in January 30, 2026
      
      
      
    </p>
    <p class="post-tags">
      
        <a href="http://127.0.0.1:4000/blog/2026"> <i class="fa-solid fa-calendar fa-sm"></i> 2026 </a>
      
      
        &nbsp; ¬∑ &nbsp;
        
          
            <a href="http://127.0.0.1:4000/blog/tag/multimodal-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Multimodal-Learning</a>
          
          
            &nbsp;
          
        
          
            <a href="http://127.0.0.1:4000/blog/tag/vision-language-modelling"> <i class="fa-solid fa-hashtag fa-sm"></i> Vision-Language-Modelling</a>
          
          
        
      

      
    </p>
    
      <script type="module">
  import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
  mermaid.initialize({ startOnLoad: true });
</script>

    
  </header>

  <article class="post-content">
    
    <div id="markdown-content">
      <h5 id="matteo-nulli"><b>Matteo Nulli</b></h5>
<!-- ###### eBay
###### <img src="https://upload.wikimedia.org/wikipedia/commons/1/1b/EBay_logo.svg" alt="eBay" height="24"/> &nbsp;  -->
<h6 id="-blogpost">üìù <a href="https://matteonulli.github.io/blog/2025/vlmsecom/" rel="external nofollow noopener" target="_blank">Blogpost</a>
</h6>
<p><br></p>

<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['\\[', '\\]']],
    },
    svg: { fontCache: 'global' }
  };
</script>

<script id="MathJax-script" async="" src="./visual_tokens_blogpost_files/tex-svg.js">
</script>

<link rel="stylesheet" href="./visual_tokens_blogpost_files/katex.min.css">

<script defer="" src="./visual_tokens_blogpost_files/katex.min.js"></script>

<script defer="" src="./visual_tokens_blogpost_files/auto-render.min.js" onload="renderMathInElement(document.body, {
          delimiters: [
            {left: &#39;$$&#39;, right: &#39;$$&#39;, display: true},
            {left: &#39;\\[&#39;, right: &#39;\\]&#39;, display: true},
            {left: &#39;$&#39;,  right: &#39;$&#39;,  display: false},
            {left: &#39;\\(&#39;, right: &#39;\\)&#39;, display: false}
          ],
          // keep default ignore list so code/pre are skipped
        });"></script>

<h2 id="introduction">Introduction</h2>
<p>In the shift from text-only models to Vision Language Models (VLMs), we often talk about ‚Äúparameters‚Äù and ‚Äúemergent reasoning.‚Äù However, there is a hidden currency that governs the performance, cost, and feasibility of these systems: <strong>Visual Tokens (VT)</strong>.</p>

<p>While Large Language Models (LLMs) are natively blind, they ‚Äúsee‚Äù by consuming images that have been decomposed, projected, and flattened into a format they can digest. This transformation isn‚Äôt free. Whether you are building a real-time assistant or an OCR pipeline, the number of visual tokens your model generates is the primary lever for inference latency, VRAM consumption, and context window management. Understanding the computational overhead of these tokens is no longer just an academic exercise‚Äîit is a production necessity.</p>

<p>In this first installment of our series, De-mystifying Multimodal Learning, we break down the mechanics of how images become language-compatible vectors. Specifically, we will cover:</p>

<p>What we will cover:<br></p>
<ul>
  <li>
    <p><a href="http://127.0.0.1:4000/blog/2026/demystifying1/#preamble-how-do-llms-see">Preamble: How do LLMs see?</a>: An architectural deep dive deep into the anatomy of Vision Language Models (VLMs) and the birth of the Visual Token.</p>
  </li>
  <li>
    <p><a href="http://127.0.0.1:4000/blog/2026/demystifying1/#calculating-visual-tokens">Calculating Visual Tokens</a>: Present a practical guide to estimating token counts across different SOTA strategies‚Äîfrom Qwen‚Äôs dynamic merging to LLaVA‚Äôs high-resolution grids‚Äîwithout running a single line of inference.</p>
  </li>
  <li>
    <p><a href="http://127.0.0.1:4000/blog/2026/demystifying1/#visual-token-count-impact">Impact of Token Count</a>: Analyse of how these tokens impact the ‚Äúthree pillars‚Äù of production: Context Windows, Latency, and VRAM.</p>
  </li>
</ul>

<p><a id="overview"></a></p>
<div class="row mt-3">
  <div class="col-12">
    <div class="mx-auto text-center" style="width: 80%;">
      

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
    <img src="./visual_tokens_blogpost_files/vlm_tokens.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();">
  </picture>

  
</figure>

      <div class="caption text-center mt-2">
        Figure 1: Overview of the Vision Token count estimation process. The estimated counts are done based on Spatial Merge Size of 2, AnyResolution of 3x3 windows and Spatial Average Pooling of size 4. C, H, W represent the channels, heigth and width of the image, whereas N and P are the number of patches and the patch dimension respectively. F and D are embedding dimension sizes outputted from the vision encoder and the MLP and V is the number of visual tokens, which depends on the type of encoder, MLP and the re-sizing of images, i.e. the actual H and W. In <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#calculating-visual-tokens">the section below</a>, we dive deeper into the difference between each of the models reported on the right/
      </div>
    </div>
  </div>
</div>

<p><br></p>

<h2 id="preamble-how-do-llms-see">Preamble: How do LLMs see?</h2>

<p>It is clear that LLMs cannot see images, so how do we allow them to understand image inputs in the first place? 
Architecturally Vision Language Models are constitued by three major components:</p>
<ul>
  <li>Vision Encoders (VE), usually a CLIP-like image encoder (<a href="http://127.0.0.1:4000/blog/2026/demystifying1/#vit-2021">Dosovitskiy et al., 2021</a>,<a href="http://127.0.0.1:4000/blog/2026/demystifying1/#clip-2021">Radford et al., 2021</a>, <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#sigmoid-loss-2023">Zhai et al., 2023</a>, <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#qwen2-5-vl-2025">Bai et al., 2025</a>), but it can vary in architecture and training style. See <a href="https://jina.ai/vision-encoder-survey.pdf" rel="external nofollow noopener" target="_blank">this</a> extensive survey for more information .</li>
  <li>Modality Connectors, often simple Multi-Layer Perceptron, with some architectures employing attention blocks (<a href="http://127.0.0.1:4000/blog/2026/demystifying1/#blip2-2023">Li et al., 2023</a>) and other alternatives (<a href="http://127.0.0.1:4000/blog/2026/demystifying1/#cambrian-1-2024">Tong et al., 2024</a>, <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#nulliobjectguided-2025">Nulli et al,. 2025</a>).</li>
  <li>Large Language Models (LLMs) (<a href="http://127.0.0.1:4000/blog/2026/demystifying1/#llama-3-herd-2024">Abhimanyu, et al. 2024</a>).</li>
</ul>

<p>Let <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>C</mi><mo>√ó</mo><mi>H</mi><mo>√ó</mo><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X} \in \mathbb{R}^{C \times H\times W} </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7252em; vertical-align: -0.0391em;"></span><span class="mord mathbf">X</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">‚àà</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span><span class="mbin mtight">√ó</span><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span><span class="mbin mtight">√ó</span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span></span></span></span></span></span></span></span></span></span></span></span></span> be an image and <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>‚àà</mo><mi mathvariant="normal">Œ£</mi></mrow><annotation encoding="application/x-tex">t \in  \Sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6542em; vertical-align: -0.0391em;"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">‚àà</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord">Œ£</span></span></span></span></span> be a text input, where <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Œ£</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord">Œ£</span></span></span></span></span> is the input space of character sequences.
Furthermore let <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>v</mi><mi>Œ∏</mi></mrow></msub></mrow><annotation encoding="application/x-tex">f_{v\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.1076em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">v</span><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">Œ∏</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> be a contrastive pre-trained Vision Encoder, defined as: <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="0" style="font-size: 119.5%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.06em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D463 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D703 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3A"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2192"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D439 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c2C"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>f</mi><mrow data-mjx-texclass="ORD"><mi>v</mi><mi>Œ∏</mi></mrow></msub><mo>:</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>C</mi><mo>√ó</mo><mi>H</mi><mo>√ó</mo><mi>W</mi></mrow></msup><mo stretchy="false">‚Üí</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>V</mi><mo>√ó</mo><mi>F</mi></mrow></msup><mo>,</mo></math></mjx-assistive-mml></mjx-container>,
where <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span></span></span></span></span> is the number of visual tokens and <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">F</span></span></span></span></span> their embedding dimension.
Let <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>Œ≥</mi></msub><mo>:</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>V</mi><mo>√ó</mo><mi>F</mi></mrow></msup><mo>‚Üí</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>V</mi><mo>√ó</mo><mi>D</mi></mrow></msup></mrow><annotation encoding="application/x-tex">m_\gamma: \mathbb{R}^{V \times F} \rightarrow \mathbb{R}^{V \times D}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7167em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0556em;">Œ≥</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">:</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.2222em;">V</span><span class="mbin mtight">√ó</span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">F</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">‚Üí</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.2222em;">V</span><span class="mbin mtight">√ó</span><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">D</span></span></span></span></span></span></span></span></span></span></span></span></span> be a modality connector and <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>œï</mi></msub></mrow><annotation encoding="application/x-tex">g_{\phi}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7167em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">œï</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span> be the LLM mapping an embedded input token sequence to an output token sequence.</p>

<p>After this heavy notation information, we get to the guiding question of this post:</p>

<p align="center"><code>What is a Visual Token?</code></p>

<p>Due to <a href="https://arxiv.org/pdf/2103.00020" rel="external nofollow noopener" target="_blank">Contrastive Training</a> (some extra sources <a href="https://medium.com/rectlabs/clip-contrastive-language-image-pre-training-dce66ae18fe1" rel="external nofollow noopener" target="_blank">here</a> and <a href="https://github.com/openai/CLIP" rel="external nofollow noopener" target="_blank">here</a>) Vision Encoders learn to be powerful feature extractors, compressing visual information into vectors (tokens) semantically aligned with language.</p>

<p>Practically, the processing flow of <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>v</mi><mi>Œ∏</mi></mrow></msub></mrow><annotation encoding="application/x-tex">f_{v\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.1076em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">v</span><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">Œ∏</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> is broken down into:</p>

<h4 id="1-patch-partitioning">1. Patch Partitioning</h4>

<p>The first step is breaking the high-resolution image <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi></mrow><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6861em;"></span><span class="mord mathbf">X</span></span></span></span></span> into a grid of fixed-size patches.
Assuming our image has <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>336</mn><mo>√ó</mo><mn>336</mn></mrow><annotation encoding="application/x-tex">336 \times 336</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">336</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">336</span></span></span></span></span> pixels and we use a patch size of <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo>=</mo><mn>14</mn></mrow><annotation encoding="application/x-tex">P=14</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">P</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">14</span></span></span></span></span>, in standard vision encoders like CLIP, we divide the image into <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>24</mn><mo>√ó</mo><mn>24</mn><mo>=</mo><mn>576</mn></mrow><annotation encoding="application/x-tex">24 \times 24 = 576</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">24</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">24</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">576</span></span></span></span></span> distinct squares.
Mathematically, the image is reshaped from <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>C</mi><mo>√ó</mo><mi>H</mi><mo>√ó</mo><mi>W</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X} \in \mathbb{R}^{C \times H \times W}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7252em; vertical-align: -0.0391em;"></span><span class="mord mathbf">X</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">‚àà</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span><span class="mbin mtight">√ó</span><span class="mord mathnormal mtight" style="margin-right: 0.0813em;">H</span><span class="mbin mtight">√ó</span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">W</span></span></span></span></span></span></span></span></span></span></span></span></span> into a sequence of flattened 2D patches <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="bold">x</mi><mi>p</mi></msub><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>√ó</mo><mo stretchy="false">(</mo><msup><mi>P</mi><mn>2</mn></msup><mo>‚ãÖ</mo><mi>C</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{x}_p \in \mathbb{R}^{N \times (P^2 \cdot C)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8252em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathbf">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">‚àà</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.9869em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.9869em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.109em;">N</span><span class="mbin mtight">√ó</span><span class="mopen mtight">(</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">P</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8913em;"><span class="" style="top: -2.931em; margin-right: 0.0714em;"><span class="pstrut" style="height: 2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">‚ãÖ</span><span class="mord mathnormal mtight" style="margin-right: 0.0715em;">C</span><span class="mclose mtight">)</span></span></span></span></span></span></span></span></span></span></span></span></span>, where <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span> is the total number of patches.</p>

<h4 id="2-linear-projection-and-position-embeddings">2. Linear Projection and Position Embeddings</h4>

<p>These are simply raw pixel values. To convert them into vectors <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>f</mi><mrow><mi>v</mi><mi>Œ∏</mi></mrow></msub></mrow><annotation encoding="application/x-tex">f_{v\theta}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.1076em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">v</span><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">Œ∏</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> projects each flattened patch into a latent representation through a linear layer. 
Given the lack of spatial prior of Vision Transformers (ViT) (see <a href="https://towardsdatascience.com/vision-transformers-vit-explained-are-they-better-than-cnns/" rel="external nofollow noopener" target="_blank">here</a> for a difference between CNNs and ViTs), these vectors are equipped with learnable positional encodings, injecting ‚ÄúGPS-like‚Äù coordinates into latent representations.</p>

<h4 id="3-transformer-layers">3. Transformer Layers</h4>

<p>The resulting vectors are passed through several Transformers Layers consisting of Multi-Head Self-Attention and MLPs, whose output is the representation of a patch within the context of the whole image.</p>

<p>This full process produces <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mi mathvariant="bold">X</mi><mtext mathvariant="bold">‚Äô</mtext></mrow><mo>=</mo><msub><mi>f</mi><mrow><mi>v</mi><mi>Œ∏</mi></mrow></msub><mo stretchy="false">(</mo><mi mathvariant="bold">X</mi><mo stretchy="false">)</mo><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>V</mi><mo>√ó</mo><mi>F</mi></mrow></msup></mrow><annotation encoding="application/x-tex">\mathbf{X‚Äô} = f_{v\theta}(\mathbf{X}) \in \mathbb{R}^{V\times F}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord"><span class="mord mathbf">X‚Äô</span></span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.1076em;">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.1076em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.0359em;">v</span><span class="mord mathnormal mtight" style="margin-right: 0.0278em;">Œ∏</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathbf">X</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">‚àà</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8413em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.8413em;"><span class="" style="top: -3.063em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right: 0.2222em;">V</span><span class="mbin mtight">√ó</span><span class="mord mathnormal mtight" style="margin-right: 0.1389em;">F</span></span></span></span></span></span></span></span></span></span></span></span></span>, we obtain a sequence of vectors with <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi></mrow><annotation encoding="application/x-tex">F</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">F</span></span></span></span></span> being the embedding dimension.</p>

<h4 id="connecting-modalities">Connecting Modalities</h4>

<p>Getting the actual vision tokens from <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">X</mi><mtext mathvariant="bold">‚Äô</mtext></mrow><annotation encoding="application/x-tex">\mathbf{X‚Äô}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord"><span class="mord mathbf">X‚Äô</span></span></span></span></span></span> is just a matter of passing it through a connector, obtaining 
<mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="1" style="font-size: 119.5%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D415 TEX-B"></mjx-c><mjx-c class="mjx-c1D413 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D45A TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-mi class="mjx-i" size="s"><mjx-c class="mjx-c1D6FE TEX-I"></mjx-c></mjx-mi></mjx-script></mjx-msub><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msup><mjx-texatom texclass="ORD"><mjx-mi class="mjx-b"><mjx-c class="mjx-c1D417 TEX-B"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-mo class="mjx-var" size="s"><mjx-c class="mjx-c2032"></mjx-c></mjx-mo></mjx-script></mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c2208"></mjx-c></mjx-mo><mjx-msup space="4"><mjx-texatom texclass="ORD"><mjx-mi class="mjx-ds mjx-b"><mjx-c class="mjx-c211D TEX-A"></mjx-c></mjx-mi></mjx-texatom><mjx-script style="vertical-align: 0.363em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D437 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msup></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">VT</mi></mrow><mo>=</mo><msub><mi>m</mi><mi>Œ≥</mi></msub><mo stretchy="false">(</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="bold">X</mi></mrow><mo data-mjx-alternate="1">‚Ä≤</mo></msup><mo stretchy="false">)</mo><mo>‚àà</mo><msup><mrow data-mjx-texclass="ORD"><mi mathvariant="double-struck">R</mi></mrow><mrow data-mjx-texclass="ORD"><mi>V</mi><mo>√ó</mo><mi>D</mi></mrow></msup></math></mjx-assistive-mml></mjx-container>
where <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>D</mi></mrow><annotation encoding="application/x-tex">D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0278em;">D</span></span></span></span></span> is the expected embedding input size of <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>g</mi><mi>œï</mi></msub></mrow><annotation encoding="application/x-tex">g_{\phi}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7167em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0359em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.0359em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">œï</span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span>.</p>

<p><br></p>

<h2 id="calculating-visual-tokens">Calculating #Visual Tokens</h2>

<p>Now that we know what Visual Tokens are, you might wonder</p>

<p align="center"><code>How many Visual Tokens VLMs produce given image input size?</code></p>

<h4 id="original-recipe">Original Recipe</h4>

<p>Within the first VLM architectures (<a href="http://127.0.0.1:4000/blog/2026/demystifying1/#visual-instruction-tuning-2023">Liu et al., 2023</a>) this estimation is straightforward. 
First generation VLMs relied on Vision Encoder which have a fixed input resolution and a patch size (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mi>S</mi></mrow><annotation encoding="application/x-tex">PS</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">PS</span></span></span></span></span>). What this means is that whatever its input size, the picture would always be re-scaled to <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>√ó</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">H \times W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span></span></span></span></span>. This implies that, given both <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span></span></span></span></span> and <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi></mrow><annotation encoding="application/x-tex">W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span></span></span></span></span>, the final number of visual tokens, i.e. dimenson <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span></span></span></span></span></p>
<p align="center"> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="2" style="font-size: 119.5%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.186em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c67"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c6C"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>V</mi><mrow data-mjx-texclass="ORD"><mtext>original</mtext></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><mi>H</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>P</mi><mi>S</mi><mo stretchy="false">)</mo><mo>√ó</mo><mo stretchy="false">(</mo><mi>W</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mi>P</mi><mi>S</mi><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> </p>

<h4 id="the-resolution-trap">The Resolution Trap</h4>

<p>This came with several problems:</p>
<ol>
  <li>Images resolutions were completely disregarded. Having the same amount of tokens for images of size 336^2 and 1024^2 does not make sense.</li>
  <li>Not only does not make sense, it also does not work. Especially for OCR, visual compositional reasoning and small object detection tasks.</li>
</ol>

<p>However, simply making vision encoders which could support higher resolutions was also not something feasible. First, because now every image would have needed to be re-sized to say <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1024</mn><mo>√ó</mo><mn>1024</mn></mrow><annotation encoding="application/x-tex">1024\times1024</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">1024</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">1024</span></span></span></span></span>. Secondly, <em>tripling</em> the supported image heigth and keeping the same patch size results in almost <em>10x the amount of visual tokens</em>.</p>

<h4 id="modern-approaches">Modern Approaches</h4>

<p><strong>Strategy A: The Dynamic Merger</strong>
We have to start with the game-changers: the QwenVL-2.5 and 3 series (<a href="http://127.0.0.1:4000/blog/2026/demystifying1/#qwen2-5-vl-2025">Bai et al. 2025</a>, <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#qwen3-vl-2025">QwenTeam, 2025</a>). These models ditched the ‚Äúfixed resolution‚Äù rule entirely. Instead of squashing every image into a square, they process images at their native resolution. This sounds great, but it complicates our math: if the image size varies, so does the token count. To calculate it, we need a specific value from the model‚Äôs config.json called the Spatial Merge Size (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>M</mi><mi>S</mi></mrow><annotation encoding="application/x-tex">SMS</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">SMS</span></span></span></span></span>). Think of <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><mi>M</mi><mi>S</mi></mrow><annotation encoding="application/x-tex">SMS</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.0576em;">SMS</span></span></span></span></span> as a compression factor‚Äîit tells the model how many raw image patches to pool together into one visual token.With this in mind, our formula becomes a bit more dynamic:</p>

<p align="center"> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="3" style="font-size: 119.5%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.186em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c51"></mjx-c><mjx-c class="mjx-c77"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c22C5"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D440 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>V</mi><mrow data-mjx-texclass="ORD"><mtext>Qwen3</mtext></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><mi>H</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mi>P</mi><mi>S</mi><mo>‚ãÖ</mo><mi>S</mi><mi>M</mi><mi>S</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>√ó</mo><mo stretchy="false">(</mo><mi>W</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mi>P</mi><mi>S</mi><mo>‚ãÖ</mo><mi>S</mi><mi>M</mi><mi>S</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container> </p>

<p>The Takeaway: You get <strong>perfect aspect ratios without distortion</strong>. The catch? You need to be careful. Large images (or several of them) can silently eat up your context window much faster than you expect.</p>

<p><strong>Strategy B: The Multi-Grid / AnyRes</strong> 
Around the same period LLaVA-Next/OneVision (<a href="http://127.0.0.1:4000/blog/2026/demystifying1/#llava-next-2024">Liu et al., 2024</a>, <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#llavonevision-2024">Li et al., 2024b</a>) came up with a clever, yet expensive encoding technique called ‚ÄúDynamic-High Resolution‚Äù/‚ÄùAny Resolution‚Äù. Depicted in <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#high_res">Figure 2</a>, it consists of splitting the image into <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>√ó</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">k\times k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7778em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span></span></span></span></span> grids, with <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>‚àà</mo><mrow><mn>1</mn><mo separator="true">,</mo><mn>9</mn></mrow></mrow><annotation encoding="application/x-tex">k \in {1, 9}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7335em; vertical-align: -0.0391em;"></span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span><span class="mspace" style="margin-right: 0.2778em;"></span><span class="mrel">‚àà</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.8389em; vertical-align: -0.1944em;"></span><span class="mord"><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right: 0.1667em;"></span><span class="mord">9</span></span></span></span></span></span> before the vision encoding.
This means repeating the encoding process <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>k</mi><mo>√ó</mo><mi>k</mi><mo stretchy="false">)</mo><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">(k \times k) + 1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 1em; vertical-align: -0.25em;"></span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span><span class="mclose">)</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">1</span></span></span></span></span> times, with the 1 being the picture in its entirety.</p>

<p><a id="high_res"></a></p>
<div class="row mt-3">
  <div class="col-12">
    <div class="mx-auto text-center" style="width: 80%;">
      

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
    <img src="./visual_tokens_blogpost_files/high_res.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();">
  </picture>

  
</figure>

      <div class="caption text-center mt-2">
        Figure 2: Illustration of Dynamic High Resolution on 2x2 grid from <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#llava-next-2024">LLaVA-NeXT</a> paper. 
      </div>
    </div>
  </div>
</div>

<p>Although this results higher detail understanding given the entire focus of the encoder on smaller portions of the image, it also most crucially implies an enormous increase in Visual Token count. Given the calculations in the <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#original-recipe">original recipe</a>, we have</p>

<p align="center"> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="4" style="font-size: 119.5%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.153em; margin-left: -0.186em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c4C"></mjx-c><mjx-c class="mjx-c4C"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c56"></mjx-c><mjx-c class="mjx-c41"></mjx-c><mjx-c class="mjx-c2D"></mjx-c><mjx-c class="mjx-c4F"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c56"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c73"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.186em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c67"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c6C"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c5B"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="3"><mjx-c class="mjx-c1D458 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2B"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c31"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c5D"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>V</mi><mrow data-mjx-texclass="ORD"><mtext>LLaVA-OneVision</mtext></mrow></msub><mo>=</mo><msub><mi>V</mi><mrow data-mjx-texclass="ORD"><mtext>original</mtext></mrow></msub><mo>‚àó</mo><mo stretchy="false">[</mo><mo stretchy="false">(</mo><mi>k</mi><mo>√ó</mo><mi>k</mi><mo stretchy="false">)</mo><mo>+</mo><mn>1</mn><mo stretchy="false">]</mo></math></mjx-assistive-mml></mjx-container> </p>

<p>In a couple of words a big trade-off: <u>Massive detail vs. Massive token count.</u></p>

<p><strong>Strategy C: The Fixed Downsampler</strong>
Gemma3 (<a href="http://127.0.0.1:4000/blog/2026/demystifying1/#gemma-3-2025">Gemma-Team, 2025</a>) family of models, the most recent open source VLM from gdm also employes a fixed input sized Vision Encoder SigLIP (<a href="http://127.0.0.1:4000/blog/2026/demystifying1/#siglip-2024">Zhai et al., 2024</a>).</p>

<p>The main difference between their technique and <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#Strategy-a-The-Dynamic-Merger">Strategy A</a> is the easy but clever solution to handle higher resolution images, applying a spatial average pooling and therefore increasing the resizing input size of the model to 896. Thanks to the pooling, this yields a fixed amount of visual tokens which corresponds to</p>

<p align="center"> <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" display="true" tabindex="0" ctxtmenu_counter="5" style="font-size: 119.5%; position: relative;"><mjx-math display="true" class="MJX-TEX" aria-hidden="true" style="margin-left: 0px; margin-right: 0px;"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.186em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c47"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c33"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43B TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="3"><mjx-c class="mjx-c70"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-cD7"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D44A TEX-I"></mjx-c></mjx-mi><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D443 TEX-I"></mjx-c></mjx-mi><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D446 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo><mjx-mtext class="mjx-n" space="3"><mjx-c class="mjx-c70"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c67"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c38"></mjx-c><mjx-c class="mjx-c39"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn><mjx-texatom texclass="ORD"><mjx-mo class="mjx-n"><mjx-c class="mjx-c2F"></mjx-c></mjx-mo></mjx-texatom><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c><mjx-c class="mjx-c34"></mjx-c></mjx-mn><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2217"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="3"><mjx-c class="mjx-c34"></mjx-c></mjx-mn><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.413em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n" space="4"><mjx-c class="mjx-c32"></mjx-c><mjx-c class="mjx-c35"></mjx-c><mjx-c class="mjx-c36"></mjx-c></mjx-mn></mjx-math><mjx-assistive-mml unselectable="on" display="block"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><msub><mi>V</mi><mrow data-mjx-texclass="ORD"><mtext>Gemma3</mtext></mrow></msub><mo>=</mo><mo stretchy="false">(</mo><mi>H</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mi>P</mi><mi>S</mi><mo>‚àó</mo><mtext>pooling</mtext><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>√ó</mo><mo stretchy="false">(</mo><mi>W</mi><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mi>P</mi><mi>S</mi><mo>‚àó</mo><mtext>pooling</mtext><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo>=</mo><mo stretchy="false">(</mo><mn>896</mn><mrow data-mjx-texclass="ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mn>14</mn><mo>‚àó</mo><mn>4</mn><mo stretchy="false">)</mo><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>=</mo><mn>256</mn></math></mjx-assistive-mml></mjx-container> </p>

<p>with the pooling being applied within the modality connector.</p>

<p>A common denominator in all of these is the special tokens, which are added for every picturem signaling the beginning and end of the visual content.</p>

<p><a id="token-count"></a></p>
<div class="row mt-3">
  <div class="col-12">
    <div class="mx-auto text-center" style="width: 60%;">
      

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
    <img src="./visual_tokens_blogpost_files/token_comparison_paper_revised.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();">
  </picture>

  
</figure>

      <div class="caption text-center mt-2">
        Figure 2: We report the increase of Visual token count given the image resolution input. The image is assumed AnyRes assumes a 3x3 grid and the SMS and pooling are equal to 2. 
      </div>
    </div>
  </div>
</div>

<p><br></p>

<h2 id="visual-token-count-impact">Visual Token Count Impact</h2>

<p>We have defined what a Visual Token (VT) is and established formulas to calculate <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span></span></span></span></span> for different architectures.</p>

<p align="center"><code>But why does this specific number matter? Why should a machine learning engineer care if an image is represented by 256 tokens (Strategy C) or 2,500 tokens (Strategy B)?</code></p>

<p>The answer lies in the constraints of production environments: <strong>Context Windows</strong>, <strong>Latency</strong>, and <strong>VRAM</strong>.</p>

<p><strong>1. The Context Window Budget</strong></p>

<p>Every LLM has a hard limit on its input size, denoted as the Context Window (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>L</mi><mtext>max</mtext></msub></mrow><annotation encoding="application/x-tex">L_{\text{max}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.1514em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">max</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>). In text-only models, this budget is consumed solely by the system prompt, user history and input prompt. In VLMs, Visual Tokens consume this budget aggressively. If we denote the available context for reasoning and history as <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mtext>text</mtext></msub></mrow><annotation encoding="application/x-tex">C_{\text{text}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.0715em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.2806em;"><span class="" style="top: -2.55em; margin-left: -0.0715em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">text</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span>, the relationship is effectively zero-sum: <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="6" style="font-size: 119.5%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D436 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.045em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c78"></mjx-c><mjx-c class="mjx-c74"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-msub space="4"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D43F TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c6D"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c78"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="3"><mjx-c class="mjx-c2212"></mjx-c></mjx-mo><mjx-munderover space="3" limits="false"><mjx-mo class="mjx-sop"><mjx-c class="mjx-c2211 TEX-S1"></mjx-c></mjx-mo><mjx-script style="vertical-align: -0.285em; margin-left: 0px;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D441 TEX-I"></mjx-c></mjx-mi></mjx-texatom><mjx-spacer style="margin-top: 0.291em;"></mjx-spacer><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c3D"></mjx-c></mjx-mo><mjx-mn class="mjx-n"><mjx-c class="mjx-c31"></mjx-c></mjx-mn></mjx-texatom></mjx-script></mjx-munderover><mjx-msub space="2"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.186em;"><mjx-texatom size="s" texclass="ORD"><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D456 TEX-I"></mjx-c></mjx-mi></mjx-texatom></mjx-script></mjx-msub></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>C</mi><mrow data-mjx-texclass="ORD"><mtext>text</mtext></mrow></msub><mo>=</mo><msub><mi>L</mi><mrow data-mjx-texclass="ORD"><mtext>max</mtext></mrow></msub><mo>‚àí</mo><munderover><mo data-mjx-texclass="OP">‚àë</mo><mrow data-mjx-texclass="ORD"><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mrow data-mjx-texclass="ORD"><mi>N</mi></mrow></munderover><msub><mi>V</mi><mrow data-mjx-texclass="ORD"><mi>i</mi></mrow></msub></math></mjx-assistive-mml></mjx-container> 
where <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.109em;">N</span></span></span></span></span> is the number of images.</p>

<p>For a ‚ÄúStrategy B‚Äù model (like LLaVA-Next) using a <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>3</mn><mo>√ó</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">3 \times 3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">3</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">3</span></span></span></span></span> grid, a single image might consume <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚âà</mo><mn>2900</mn></mrow><annotation encoding="application/x-tex">\approx 2900</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.4831em;"></span><span class="mrel">‚âà</span><span class="mspace" style="margin-right: 0.2778em;"></span></span><span class="base"><span class="strut" style="height: 0.6444em;"></span><span class="mord">2900</span></span></span></span></span> tokens. If you are serving a model with a 4k or 8k context limit, a single image can consume 30-70% of your entire input capacity. 
This leaves little room for few-shot examples or long conversation history, potentially degrading the model‚Äôs ability to follow complex instructions.</p>

<p><strong>2. Inference Latency:</strong> The ‚ÄúPre-fill‚Äù BottleneckIn production use-cases, inference cost is often a function of latency. Large companies typically enforce fixed input sizes to ensure predictable response times. Visual Tokens disrupt this predictability. When a VLM processes a request, it undergoes two phases: 
a. Pre-fill: The model processes all input tokens (Text + Visual) in parallel to compute the Key-Value (KV) cache.
b. Decoding: The model generates the output one token at a time. 
Visual Tokens sit in the Pre-fill phase. A high VT count dramatically increases the Time To First Token (TTFT). <mjx-container class="MathJax CtxtMenu_Attached_0" jax="CHTML" tabindex="0" ctxtmenu_counter="7" style="font-size: 119.5%; position: relative;"><mjx-math class="MJX-TEX" aria-hidden="true"><mjx-msub><mjx-mtext class="mjx-n"><mjx-c class="mjx-c4C"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c63"></mjx-c><mjx-c class="mjx-c79"></mjx-c></mjx-mtext><mjx-script style="vertical-align: -0.239em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c70"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c66"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6C"></mjx-c><mjx-c class="mjx-c6C"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-mo class="mjx-n" space="4"><mjx-c class="mjx-c221D"></mjx-c></mjx-mo><mjx-mi class="mjx-i" space="4"><mjx-c class="mjx-c1D453 TEX-I"></mjx-c></mjx-mi><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-msub><mjx-mi class="mjx-i"><mjx-c class="mjx-c1D449 TEX-I"></mjx-c></mjx-mi><mjx-script style="vertical-align: -0.15em; margin-left: -0.186em;"><mjx-texatom size="s" texclass="ORD"><mjx-mtext class="mjx-n"><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c6C"></mjx-c></mjx-mtext></mjx-texatom></mjx-script></mjx-msub><mjx-msup><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo><mjx-script style="vertical-align: 0.363em;"><mjx-mn class="mjx-n" size="s"><mjx-c class="mjx-c32"></mjx-c></mjx-mn></mjx-script></mjx-msup><mjx-mstyle><mjx-mspace style="width: 1em;"></mjx-mspace></mjx-mstyle><mjx-mo class="mjx-n"><mjx-c class="mjx-c28"></mjx-c></mjx-mo><mjx-mtext class="mjx-n"><mjx-c class="mjx-c66"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c73"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c72"></mjx-c><mjx-c class="mjx-c64"></mjx-c><mjx-c class="mjx-c20"></mjx-c><mjx-c class="mjx-c61"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c65"></mjx-c><mjx-c class="mjx-c6E"></mjx-c><mjx-c class="mjx-c74"></mjx-c><mjx-c class="mjx-c69"></mjx-c><mjx-c class="mjx-c6F"></mjx-c><mjx-c class="mjx-c6E"></mjx-c></mjx-mtext><mjx-mo class="mjx-n"><mjx-c class="mjx-c29"></mjx-c></mjx-mo></mjx-math><mjx-assistive-mml unselectable="on" display="inline"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mtext>Latency</mtext><mrow data-mjx-texclass="ORD"><mtext>prefill</mtext></mrow></msub><mo>‚àù</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>V</mi><mrow data-mjx-texclass="ORD"><mtext>total</mtext></mrow></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mstyle scriptlevel="0"><mspace width="1em"></mspace></mstyle><mo stretchy="false">(</mo><mtext>for standard attention</mtext><mo stretchy="false">)</mo></math></mjx-assistive-mml></mjx-container>
Even with linear attention optimizations (like FlashAttention), processing 3,000 visual tokens requires significantly more floating-point operations (FLOPs) than processing 256. If your application requires real-time responsiveness (e.g., a voice assistant ‚Äúseeing‚Äù via camera), the difference between 50ms and 500ms in pre-fill latency is a dealbreaker.</p>

<p><a id="high_res"></a></p>
<div class="row mt-3">
  <div class="col-12">
    <div class="mx-auto text-center" style="width: 60%;">
      

<figure>
  <picture>
    <!-- Auto scaling with imagemagick -->
    <!--
      See https://www.debugbear.com/blog/responsive-images#w-descriptors-and-the-sizes-attribute and
      https://developer.mozilla.org/en-US/docs/Learn/HTML/Multimedia_and_embedding/Responsive_images for info on defining 'sizes' for responsive images
    -->
    
    <img src="./visual_tokens_blogpost_files/apple_latency_vlms.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $(&#39;.responsive-img-srcset&#39;).remove();">
  </picture>

  
</figure>

      <div class="caption text-center mt-2">
        Figure 3: Figure from <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#fastvlms-2025">FastVLM</a> paper from Apple MLR, illustrating how Vision latency dominates at high resolution. Breakdown of FastVLM‚Äôs time to the first token for different image resolutions. The vision encoder is <a href="http://127.0.0.1:4000/blog/2026/demystifying1/#fastvlms-2025">FastViT-HD</a>, and the LLM has 1.5B parameters. 
      </div>
    </div>
  </div>
</div>

<p><strong>3. The Cascading Impact on VRAM</strong>
Perhaps the most critical ‚Äúhidden‚Äù cost is memory. When serving models using high-performance engines like <a href="https://github.com/vllm-project/vllm" rel="external nofollow noopener" target="_blank">vLLM</a> or <a href="https://developer.nvidia.com/tensorrt-llm" rel="external nofollow noopener" target="_blank">TensorRT-LLM</a>, the throughput is bound by how many requests can fit into the GPU memory simultaneously (batch size).
This depends heavily on the KV Cache‚Äîthe memory required to store the attention context for every token in the sequence. Higher VT Count <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.3669em;"></span><span class="mrel">‚Üí</span></span></span></span></span> Larger KV Cache per request.Larger KV Cache <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.3669em;"></span><span class="mrel">‚Üí</span></span></span></span></span> Fewer requests fit in VRAM. Fewer Requests <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚Üí</mo></mrow><annotation encoding="application/x-tex">\rightarrow</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.3669em;"></span><span class="mrel">‚Üí</span></span></span></span></span> Smaller Batch Size.T his creates a cascading effect on cost.</p>

<p>If a high-resolution strategy increases your visual tokens by <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mo>√ó</mo></mrow><annotation encoding="application/x-tex">10\times</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7278em; vertical-align: -0.0833em;"></span><span class="mord">10</span><span class="mord">√ó</span></span></span></span></span>, you might be forced to reduce your batch size by roughly the same factor to avoid Out-Of-Memory (OOM) errors. This effectively multiplies your cost per inference, as you need more GPUs to handle the same traffic.4. Operational NecessityThis brings us back to the premise of this post. For experimental research, we often ignore these overheads in favor of higher benchmarks. But for production deployment, <em>predictability is king</em>. To optimize serving, engineers need to:</p>
<ul>
  <li>Dynamically adjust expectations: If a client uploads a high-res image to a ‚ÄúStrategy A‚Äù (Dynamic) model, the system must instantly calculate <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mtext>Qwen</mtext></msub></mrow><annotation encoding="application/x-tex">V_{\text{Qwen}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.9694em; vertical-align: -0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3283em;"><span class="" style="top: -2.55em; margin-left: -0.2222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">Qwen</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.2861em;"><span class=""></span></span></span></span></span></span></span></span></span></span> to check if it fits the current VRAM budget.</li>
  <li>Downsample intelligently: If <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mtext>calculated</mtext></msub></mrow><annotation encoding="application/x-tex">V_{\text{calculated}}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8333em; vertical-align: -0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right: 0.2222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.3361em;"><span class="" style="top: -2.55em; margin-left: -0.2222em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord text mtight"><span class="mord mtight">calculated</span></span></span></span></span></span><span class="vlist-s">‚Äã</span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span></span></span> exceeds the limit, the system needs to resize the input image before it hits the Vision Encoder to meet a specific token target.
This is why the formulas in Part 2 are not just theoretical trivia‚Äîthey are essential tools for building robust, cost-effective Multimodal systems.</li>
</ul>

<h2 id="conclusions--key-takeaways">Conclusions &amp; Key Takeaways</h2>

<div class="row mt-3">
    <div class="col-12">
        <div class="table-responsive">
            <table class="table table-hover table-bordered table-dark" data-toggle="table">
                <thead class="thead-light">
                    <tr>
                        <th scope="col">Representative Models</th>
                        <th scope="col">Strategy</th>
                        <th scope="col">Resolution Logic</th>
                        <th scope="col">Token Efficiency</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td class="font-weight-bold">LLaVA1.5</td>
                        <td>Standard Resize</td>
                        <td>Squash to fixed <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>√ó</mo><mi>W</mi></mrow><annotation encoding="application/x-tex">H \times W</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7667em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0813em;">H</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6833em;"></span><span class="mord mathnormal" style="margin-right: 0.1389em;">W</span></span></span></span></span></td>
                        <td><span class="badge badge-secondary">Fixed Count</span></td>
                    </tr>
                    <tr>
                        <td class="font-weight-bold">Qwen3-VL</td>
                        <td>Dynamic Merger</td>
                        <td>Native (Preserves Aspect Ratio)</td>
                        <td><span class="badge badge-warning">Linear Growth</span></td>
                    </tr>
                    <tr>
                        <td class="font-weight-bold">LLaVA-OneVision</td>
                        <td>AnyRes / Multi-Grid</td>
                        <td>Grid Split (<span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>√ó</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">k \times k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.7778em; vertical-align: -0.0833em;"></span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span><span class="mspace" style="margin-right: 0.2222em;"></span><span class="mbin">√ó</span><span class="mspace" style="margin-right: 0.2222em;"></span></span><span class="base"><span class="strut" style="height: 0.6944em;"></span><span class="mord mathnormal" style="margin-right: 0.0315em;">k</span></span></span></span></span>) + Overview</td>
                        <td><span class="badge badge-danger">High Cost</span></td>
                    </tr>
                    <tr>
                        <td class="font-weight-bold">Gemma3</td>
                        <td>Fixed Downsampler</td>
                        <td>Resize + Spatial Pooling</td>
                        <td><span class="badge badge-success">Highly Compact</span></td>
                    </tr>
                </tbody>
            </table>
        </div>
        <div class="caption text-center mt-2">
            Table 1: We report a comparison of Visual Token calculation strategies across modern architectures in current SOTA VLMs.
        </div>
    </div>
</div>

<p>Visual Tokens (VT) are the bridge between the image and language world, but they are also the primary bottleneck in VLM deployment. As we have seen, moving from a fixed-resolution model (like Gemma 3) to a dynamic one (like Qwen 2.5-VL or LLaVA-Next) can increase your input size by an order of magnitude.Here are the key takeaways to keep in mind when building multimodal systems:</p>
<ul>
  <li>Tokens <span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo mathvariant="normal">‚â†</mo></mrow><annotation encoding="application/x-tex">\neq</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="mrel"><span class="mrel"><span class="mord vbox"><span class="thinbox"><span class="rlap"><span class="strut" style="height: 0.8889em; vertical-align: -0.1944em;"></span><span class="inner"><span class="mord"><span class="mrel">ÓÄ†</span></span></span><span class="fix"></span></span></span></span></span><span class="mrel">=</span></span></span></span></span></span> Pixels: High resolution doesn‚Äôt always mean high cost. It depends entirely on the architecture (e.g., Fixed Downsampler vs. Multi-Grid).</li>
  <li>The ‚ÄúPre-fill‚Äù Trap: Visual tokens are processed before the first word is generated. If your latency is high, check your image resolution before checking your LLM size.Context is Zero-Sum: Every visual token you use is one less token available for conversation history or few-shot examples.</li>
  <li>Calculate, Don‚Äôt Guess: Use the formulas provided in Part 2 to pre-calculate token counts. This allows you to dynamically resize images or adjust batch sizes to prevent OOM errors in production.</li>
</ul>

<p>Multimodal learning is evolving rapidly, but the fundamental constraint remains: compute is finite. Mastering the math of Visual Tokens is the first step toward mastering VLM efficiency.</p>

<!-- ## Additional Questions to Answer -->

<!-- D. What is the cost trade-off? -->

<!-- Context: LLaVA-OneVision uses many more tokens. -->

<!-- Question: "If Model A uses 256 tokens and Model B uses 2,800 tokens for the same image, what is the impact on 'Time to First Token' (latency) and VRAM?" -->

<h2 id="citation">Citation</h2>

<p>If you use this work, please cite:</p>

<div class="code-display-wrapper"><pre><code class="language-bibtex">@misc{nulli2026demistifying,
title={De-mystifying Multimodal Learning: The Hidden Cost in Vision Language Modelling.},
author={Matteo Nulli},
year={2026},
url={https://matteonulli.github.io/blog/2026/demystifying1/}
}
</code></pre><button class="copy" type="button" aria-label="Copy code to clipboard"><i class="fa-solid fa-clipboard"></i></button></div>

<p><br></p>

<p><strong>References</strong></p>

<p><a id="conme-2024">Huang Irene, Lin Wei, Mirza M. Jehanzeb, Hansen Jacob A., Doveh Sivan, Butoi Victor Ion, Herzig Roei, Arbelle Assaf, Kuehne Hilde, Darrell Trevor, Gan Chuang, Oliva Aude, Feris Rogerio, Karlinsky Leonid. (2024). Conme: Rethinking Evaluation of Compositional Reasoning for Modern VLMs. arXiv preprint arXiv:2406.08164.</a></p>

<p><a id="eyes-wide-shut-2024">Tong Shengbang, Liu Zhuang, Zhai Yuexiang, Ma Yi, LeCun Yann, Xie Saining. (2024). Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. arXiv preprint arXiv:2401.06209.</a></p>

<p><a id="visual-instruction-tuning-2023">Liu Haotian, Li Chunyuan, Wu Qingyang, Lee Yong Jae. (2023). Visual Instruction Tuning. arXiv preprint arXiv:2304.08485.</a></p>

<p><a id="llavonevision-2024">Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llava-onevision: Easy visual task transfer. Preprint,
arXiv:2408.03326.</a></p>

<p><a id="qwen2-5-vl-2025">Bai Shuai, Chen Keqin, Liu Xuejing, Wang Jialin, Ge Wenbin, Song Sibo, Dang Kai, Wang Peng, Wang Shijie, Tang Jun, Zhong Humen, Zhu Yuanzhi, Yang Mingkun, Li Zhaohai, Wan Jianqiang, Wang Pengfei, Ding Wei, Fu Zheren, Xu Yiheng, Ye Jiabo, Zhang Xi, Xie Tianbao, Cheng Zesen, Zhang Hang, Yang Zhibo, Xu Haiyang, Lin Junyang. (2025). Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923.</a></p>

<p><a id="qwen3-vl-2025">QwenTeam. 2025. Qwen3-vl: Sharper vision, deeper
thought, broader action.</a></p>

<p><a id="internvl2-2024">OpenGVLab-Team. (2024). InternVL2: Better Than the Best‚ÄîExpanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy. Blog post. URL https://internvl.github.io/blog/2024-07-02-InternVL-2.0/.</a></p>

<p><a id="gemma-3-2025">Gemma-Team. (2025). Gemma 3 Technical Report. arXiv preprint arXiv:2503.19786.</a></p>

<p><a id="bags-of-words-vlms-2023">Yuksekgonul Mert, Bianchi Federico, Kalluri Pratyusha, Jurafsky Dan, Zou James. (2023). When and Why Vision-Language Models Behave Like Bags-of-Words, and What to Do About It? arXiv preprint arXiv:2210.01936.</a></p>

<p><a id="icl-compositional-vlms-2024">Nulli Matteo, Ibrahimi Anesa, Pal Avik, Lee Hoshe, Najdenkoska Ivona. (2024). In-Context Learning Improves Compositional Understanding of Vision-Language Models. In ICML 2024 Workshop on Foundation Models in the Wild. arXiv preprint arXiv:2407.15487.</a></p>

<p><a id="nulliobjectguided-2025">Matteo Nulli, Ivona Najdenkoska, Mohammad Mahdi Derakhshani, and Yuki M Asano. 2025. Objectguided visual tokens: Eliciting compositional reasoning in multimodal language models. In EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)</a></p>

<p><a id="vismin-2025">Awal Rabiul, Ahmadi Saba, Zhang Le, Agrawal Aishwarya. (2025). Vismin: Visual Minimal-Change Understanding. arXiv preprint arXiv:2407.16772.</a></p>

<p><a id="cambrian-1-2024">Tong Shengbang, Brown Ellis, Wu Penghao, Woo Sanghyun, Middepogu Manoj, Akula Sai Charitha, Yang Jihan, Yang Shusheng, Iyer Adithya, Pan Xichen, Wang Austin, Fergus Rob, LeCun Yann, Xie Saining. (2024). Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs. arXiv preprint arXiv:2406.16860.</a></p>

<p><a id="llava-next-2024">Liu Haotian, Li Chunyuan, Li Yuheng, Li Bo, Zhang Yuanhan, Shen Sheng, Lee Yong Jae. (2024). LLaVA-NeXT: Improved Reasoning, OCR, and World Knowledge. Blog post (January 2024). URL https://llava-vl.github.io/blog/2024-01-30-llava-next/.</a></p>

<p><a id="sam-2-2024">Ravi Nikhila, Gabeur Valentin, Hu Yuan-Ting, Hu Ronghang, Ryali Chaitanya, Ma Tengyu, Khedr Haitham, R√§dle Roman, Rolland Chloe, Gustafson Laura, Mintun Eric, Pan Junting, Alwala Kalyan Vasudev, Carion Nicolas, Wu Chao-Yuan, Girshick Ross, Doll√°r Piotr, Feichtenhofer Christoph. (2024). SAM 2: Segment Anything in Images and Videos. arXiv preprint arXiv:2408.00714.</a></p>

<p><a id="omg-seg-cvpr-2024">Li Xiangtai, Yuan Haobo, Li Wei, Ding Henghui, Wu Size, Zhang Wenwei, Li Yining, Chen Kai, Loy Chen Change. (2024). OMG-Seg: Is One Model Good Enough for All Segmentation? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 27948‚Äì27959.</a></p>

<p><a id="eagle-2-5-2025">Chen Guo, Li Zhiqi, Wang Shihao, Jiang Jindong, Liu Yicheng, Lu Lidong, Huang De-An, Byeon Wonmin, Le Matthieu, Rintamaki Tuomas, Poon Tyler, Ehrlich Max, Lu Tong, Wang Limin, Catanzaro Bryan, Kautz Jan, Tao Andrew, Yu Zhiding, Liu Guilin. (2025). EAGLE 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models. arXiv preprint arXiv:2504.15271.</a></p>

<p><a id="omg-llava-2024">Zhang Tao, Li Xiangtai, Fei Hao, Yuan Haobo, Wu Shengqiong, Ji Shunping, Loy Chen Change, Yan Shuicheng. (2024). OMG-LLaVA: Bridging Image-Level, Object-Level, Pixel-Level Reasoning and Understanding. arXiv preprint arXiv:2406.19389.</a></p>

<p><a id="sa2va-2025">Yuan Haobo, Li Xiangtai, Zhang Tao, Huang Zilong, Xu Shilin, Ji Shunping, Tong Yunhai, Qi Lu, Feng Jiashi, Yang Ming-Hsuan. (2025). SA2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos. arXiv preprint arXiv:2501.04001.</a></p>

<p><a id="clip-2021">Radford Alec, Kim Jong Wook, Hallacy Chris, Ramesh Aditya, Goh Gabriel, Agarwal Sandhini, Sastry Girish, Askell Amanda, Mishkin Pamela, Clark Jack, Krueger Gretchen, Sutskever Ilya. (2021). Learning Transferable Visual Models from Natural Language Supervision. arXiv preprint arXiv:2103.00020.</a></p>

<p><a id="improved-vit-baselines-2024">Liu Haotian, Li Chunyuan, Li Yuheng, Lee Yong Jae. (2024). Improved Baselines with Visual Instruction Tuning. arXiv preprint arXiv:2310.03744.</a></p>

<p><a id="vit-2021">Dosovitskiy Alexey, Beyer Lucas, Kolesnikov Alexander, Weissenborn Dirk, Zhai Xiaohua, Unterthiner Thomas, Dehghani Mostafa, Minderer Matthias, Heigold Georg, Gelly Sylvain, Uszkoreit Jakob, Houlsby Neil. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010.11929.</a></p>

<p><a id="llama-2-2023">Touvron Hugo, Martin Louis, Stone Kevin, Albert Peter, Almahairi Amjad, Babaei Yasmine, Bashlykov Nikolay, Batra Soumya, Bhargava Prajjwal, Bhosale Shruti, Bikel Dan, Blecher Lukas, Canton Ferrer Cristian, Chen Moya, Cucurull Guillem, Esiobu David, Fernandes Jude, Fu Jeremy, Fu Wenyin, Fuller Brian, Gao Cynthia, Goswami Vedanuj, Goyal Naman, Hartshorn Anthony, Hosseini Saghar, Hou Rui, Inan Hakan, Kardas Marcin, Kerkez Viktor, Khabsa Madian, Kloumann Isabel, Korenev Artem, Koura Punit Singh, Lachaux Marie-Anne, Lavril Thibaut, Lee Jenya, Liskovich Diana, Lu Yinghai, Mao Yuning, Martinet Xavier, Mihaylov Todor, Mishra Pushkar, Molybog Igor, Nie Yixin, Poulton Andrew, Reizenstein Jeremy, Rungta Rashi, Saladi Kalyan, Schelten Alan, Silva Ruan, Smith Eric Michael, Subramanian Ranjan, Tan Xiao-qing Ellen, Tang Binh, Taylor Ross, Williams Adina, Kuan Jian Xiang, Xu Puxin, Yan Zheng, Zarov Iliyan, Zhang Yuchen, Fan Angela, Kambadur Melanie, Narang Sharan, Rodriguez Aurelien, Stojnic Robert, Edunov Sergey, Scialom Thomas. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models.</a></p>

<p><a id="llama-3-2-2024">Meta. (2024). Llama 3.2: Revolutionizing Edge AI and Vision with Open, Customizable Models. Blog post. URL https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.</a></p>

<p><a id="lora-2021">Hu Edward J., Shen Yelong, Wallis Phillip, Allen-Zhu Zeyuan, Li Yuanzhi, Wang Shean, Wang Lu, Chen Weizhu. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.</a></p>

<p><a id="coco-2014">Lin Tsung-Yi, Maire Michael, Belongie Serge, Hays James, Perona Pietro, Ramanan Deva, Doll√°r Piotr, Zitnick C. Lawrence. (2014). Microsoft COCO: Common Objects in Context. In Computer Vision ‚Äì ECCV 2014, pages 740‚Äì755. Springer.</a></p>

<p><a id="image-descriptions-2014">Young Peter, Lai Alice, Hodosh Micah, Hockenmaier Julia. (2014). From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference Over Event Descriptions. Transactions of the Association for Computational Linguistics, 2:67‚Äì78.</a></p>

<p><a id="visual-genome-2017">Krishna Ranjay, Zhu Yuke, Groth Oliver, Johnson Justin, Hata Kenji, Kravitz Joshua, Chen Stephanie, Kalantidis Yannis, Li Li-Jia, Shamma David A., et al. (2017). Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision, 123:32‚Äì73.</a></p>

<p><a id="gqa-2019">Hudson Drew A., Manning Christopher D. (2019). GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6700‚Äì6709.</a></p>

<p><a id="sugar-crepe-2023">Hsieh Cheng-Yu, Zhang Jieyu, Ma Zixian, Kembhavi Aniruddha, Krishna Ranjay. (2023). SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality. Advances in Neural Information Processing Systems, 36:31096‚Äì31116.</a></p>

<p><a id="gpt-4-technical-report-2024">OpenAI. (2024). GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.</a></p>

<p><a id="scaling-instruction-finetuned-2024">Chung Hyung Won, Hou Le, Longpre Shayne, Zoph Barret, Tay Yi, Fedus William, Li Yunxuan, Wang Xuezhi, Dehghani Mostafa, Brahma Siddhartha, et al. (2024). Scaling Instruction-Finetuned Language Models. Journal of Machine Learning Research, 25(70):1‚Äì53.</a></p>

<p><a id="diagram-2016">Kembhavi Aniruddha, Salvato Mike, Kolve Eric, Seo Minjoon, Hajishirzi Hannaneh, Farhadi Ali. (2016). A Diagram is Worth a Dozen Images. arXiv preprint arXiv:1603.07396.</a></p>

<p><a id="mme-2024">Fu Chaoyou, Chen Peixian, Shen Yunhang, Qin Yulei, Zhang Mengdan, Lin Xu, Yang Jinrui, Zheng Xiawu, Li Ke, Sun Xing, Wu Yunsheng, Ji Rongrong. (2024). MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv preprint arXiv:2306.13394.</a></p>

<p><a id="evaluating-vlms-right-way-2024">Chen Lin, Li Jinsong, Dong Xiaoyi, Zhang Pan, Zang Yuhang, Chen Zehui, Duan Haodong, Wang Jiaqi, Qiao Yu, Lin Dahua, Zhao Feng. (2024). Are We on the Right Way for Evaluating Large Vision-Language Models? arXiv preprint arXiv:2403.20330.</a></p>

<p><a id="mmbench-2024">Liu Yuan, Duan Haodong, Zhang Yuanhan, Li Bo, Zhang Songyang, Zhao Wangbo, Yuan Yike, Wang Jiaqi, He Conghui, Liu Ziwei, Chen Kai, Lin Dahua. (2024). MMBench: Is Your Multi-Modal Model an All-Around Player? arXiv preprint arXiv:2307.06281.</a></p>

<p><a id="subobject-tokenization-2025">Chen Delong, Cahyawijaya Samuel, Liu Jianfeng, Wang Baoyuan, Fung Pascale. (2025). Subobject-Level Image Tokenization. arXiv preprint arXiv:2402.14327.</a></p>

<p><a id="deepspeed-2020">Rasley Jeff, Rajbhandari Samyam, Ruwase Olatunji, He Yuxiong. (2020). DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD ‚Äô20), pages 3505‚Äì3506. doi:10.1145/3394486.3406703.</a></p>

<p><a id="zero-2020">Rajbhandari Samyam, Rasley Jeff, Ruwase Olatunji, He Yuxiong. (2020). ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1‚Äì16. doi:10.1109/SC41405.2020.00024.</a></p>

<p><a id="adam-2017">Kingma Diederik P., Ba Jimmy. (2017). Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980.</a></p>

<p><a id="adamw-2019">Loshchilov Ilya, Hutter Frank. (2019). Decoupled Weight Decay Regularization. arXiv preprint arXiv:1711.05101.</a></p>

<p><a id="bert-2019">Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina. (2019). BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT 2019, pages 4171‚Äì4186.</a></p>

<p><a id="attention-is-all-you-need-2017">Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob, Jones Llion, Gomez Aidan N., Kaiser ≈Åukasz, Polosukhin Illia. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems, 30.</a></p>

<p><a id="pixtral-12b-2024">Agrawal Pravesh, Antoniak Szymon, Bou Hanna Emma, Bout Baptiste, Chaplot Devendra, Chudnovsky Jessica, Costa Diogo, De Monicault Baudouin, Garg Saurabh, Gervet Theophile, Ghosh Soham, H√©liou Am√©lie, Jacob Paul, Jiang Albert Q., Khandelwal Kartik, Lacroix Timoth√©e, Lample Guillaume, Las Casas Diego, Lavril Thibaut, Le Scao Teven, Lo Andy, Marshall Louis, Martin Arthur, Mensch Arthur, Muddireddy Pavankumar, Nemychnikova Valera, Pellat Marie, Von Platen Patrick, Raghuraman Nikhil, Bout Rozi√®re Baptiste, Sablayrolles Alexandre, Saulnier Lucile, Sauvestre Romain, Rozi√®re Baptiste, Shang Wendy, Soletskyi Roman, Stewart Lawrence, Stock Pierre, Studnia Joachim, Subramanian Sandeep, Vaze Sagar, Wang Thomas, Yang Sophia. (2024). Pixtral 12B. arXiv preprint arXiv:2410.07073.</a></p>

<p><a id="roformer-2023">Su Jianlin, Lu Yu, Pan Shengfeng, Murtadha Ahmed, Wen Bo, Liu Yunfeng. (2023). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint arXiv:2104.09864.</a></p>

<p><a id="blip2-2023">Li J, Li D, Savarese S, Hoi S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. InInternational conference on machine learning 2023 Ju</a><a></a></p>

<p><a id="llama-3-herd-2024">Dubey Abhimanyu, et al. (2024). The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783.</a></p>

<p><a id="reproducible-scaling-laws-2023">Cherti Mehdi, Beaumont Romain, Wightman Ross, Wortsman Mitchell, Ilharco Gabriel, Gordon Cade, Schuhmann Christoph, Schmidt Ludwig, Jitsev Jenia. (2023). Reproducible Scaling Laws for Contrastive Language-Image Learning. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818‚Äì2829. doi:10.1109/CVPR52729.2023.00276.</a></p>

<p><a id="sigmoid-loss-2023">Zhai Xiaohua, Mustafa Basil, Kolesnikov Alexander, Beyer Lucas. (2023). Sigmoid Loss for Language Image Pre-Training. arXiv preprint arXiv:2303.15343.</a></p>

<p><a id="dinov2-2024">Oquab Maxime, Darcet Timoth√©e, Moutakanni Th√©o, Vo Huy, Szafraniec Marc, Khalidov Vasil, Fernandez Pierre, Haziza Daniel, Massa Francisco, El-Nouby Alaaeldin, Assran Mahmoud, Ballas Nicolas, Galuba Wojciech, Misra Ishan, Rabbat Michael, Sharma Vasu, Synnaeve Gabriel, Xu Hu, Jegou Herv√©, Mairal Julien, Labatut Patrick, Joulin Armand, Bojanowski Piotr. (2024). DINOv2: Learning Robust Visual Features Without Supervision. arXiv preprint arXiv:2304.07193.</a></p>

<p><a id="internlm2-2024">Cai Zheng, Cao Maosong, Chen Haojiong, Chen Kai, Chen Keyu, Chen Xin, Chen Xun, Chen Zehui, Chen Zhi, Chu Pei, Dong Xiaoyi, Duan Haodong, Fan Qi, Fei Zhaoye, Gao Yang, Ge Jiaye, Gu Chenya, Gu Yuzhe, Gui Tao, Guo Aijia, Guo Qipeng, He Conghui, Hu Yingfan, Huang Ting, Jiang Tao, Jiao Penglong, Jin Zhenjiang, Lei Zhikai, Li Jiaxing, Li Jingwen, Li Linyang, Li Shuaibin, Li Wei, Li Yining, Liu Hongwei, Liu Jiawei, Liu Kaiwen, Liu Kuikun, Liu Xiaoran, Lv Chengqi, Lv Haijun, Lv Kai, Ma Li, Ma Runyuan, Ma Zerun, Ning Wenchang, Ouyang Linke, Qiu Jiantao, Qu Yuan, Shang Fukai, Shao Yunfan, Song Demin, Song Zifan, Sui Zhihao, Sun Peng, Sun Yu, Tang Huanze, Wang Bin, Wang Guoteng, Wang Jiaqi, Wang Jiayu, Wang Rui, Wang Yudong, Wang Ziyi, Wei Xingjian, Weng Qizhen, Wu Fan, Xiong Yingtong, Xu Chao, Xu Ruiliang, Yan Hang, Yan Yirong, Yang Xiaogui, Ye Haochen, Ying Huaiyuan, Yu Jia, Yu Jing, Zang Yuhang, Zhang Chuyu, Zhang Li, Zhang Pan, Zhang Peng, Zhang Ruijie, Zhang Shuo, Zhang Songyang, Zhang Wenjian, Zhang Wenwei, Zhang Xingcheng, Zhang Xinyue, Zhao Hui, Zhao Qian, Zhao Xiaomeng, Zhao Fengzhe, Zhou Zaida, Zhou Jingming, Zhuo Jingming, Zou Yicheng, Qiu Xipeng, Qiao Yu, Lin Dahua. (2024). InternLM2 Technical Report. arXiv preprint arXiv:2403.17297.</a></p>

<p><a id="omg-seg-arxiv-2024">Li Xiangtai, Yuan Haobo, Li Wei, Ding Henghui, Wu Size, Zhang Wenwei, Li Yining, Chen Kai, Loy Chen Change. (2024). OMG-Seg: Is One Model Good Enough for All Segmentation? arXiv preprint arXiv:2401.10229.</a></p>

<p><a id="seem-2023">Zou Xueyan, Yang Jianwei, Zhang Hao, Li Feng, Li Linjie, Wang Jianfeng, Wang Lijuan, Gao Jianfeng, Lee Yong Jae. (2023). Segment Everything Everywhere All at Once. arXiv preprint arXiv:2304.06718.</a></p>

<p><a id="siglip-2024">Xiaohua Zhai,Basil Mustafa,Alexander Kolesnikov,Lucas Beyer. Sigmoid Loss for Language Image Pre-Training, 2024. URL https://arxiv.org/abs/2303.15343.</a></p>

<p><a id="fastvlms-2025">Vasu, Pavan Kumar Anasosalu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam et al. ‚ÄúFastvlm: Efficient vision encoding for vision language models.‚Äù In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 19769-19780. 2025.</a></p>

    </div>
  </article>

  

  

  
    
      

  
    
    <br>
    <hr>
    <br>
    <ul class="list-disc pl-8"></ul>

    <!-- Adds related posts to the end of an article -->
    <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2>
    <p class="mb-2">Here are some more articles you might like to read next:</p>
  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="http://127.0.0.1:4000/blog/2025/ogllava/">Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="http://127.0.0.1:4000/blog/2024/earlyexit/">Optimizing Predictions: Vocabulary Reduction and Contrastive Decoding in LLMs</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="http://127.0.0.1:4000/blog/2025/cvar/">Perception, Localization, Planning and Control 
 on RAE Robots</a>
  </li>

  

  <li class="my-2">
    <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="http://127.0.0.1:4000/blog/2024/alma/">Model Compression for Machine Translation in Large Language Models</a>
  </li>


    
  

  
  
</div>

      
    </div>

    <!-- Footer -->
    
  <footer class="fixed-bottom" role="contentinfo">
    <div class="container mt-0">
      ¬© Copyright 2026
      Matteo
      
      Nulli. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      
      
    </div>
  </footer>



    <!-- JavaScripts -->
    <!-- jQuery -->
<script src="./visual_tokens_blogpost_files/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
<script src="./visual_tokens_blogpost_files/bootstrap.bundle.min.js"></script>
<!-- <script src="/assets/js/mdb.min.js"></script> -->
<script src="./visual_tokens_blogpost_files/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    
  <!-- Masonry & imagesLoaded -->
  <script defer="" src="./visual_tokens_blogpost_files/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer="" src="./visual_tokens_blogpost_files/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
  <script defer="" src="./visual_tokens_blogpost_files/masonry.js" type="text/javascript"></script>


    

    

    

    

    

    

    

    

    

  <!-- Medium Zoom JS -->
  <script defer="" src="./visual_tokens_blogpost_files/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script>
  <script defer="" src="./visual_tokens_blogpost_files/zoom.js"></script>



<!-- Bootstrap Table -->


<!-- Load Common JS -->
<script src="./visual_tokens_blogpost_files/no_defer.js"></script>
<script defer="" src="./visual_tokens_blogpost_files/common.js"></script>
<script defer="" src="./visual_tokens_blogpost_files/copy_code.js" type="text/javascript"></script>

<!-- Jupyter Open External Links New Tab -->
<script defer="" src="./visual_tokens_blogpost_files/jupyter_new_tab.js"></script>



    
  <script async="" src="./visual_tokens_blogpost_files/embed.js"></script>


  <script async="" src="./visual_tokens_blogpost_files/badge.js"></script>


    
  
    <!-- MathJax -->
    <script type="text/javascript">
      window.MathJax = {
        tex: {
          tags: 'ams',
        },
      };
    </script>
    <script defer="" type="text/javascript" id="MathJax-script" src="./visual_tokens_blogpost_files/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script>
    <script defer="" src="./visual_tokens_blogpost_files/polyfill.min.js" crossorigin="anonymous"></script>
  


    

    


    
  <!-- Scrolling Progress Bar -->
  <script type="text/javascript">
    /*
     * This JavaScript code has been adapted from the article
     * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar,
     * published on the website https://css-tricks.com on the 7th of May, 2014.
     * Couple of changes were made to the original code to make it compatible
     * with the `al-foio` theme.
     */
    const progressBar = $('#progress');
    /*
     * We set up the bar after all elements are done loading.
     * In some cases, if the images in the page are larger than the intended
     * size they'll have on the page, they'll be resized via CSS to accomodate
     * the desired size. This mistake, however, breaks the computations as the
     * scroll size is computed as soon as the elements finish loading.
     * To account for this, a minimal delay was introduced before computing the
     * values.
     */
    window.onload = function () {
      setTimeout(progressBarSetup, 50);
    };
    /*
     * We set up the bar according to the browser.
     * If the browser supports the progress element we use that.
     * Otherwise, we resize the bar thru CSS styling
     */
    function progressBarSetup() {
      if ('max' in document.createElement('progress')) {
        initializeProgressElement();
        $(document).on('scroll', function () {
          progressBar.attr({ value: getCurrentScrollPosition() });
        });
        $(window).on('resize', initializeProgressElement);
      } else {
        resizeProgressBar();
        $(document).on('scroll', resizeProgressBar);
        $(window).on('resize', resizeProgressBar);
      }
    }
    /*
     * The vertical scroll position is the same as the number of pixels that
     * are hidden from view above the scrollable area. Thus, a value > 0 is
     * how much the user has scrolled from the top
     */
    function getCurrentScrollPosition() {
      return $(window).scrollTop();
    }

    function initializeProgressElement() {
      let navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      progressBar.css({ top: navbarHeight });
      progressBar.attr({
        max: getDistanceToScroll(),
        value: getCurrentScrollPosition(),
      });
    }
    /*
     * The offset between the html document height and the browser viewport
     * height will be greater than zero if vertical scroll is possible.
     * This is the distance the user can scroll
     */
    function getDistanceToScroll() {
      return $(document).height() - $(window).height();
    }

    function resizeProgressBar() {
      progressBar.css({ width: getWidthPercentage() + '%' });
    }
    // The scroll ratio equals the percentage to resize the bar
    function getWidthPercentage() {
      return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
    }
  </script>


    

    

    

    <script src="./visual_tokens_blogpost_files/vanilla-back-to-top.min.js"></script>
<script>
  
    addBackToTop();
  
</script><div id="back-to-top" class="hidden"><svg viewBox="0 0 24 24"><path d="M7.41 15.41L12 10.83l4.59 4.58L18 14l-6-6-6 6z"></path></svg></div>

    
  <script type="module" src="./visual_tokens_blogpost_files/ninja-keys.min.js"></script>
  <ninja-keys hidebreadcrumbs="" noautoloadmdicons="" placeholder="Type to start searching" class="dark"><template shadowrootmode="open"><!---->
      <div class=" modal ">
        <div class=" bump modal-content ">
          <ninja-header exportparts="ninja-input,ninja-input-wrapper"><template shadowrootmode="open"><!---->
      <!--?lit$397725433$-->
      <div part="ninja-input-wrapper" class="search-wrapper">
        <input part="ninja-input" type="text" id="search" spellcheck="false" autocomplete="off" class="search" placeholder="Type to start searching">
      </div>
    </template>
          </ninja-header>
          <div class="modal-body">
            <div class="loading-indicator">
              <span class="bar1"></span>
              <span class="bar2"></span>
            </div>
            <div class="actions-list" part="actions-list"><!--?lit$397725433$--></div>
          </div>
          <slot name="footer"> <!--?lit$397725433$--> <div class="modal-footer" slot="footer">
  <span class="help">
    <svg version="1.0" class="ninja-examplekey" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 1280 1280">
      <path d="M1013 376c0 73.4-.4 113.3-1.1 120.2a159.9 159.9 0 0 1-90.2 127.3c-20 9.6-36.7 14-59.2 15.5-7.1.5-121.9.9-255 1h-242l95.5-95.5 95.5-95.5-38.3-38.2-38.2-38.3-160 160c-88 88-160 160.4-160 161 0 .6 72 73 160 161l160 160 38.2-38.3 38.3-38.2-95.5-95.5-95.5-95.5h251.1c252.9 0 259.8-.1 281.4-3.6 72.1-11.8 136.9-54.1 178.5-116.4 8.6-12.9 22.6-40.5 28-55.4 4.4-12 10.7-36.1 13.1-50.6 1.6-9.6 1.8-21 2.1-132.8l.4-122.2H1013v110z"></path>
    </svg>

    to select
  </span>
  <span class="help">
    <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewBox="0 0 24 24">
      <path d="M0 0h24v24H0V0z" fill="none"></path>
      <path d="M20 12l-1.41-1.41L13 16.17V4h-2v12.17l-5.58-5.59L4 12l8 8 8-8z"></path>
    </svg>
    <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey" viewBox="0 0 24 24">
      <path d="M0 0h24v24H0V0z" fill="none"></path>
      <path d="M4 12l1.41 1.41L11 7.83V20h2V7.83l5.58 5.59L20 12l-8-8-8 8z"></path>
    </svg>
    to navigate
  </span>
  <span class="help">
    <span class="ninja-examplekey esc">esc</span>
    to close
  </span>
  <span class="help">
    <svg xmlns="http://www.w3.org/2000/svg" class="ninja-examplekey backspace" viewBox="0 0 20 20" fill="currentColor">
      <path fill-rule="evenodd" d="M6.707 4.879A3 3 0 018.828 4H15a3 3 0 013 3v6a3 3 0 01-3 3H8.828a3 3 0 01-2.12-.879l-4.415-4.414a1 1 0 010-1.414l4.414-4.414zm4 2.414a1 1 0 00-1.414 1.414L10.586 10l-1.293 1.293a1 1 0 101.414 1.414L12 11.414l1.293 1.293a1 1 0 001.414-1.414L13.414 10l1.293-1.293a1 1 0 00-1.414-1.414L12 8.586l-1.293-1.293z" clip-rule="evenodd"></path>
    </svg>
    move to parent
  </span>
</div> </slot>
        </div>
      </div>
    </template></ninja-keys>
  <script>
    let theme = determineComputedTheme();
    const ninjaKeys = document.querySelector('ninja-keys');

    if (theme === 'dark') {
      ninjaKeys.classList.add('dark');
    } else {
      ninjaKeys.classList.remove('dark');
    }

    const openSearchModal = () => {
      // collapse navbarNav if expanded on mobile
      const $navbarNav = $('#navbarNav');
      if ($navbarNav.hasClass('show')) {
        $navbarNav.collapse('hide');
      }
      ninjaKeys.open();
    };
  </script>
  <script>
    // get the ninja-keys element
    const ninja = document.querySelector('ninja-keys');

    // add the home and posts menu items
    ninja.data = [{
        id: "nav-about",
        title: "about",
        section: "Navigation",
        handler: () => {
          window.location.href = "/";
        },
      },{id: "nav-publications",
              title: "publications",
              description: "Publications by categories in reversed chronological order.",
              section: "Navigation",
              handler: () => {
                window.location.href = "/publications/";
              },
            },{id: "nav-blog",
              title: "blog",
              description: "",
              section: "Navigation",
              handler: () => {
                window.location.href = "/blog/";
              },
            },{id: "nav-cv",
              title: "cv",
              description: "Here you can find my curriculum vitae and download the pdf on the top right.",
              section: "Navigation",
              handler: () => {
                window.location.href = "/cv/";
              },
            },{id: "nav-more",
              title: "more",
              description: "Some more things I do üìö üì∑",
              section: "Navigation",
              handler: () => {
                window.location.href = "/projects/";
              },
            },{id: "post-de-mystifying-multimodal-learning-lt-br-gt-lt-small-gt-the-hidden-inefficiency-in-vision-language-modelling",
          
            title: "De-mystifying Multimodal Learning&lt;br&gt;&lt;small&gt;The Hidden Inefficiency in Vision Language Modelling",
          
          description: "Discussing Vision-Language biggest performance bottleneck",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2026/demystifying1/";
            
          },
        },{id: "post-object-guided-visual-tokens-eliciting-compositional-reasoning-in-multimodal-language-models",
          
            title: "Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models",
          
          description: "Addressing MLLMs shortcomings in Compositional Reasoning through CLIP-Segmentation fusion",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2025/ogllava/";
            
          },
        },{id: "post-perception-localization-planning-and-control-on-rae-robots",
          
            title: "Perception, Localization, Planning and Control 
 on RAE Robots",
          
          description: "Computer Vision for Autonomour Robots",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2025/cvar/";
            
          },
        },{id: "post-optimizing-predictions-vocabulary-reduction-and-contrastive-decoding-in-llms",
          
            title: "Optimizing Predictions: Vocabulary Reduction and Contrastive Decoding in LLMs",
          
          description: "Efficiency-focused early exiting, vocabulary pruning, and contrastive decoding for LLM inference",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2024/earlyexit/";
            
          },
        },{id: "post-model-compression-for-machine-translation-in-large-language-models",
          
            title: "Model Compression for Machine Translation in Large Language Models",
          
          description: "Model Compression for Machine Translation in Large Language Models",
          section: "Posts",
          handler: () => {
            
              window.location.href = "/blog/2024/alma/";
            
          },
        },{id: "news-going-to-eurips-https-eurips-cc-our-paper-object-guided-visual-tokens-eliciting-compositional-reasoning-in-multimodal-language-models-https-openreview-net-forum-id-yvy1t3hheq-was-just-accepted-to-eurips-principles-of-generative-modelling-workshop-see-you-in-copenhagen",
          title: 'Going to [EurIPS](https://eurips.cc/)! Our paper [Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models](https://openreview.net/forum?id=yvY1T3hHEQ) was just accepted to EurIPS Principles of Generative Modelling Workshop, see you in Copenhagen. üá©üá∞',
          description: "",
          section: "News",},{id: "news-happy-to-share-i-just-graduated-lt-b-gt-cum-laude-lt-b-gt-and-ellis-honours-https-ivi-fnwi-uva-nl-ellis-news-recognizing-young-ai-talent-excellence-11-ellis-msc-honours-students-graduated-cum-laude-under-joint-supervision-of-ellis-members-across-europe-text-6-20matteo-20nulli-rigorous-20and-20rewarding-the-master-of-science-in-artificial-intelligence-at-university-of-amsterdam",
          title: 'Happy to share I just graduated &lt;b&gt;Cum-Laude&lt;/b&gt; and [ELLIS Honours](https://ivi.fnwi.uva.nl/ellis/news-/recognizing-young-ai-talent-excellence-11-ellis-msc-honours-students-graduated-cum-laude-under-joint-supervision-of-ellis-members-across-europe/#:~:text=(6)%20Matteo%20Nulli,rigorous%20and%20rewarding.) the Master of Science in Artificial Intelligence at University of Amsterdam! üéì',
          description: "",
          section: "News",},{id: "news-just-started-working-as-full-time-applied-researcher-at-ebay-foundation-models-team-with-hadi-https-nl-linkedin-com-in-hadi-hashemi-75a82649-and-shahram-https-www-linkedin-com-in-khadivi-excited-to-re-start",
          title: 'Just started working as full time Applied Researcher at eBay Foundation Models Team with [Hadi](https://nl.linkedin.com/in/hadi-hashemi-75a82649) and [Shahram](https://www.linkedin.com/in/khadivi/). Excited to (re-)start. üëæ',
          description: "",
          section: "News",},{id: "news-fortunate-to-visit-prof-yuki-asano-https-www-novatalent-com-111-italy-student-list-2025-at-funai-lab-in-utn-nuremberg-https-fundamentalailab-github-io-as-part-of-ellis-honours-program-https-ivi-fnwi-uva-nl-ellis-people-for-my-master-thesis-on-compositional-reasoning-in-vision-language",
          title: 'Fortunate to visit Prof. [Yuki Asano](https://www.novatalent.com/111/italy/student-list/2025)
at [FunAI Lab in UTN Nuremberg](https://fundamentalailab.github.io/) as part of [ELLIS Honours program](https://ivi.fnwi.uva.nl/ellis/people/) for my master thesis on Compositional Reasoning in Vision-Language.',
          description: "",
          section: "News",},{id: "news-part-of-nova-111-student-list-https-www-novatalent-com-111-list-the-nova-111-student-list-italy-2025-text-data-20analytics-20-26-20physics-matteo-20nulli-mathematics-2c-20data-20analytics-for-2025-among-the-most-promising-itaian-students-in-my-field-read-more-under-more-projects",
          title: 'Part of [Nova 111 Student List](https://www.novatalent.com/111-list/the-nova-111-student-list-italy-2025#:~:text=Data%20Analytics%20%26%20Physics-,Matteo%20Nulli,-Mathematics%2C%20Data%20Analytics) for 2025, among the most promising itaian students in my field! Read more under [more/](projects/).',
          description: "",
          section: "News",},{id: "news-going-to-neurips-our-paper-dynamic-vocabulary-pruning-in-early-exit-llms-https-arxiv-org-abs-2410-18952-was-just-accepted-to-neurips-efficient-natural-language-and-speech-processing-enlsp-iv-workshop-https-neurips2024-enlsp-github-io",
          title: 'Going to NeurIPS! Our paper [Dynamic Vocabulary Pruning in Early-Exit LLMs](https://arxiv.org/abs/2410.18952) was just accepted to [NeurIPS Efficient Natural Language and Speech Processing (ENLSP-IV) Workshop](https://neurips2024-enlsp.github.io/). üöÄ',
          description: "",
          section: "News",},{id: "news-today-i-started-working-at-ebay-joining-the-foundation-models-team-in-the-amsterdam-office-the-internship-will-focus-on-multimodal-research-and-development-for-data-architecture-and-pre-training-excited-to-start",
          title: 'Today, I started working at eBay, joining the Foundation Models Team in the Amsterdam office. The internship will focus on multimodal research and development for data, architecture and pre-training. Excited to start!',
          description: "",
          section: "News",},{id: "news-in-context-learning-improves-compositional-understanding-of-vision-language-models-https-arxiv-org-abs-2407-15487-was-just-published-in-icml-workshop-on-foundation-models-in-the-wild-https-icml-fm-wild-github-io",
          title: '[In-Context Learning Improves Compositional Understanding of Vision-Language Models](https://arxiv.org/abs/2407.15487) was just published in [ICML Workshop on Foundation Models in the Wild](https://icml-fm-wild.github.io) üôå',
          description: "",
          section: "News",},{id: "news-my-first-paper-39-explaining-rl-decisions-with-trajectories-a-reproducibility-study-https-openreview-net-forum-id-qdebbk5csh-was-just-published-in-transactions-on-machine-learning-research-https-www-jmlr-org-tmlr-papers",
          title: 'My first paper [&#39;Explaining RL Decisions with Trajectories‚Äô: A Reproducibility Study](https://openreview.net/forum?id=QdeBbK5CSh) was just published in [Transactions on Machine Learning Research](https://www.jmlr.org/tmlr/papers/#) üéä',
          description: "",
          section: "News",},{id: "news-i-joined-the-university-of-amsterdam-to-pursue-the-master-in-aritifcial-intelligence-https-www-uva-nl-shared-content-programmas-en-masters-artificial-intelligence-artificial-intelligence-html",
          title: 'I joined the University of Amsterdam to pursue the [Master in Aritifcial Intelligence](https://www.uva.nl/shared-content/programmas/en/masters/artificial-intelligence/artificial-intelligence.html)',
          description: "",
          section: "News",},{id: "news-i-graduated-from-bocconi-university-and-was-among-the-first-9-graduates-in-the-history-of-the-course-mathematical-and-computing-sciences-for-ai-https-www-unibocconi-eu-wps-wcm-connect-bocconi-sitopubblico-en-navigation-tree-home-programs-bachelor-of-science-mathematical-and-computing-sciences-for-artificial-intelligence-mathematical-and-computing-sciences-for-artificial-intelligence",
          title: 'I graduated from Bocconi University and was among the first 9 graduates in the history of the course [Mathematical and Computing Sciences for AI](https://www.unibocconi.eu/wps/wcm/connect/bocconi/sitopubblico_en/navigation+tree/home/programs/bachelor+of+science/mathematical+and+computing+sciences+for+artificial+intelligence/mathematical+and+computing+sciences+for+artificial+intelligence/)! üéì',
          description: "",
          section: "News",},{id: "news-just-landed-in-australia-to-join-the-university-of-sydney-for-a-full-semester-i-will-be-taking-deep-learning-https-www-sydney-edu-au-units-comp5329-stochastic-processes-https-www-sydney-edu-au-units-stat3921-and-big-data-and-data-diversity-https-www-sydney-edu-au-units-data2901-courses-as-well-as-surfing",
          title: 'Just landed in Australia to join the University of Sydney for a full semester, I will be taking [Deep Learning](https://www.sydney.edu.au/units/COMP5329), [Stochastic Processes](https://www.sydney.edu.au/units/STAT3921) and [Big Data and Data Diversity](https://www.sydney.edu.au/units/DATA2901) courses as well as surfing üèÑ',
          description: "",
          section: "News",},{id: "projects-amsterdam",
              title: 'Amsterdam',
              description: "Camera Roll pictures from Amsterdam and The Netherlands",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/1_project/";
                },},{id: "projects-f1",
              title: 'F1',
              description: "Formula 1 side-track pictures",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/7_project/";
                },},{id: "projects-bainsa",
              title: 'BAINSA',
              description: "Bocconi AI &amp; Neuroscience Student Association",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/bainsa/";
                },},{id: "projects-ellis-honours-student-2025",
              title: 'ELLIS Honours Student 2025',
              description: "Part of ELLIS Honours Students for the Master of AI",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/ellis_project/";
                },},{id: "projects-nova-111-student-list-2025",
              title: 'Nova 111 Student List 2025',
              description: "Among the most promising Italian Students",
              section: "Projects",handler: () => {
                  window.location.href = "/projects/nova_project/";
                },},{
            id: 'socials-google-scholar',
            title: 'Google Scholar',
            section: 'Socials',
            handler: () => {
              window.open("https://scholar.google.com/citations?user=WR5G-XcAAAAJ", "_blank");
            },
          },{
            id: 'socials-github',
            title: 'GitHub',
            section: 'Socials',
            handler: () => {
              window.open("https://github.com/MatteoNulli", "_blank");
            },
          },{
            id: 'socials-linkedin',
            title: 'LinkedIn',
            section: 'Socials',
            handler: () => {
              window.open("https://www.linkedin.com/in/matteonulli", "_blank");
            },
          },{
            id: 'socials-x',
            title: 'X',
            description: 'Twitter',
            section: 'Socials',
            handler: () => {
              window.open("https://twitter.com/matteo_nulli", "_blank");
            },
          },{
            id: 'socials-dblp',
            title: 'DBLP',
            section: 'Socials',
            handler: () => {
              window.open("https://dblp.org/pid/382/5836", "_blank");
            },
          },{
          id: 'light-theme',
          title: 'Change theme to light',
          description: 'Change the theme of the site to Light',
          section: 'Theme',
          handler: () => {
            setThemeSetting("light");
          },
        },
        {
          id: 'dark-theme',
          title: 'Change theme to dark',
          description: 'Change the theme of the site to Dark',
          section: 'Theme',
          handler: () => {
            setThemeSetting("dark");
          },
        },
        {
          id: 'system-theme',
          title: 'Use system default theme',
          description: 'Change the theme of the site to System Default',
          section: 'Theme',
          handler: () => {
            setThemeSetting("system");
          },
        },];
  </script>


    <script src="./visual_tokens_blogpost_files/shortcut-key.js"></script>
  

<div class="hiddendiv common"></div></body></html>