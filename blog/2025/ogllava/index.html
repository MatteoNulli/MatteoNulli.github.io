<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Object-Guided Visual Tokens: Eliciting Compositional Reasoning<br>in Multimodal Language Models | Matteo Nulli </title> <meta name="author" content="Matteo Nulli"> <meta name="description" content="Addressing shortcomings of MLLMs in Compositional Reasoning through Segmentation"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://matteonulli.github.io/blog/2025/ogllava/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Matteo</span> Nulli </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">other stuff </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Object-Guided Visual Tokens: Eliciting Compositional Reasoning<br>in Multimodal Language Models</h1> <p class="post-meta"> Created in September 05, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/multimodal-learning"> <i class="fa-solid fa-hashtag fa-sm"></i> Multimodal-Learning</a>   <a href="/blog/tag/compositional-reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> Compositional-Reasoning</a>   <a href="/blog/tag/visual-grounding"> <i class="fa-solid fa-hashtag fa-sm"></i> Visual-Grounding</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h5 id="m-nulli-i-najdenkoska-m-m-derakhshani-v-orshulevich-y-m-asano">M. Nulli, I. Najdenkoska, M. M. Derakhshani, V. Orshulevich, Y. M. Asano</h5> <h6 id="university-of-amsterdam-ebay-university-of-technology-nuremberg">University of Amsterdam, eBay, University of Technology Nuremberg</h6> <h6 id="links--long-paper---blogpost---code">Links: 📄 <a href="https://github.com/MatteoNulli/og_llava/blob/main/paper/LongPaper.pdf" rel="external nofollow noopener" target="_blank">Long Paper</a> | 📝 <a href="https://matteonulli.github.io/blog/2025/ogllava/" rel="external nofollow noopener" target="_blank">Blogpost</a> | 🧑‍💻 <a href="https://github.com/MatteoNulli/og_llava/tree/main" rel="external nofollow noopener" target="_blank">Code</a> </h6> <p><br></p> <h2 id="motivation">Motivation</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="mainfigure"></a> <figure> <picture> <img src="/assets/img/ogllava_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: <b> OG-LLaVA </b> architecture with <code>OG-Fusion</code> internal proces </div> <p>Most Multimodal Large Language Models (MLLMs) use contrastively pre-trained vision encoders.<br> They work well on many tasks, but often struggle when it comes to <strong>compositional understanding</strong> and <strong>reasoning</strong> about what’s actually in an image. That’s because these encoders are mainly trained for image–caption retrieval, not for truly breaking down and understanding all parts of a scene. Another issue is efficiency: state-of-the-art vision encoders generate <strong>2–3x more visual tokens</strong>, which slows down both training and inference.</p> <p>To tackle these problems, we introduce <strong>OG-LLaVA (Object-Guided <a href="https://arxiv.org/pdf/2310.03744" rel="external nofollow noopener" target="_blank">LLaVA</a>)</strong>. With our new connector design, <strong><code class="language-plaintext highlighter-rouge">OG-Fusion</code></strong>, the model can reason about visual content more effectively—without adding lots of extra tokens or fine-tuning the vision encoder itself. At the core of <code class="language-plaintext highlighter-rouge">OG-Fusion</code> is a simple but powerful idea: combine <strong>CLIP representations</strong> with <strong>segmentation masks</strong>. This lets OG-LLaVA leverage the descriptive strength of segmentation models to better capture <strong>object relationships</strong> and <strong>spatial arrangements</strong>. The result? <strong>OG-LLaVA outperforms existing comparable models on tasks that demand deeper visual reasoning and grounding</strong>, all while staying efficient.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/conme_visual.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2: <b> OG-LLaVA </b> vs LLaVA-1.5 on Compositional Reasoning Benchmark ConMe. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/mmvp_visual.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 3: <b> OG-LLaVA </b> vs LLaVA-1.5 on Vision Grounding benchmark MMVP. </div> <h2 id="underlying-procedure">Underlying Procedure</h2> <p>As illustrated in <a href="#mainfigure">Figure 1</a>, we extract visual features from the input image through a Vision Encoder. Concurrently, we pass the input image through <code class="language-plaintext highlighter-rouge">OG-Fusion</code>. Here we:</p> <ol> <li>Use a Segmentation model to retrieve the masks,</li> <li>Downsample the segmentations, and</li> <li>Apply these masks onto the visual features.</li> <li>Concatenated together and passed through a Multi-Layer Perceptron to produce Object-Guided Visual Tokens (<strong><em>OGVT</em></strong>).</li> </ol> <p>The <strong><em>OGVT</em></strong> are then given as input to a Large Language Model together with Textual Tokens to produce an output.<br> The ❄️ (snowflake) and 🔥 (fire) symbols in <a href="#mainfigure">Figure 1</a> represent modules whose parameters are kept <strong>frozen</strong> or <strong>turned on</strong>.<br> LoRA emphasizes that not all parameters of the LLM are unfrozen, only the LoRA layers.</p> <h2 id="visualizations">Visualizations</h2> <p>The images we picked cover all kinds of tricky challenges—spotting tiny details, telling apart subtle colors, reading depth cues, recognizing materials, making sense of spatial layouts, and even detecting small objects. They’re designed to push visual–language reasoning to its limits. What’s key is that these examples are tested at inference time with no extra fine-tuning, so any boost (or drop) in performance comes purely from the Object-Guided priors built into <strong>OG-LLaVA</strong>.</p> <p>In <a href="#fig-conme-rel-2">Figure 4</a>, <a href="#fig-conme-obj"> 5</a> and <a href="#fig-conme-rel-1"> 6</a> we highlight a range of cases where <strong>OG-LLaVA</strong> consistently demonstrates sharper perception and more grounded reasoning, from subtle posture cues to tricky color judgments and material recognition.</p> <ul> <li> <strong>Fine-grained human pose</strong>: correctly reads a batter’s stance, placing the bat <em>up and behind</em> instead of <em>in front</em>. (<a href="#fig-conme-rel-2">Figure 4 - Picture1</a>)</li> <li> <strong>Precise color &amp; reflection reasoning</strong>: rules out red in umbrellas, confining it to apples/plate, while the baseline gets misled, as well as captures realistic color reflections on materials and disambiguates different hues. (<a href="#fig-conme-rel-2">Figure 4 - Picture2</a>), (<a href="#fig-conme-obj">Figure 5 - Picture8</a>) &amp; (<a href="#fig-conme-rel-1">Figure 6 - Picture2</a>)</li> <li> <strong>Depth-of-field understanding</strong>: detects focus fading <em>front-to-back</em> instead of mistakenly <em>left-to-right</em>. (<a href="#fig-conme-rel-2">Figure 4 - Picture3</a>)</li> <li> <strong>Material recognition</strong>: identifies a skate-park surface as <em>concrete</em>, not asphalt and glass instead of curtains. (<a href="#fig-conme-rel-2">Figure 4 - Picture4</a>) &amp; (<a href="#fig-conme-rel-2">Figure 5 - Picture1</a>)</li> <li> <strong>Font recognition</strong>: picks up subtle font characteristics, showing solid OCR ability. (<a href="#fig-conme-obj">Figure 5 - Picture2</a>)</li> <li> <strong>Spatial reasoning</strong>: accurately locates people and objects in complex scenes. (<a href="#fig-conme-obj">Figure 5 - Picture3</a>) &amp; (<a href="#fig-conme-obj">Figure 5 - Picture6</a>)</li> <li> <strong>Object counting &amp; detection</strong>: counts giraffes correctly where the baseline stumbles and spots a distant coffee maker amid clutter, avoiding confusion with a blender. (<a href="#fig-conme-obj">Figure 5 - Picture4</a>) &amp; (<a href="#fig-conme-rel-1">Figure 6 - Picture3</a>)</li> <li> <strong>Fashion understanding</strong>: distinguishes between short sleeves and cap sleeves. (<a href="#fig-conme-obj">Figure 5 - Picture7</a>)</li> <li> <strong>Dynamic cues</strong>: understands a distant sail is <em>inflated with strong breeze</em>, matching the surfing context. (<a href="#fig-conme-rel-1">Figure 6 - Picture1</a>)</li> <li> <strong>Shape recognition</strong>: correctly identifies train tracks in the background. (<a href="#fig-conme-rel-1">Figure 6 - Picture4</a>)</li> </ul> <p>Together, these examples underline how <strong>OG-LLaVA</strong> moves beyond surface-level cues. It pays attention to fine details, adapts across diverse tasks, and reasons about entire scenes in a way that more closely reflects human understanding.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-conme-rel-2"></a> <figure> <picture> <img src="/assets/img/conme_additional_rel_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 4: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Relation examples. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-conme-obj"></a> <figure> <picture> <img src="/assets/img/conme_additional_obj.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 5: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Object examples. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-conme-rel-1"></a> <figure> <picture> <img src="/assets/img/conme_additional_rel_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 6: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Relation examples. </div> <h2 id="results">Results</h2> <p>Our results on compositional reasoning and vision-centric benchmarks <a href="#tab-main-res">Table 1</a>, show that <strong>OG-LLaVA</strong> consistently outperforms its baselines, across both <em><a href="https://arxiv.org/pdf/2310.03744" rel="external nofollow noopener" target="_blank">LLaVA-1.5</a></em> and <em><a href="https://arxiv.org/pdf/2406.16860" rel="external nofollow noopener" target="_blank">Cambrian-1</a></em> training setups. The improvements are not marginal—they’re large and systematic.</p> <ul> <li> <strong>Compositional understanding</strong> <ul> <li> <strong>ARO</strong>: <ul> <li>+21% on <em>Coco-Order</em> (38.2 → 82.6) and +16% on <em>Flickr-Order</em> (49.1 → 84.0).</li> <li> <em>Visual Genome Attribution</em> on average +10% across backbones and on <em>Visual Genome Relation</em> +20% across training data and model sizes.</li> </ul> </li> <li> <strong>ConME</strong>: steady +2% gains, peaking at 65.2 in the 8B setting (+3.6 over the strongest baseline).</li> </ul> </li> <li> <strong>Vision-centric reasoning</strong> <ul> <li> <strong>MMVP</strong>: about +3 points on average (e.g. 32.0 → 37.0 in 8B, 61.6 → 66.0 with <em>Cambrian-1</em> data).</li> <li> <strong>CVBench</strong>: stable performance, with only ±1 point fluctuations.</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="tab-main-res"></a> <figure> <picture> <img src="/assets/img/main-table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 1: <b> OG-LLaVA </b> performance on Compositional Reasoning and Vision Centric tasks compared with LLaVA baselines. </div> <p>In <a href="#fig-sit-vs-ours">Figure 7</a>, we compare <strong>OG-LLaVA-8B</strong> with SIT-8B, and LLaVA-1.5-8B under the same backbone. SIT-8B stands for <em><a href="https://arxiv.org/pdf/2402.14327" rel="external nofollow noopener" target="_blank">Subobject-level Image Tokenization (SIT)</a></em> a new study employing a comparable segmentation-infusion method. The results are clear: <strong>OG-LLaVA</strong> consistently outperforms SIT, with more than a 25% advantage on compositional reasoning and a 10% edge in visual grounding.</p> <p>There’s also a key difference in usability. <strong>OG-LLaVA</strong> works flexibly both with and without segmentation masks at inference, while SIT requires pre-computed masks every time. This not only adds non-trivial overhead—since a separate segmentation model must run first—but also makes the system less adaptable. In practice, the reduced token count doesn’t outweigh the complexity introduced, whereas <strong>OG-LLaVA</strong> preserves efficiency without imposing such constraints.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-sit-vs-ours"></a> <figure> <picture> <img src="/assets/img/og_llava_vs_rivals_weighted.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 7: <b> OG-LLaVA </b> vs Subobject Level Image Tokenization and LLaVA-1.5 on Compositional Reasoning and Vision Centric tasks. </div> <h2 id="citation">Citation</h2> <p>If you use this work, please cite:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">nulli2025ogllava</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Nulli, M. and Najdenkoska, I., and Derakhshani, M. M., and Dorkenwald, M., Orshulevich, V., and Asano, Y. M.}</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models}</span><span class="p">,</span>
  <span class="na">howpublished</span>  <span class="p">=</span> <span class="s">{https://matteonulli.github.io/blog/2025/ogllava/}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Accessed: 2025-09-05}</span>
<span class="p">}</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/alma/">Model Compression for Machine Translation in Large Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/confidently_exitingblogpostmd-at-main-joanveljaconfidently_exiting-github/">Confidently_Exiting/blogpost.md at main · joanvelja/Confidently_Exiting · GitHub</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Matteo Nulli. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>