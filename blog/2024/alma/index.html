<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Model Compression for Machine Translation in Large Language Models | Matteo Nulli </title> <meta name="author" content="Matteo Nulli"> <meta name="description" content="Model Compression for Machine Translation in Large Language Models"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%8F%80&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://matteonulli.github.io/blog/2024/alma/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Matteo</span> Nulli </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">other stuff </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Model Compression for Machine Translation in Large Language Models</h1> <p class="post-meta"> Created in November 05, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a> ¬† ¬∑ ¬† <a href="/blog/tag/inference-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> Inference-Optimization</a> ¬† <a href="/blog/tag/llms"> <i class="fa-solid fa-hashtag fa-sm"></i> LLMs</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h5 id="j-vincenti-m-nulli-a-tragoudaras-z-tzifa-kratira-and-a-b-gomez">J. Vincenti, M. Nulli, A. Tragoudaras, Z. Tzifa-Kratira, and A. B. Gomez</h5> <h6 id="university-of-amsterdam">University of Amsterdam</h6> <h6 id="links--paper---blogpost---code">Links: üìÑ <a href="https://www.overleaf.com/read/dkkcbhpycncf#bec4c7" rel="external nofollow noopener" target="_blank">Paper</a> | üìù <a href="https://matteonulli.github.io/blog/2024/alma/" rel="external nofollow noopener" target="_blank">Blogpost</a> | üßë‚Äçüíª <a href="https://github.com/JortVincenti/ALMA" rel="external nofollow noopener" target="_blank">Code</a> </h6> <p><br></p> <h2 id="motivation">Motivation</h2> <p>Small traditional machine translation models can produce high-quality translations in a one-to-one language setting [<a href="https://arxiv.org/pdf/2207.04672" rel="external nofollow noopener" target="_blank">1</a>]. However, when scaling these models to handle multiple languages, they often struggle to perform well due to the limited amount of parallel data and the complex architectures required to fully understand the task. In contrast, Large Language Models (LLMs) [<a href="https://arxiv.org/pdf/2005.14165" rel="external nofollow noopener" target="_blank">2</a>] can handle complex settings due to their large architectures and overcome smaller datasets by leveraging their large English training data. As a result, ALMA is a fine-tuned LLM designed for machine translation across multiple language directions from and to English and is the first LLM to become competitive with traditional machine translation models [<a href="https://arxiv.org/pdf/2309.11674" rel="external nofollow noopener" target="_blank">3</a>].</p> <p>Despite performing remarkably well in translation, ALMA incurs significant computational and memory costs, making even inference prohibitively expensive to scale to consumer hardwar [<a href="https://arxiv.org/pdf/2308.07633" rel="external nofollow noopener" target="_blank">4</a>]. To address these challenges, several compression techniques have been proposed for LLMs. Among many others, these include quantization [<a href="https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/720541/&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=4619659333798904151&amp;ei=4Pn9aLLLGNyOieoP1t-zoQM&amp;scisig=ABGrvjJbo0ZiWOaWn3QOaSjwLpbc" rel="external nofollow noopener" target="_blank">5</a>, <a href="https://scholar.google.com/scholar_url?url=http://proceedings.mlr.press/v202/xiao23c.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=17814731842610185667&amp;ei=-vn9aImlILmAieoPlr2tqAg&amp;scisig=ABGrvjIl4KLQN2x_Kith27oHWlKF" rel="external nofollow noopener" target="_blank">6</a>] and pruning [<a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=4588992562541514941&amp;ei=Efr9aNeCM_rUieoPvMbB8Qc&amp;scisig=ABGrvjKlhw7h_hc2pm1eDaB_57Jp" rel="external nofollow noopener" target="_blank">7</a>, <a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=6338930303179684776&amp;ei=I_r9aIX5F77WieoPn4GZgQE&amp;scisig=ABGrvjJDL40cEEu6_3RiqtrpmeNY" rel="external nofollow noopener" target="_blank">8</a>, <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/1510.00149&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=7860777411990654691&amp;ei=Mfr9aKfnEv2lieoP1Jf4uAw&amp;scisig=ABGrvjKoCGzYeTPmCdCI6xO9Hgxo" rel="external nofollow noopener" target="_blank">9</a>] methods. However, none of these techniques have been applied to ALMA and evaluated in terms of translation quality.</p> <p>In this work, we explore applying compression techniques to ALMA to preserve its translation quality. We experiment with five distinct compression methods: GPTQ [<a href="https://scholar.google.com/scholar_url?url=https://proceedings.mlr.press/v202/frantar23a&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=2009267211619482754&amp;ei=Rfr9aKK0DYGpieoPyfaRyA4&amp;scisig=ABGrvjJl-us59RdHA3e_MXFWxM8X" rel="external nofollow noopener" target="_blank">10</a>], Q-LoRA , [<a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=8079217396100949644&amp;ei=gvr9aPPOK_2lieoP1Jf4uAw&amp;scisig=ABGrvjIqYBQ-eh1JEPCyzNEGJAtt" rel="external nofollow noopener" target="_blank">11</a>], SmoothQuant [<a href="https://scholar.google.com/scholar_url?url=http://proceedings.mlr.press/v202/xiao23c.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=17814731842610185667&amp;ei=jfr9aLrzIbmAieoPlr2tqAg&amp;scisig=ABGrvjIl4KLQN2x_Kith27oHWlKF" rel="external nofollow noopener" target="_blank">12</a>], Wanda [<a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2306.11695&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=8181806601256870843&amp;ei=p_r9aLD8JNrZzwKX7-SYDw&amp;scisig=ABGrvjJYsdHWsU9e_-K7BUqRU9W8" rel="external nofollow noopener" target="_blank">13</a>], and DSnot [<a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2310.08915&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=6652760928495592428&amp;ei=v_r9aOaYCe2ZieoP7ueJyQk&amp;scisig=ABGrvjLWrVul2-STCvxcctLhFb5d" rel="external nofollow noopener" target="_blank">14</a>]. To evaluate the effectiveness of these techniques, we assess their translation quality, memory usage, and inference time, providing a comprehensive analysis of the trade-offs involved in compressing LLMs fine-tuned for translation.</p> <h2 id="results">Results</h2> <p>Table 1 presents the results for the five different compression techniques. A noticeable performance gap can be observed between the <code class="language-plaintext highlighter-rouge">en‚Üíxx</code> and <code class="language-plaintext highlighter-rouge">xx‚Üíen</code> translation directions. As noted in [<a href="https://arxiv.org/pdf/2309.11674" rel="external nofollow noopener" target="_blank">3</a>], this disparity is likely due to the large amount of English data used during the training of LLAMA 2 [<a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2307.09288&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=10722545090167785628&amp;ei=Jfv9aPKCF-2ZieoP7ueJyQk&amp;scisig=ABGrvjIKbmSm-weoKpHx13AzXNT6" rel="external nofollow noopener" target="_blank">15</a>], resulting in a bias towards better performance when translating into English.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="tab-main-res"></a> <figure> <picture> <img src="/assets/img/alma-table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Table 1: Summary of calculated metrics across all analyzed model compression techniques. All results are calculated on ALMA-7B and the Method column highlights the method used along with a precision/sparsity level (NA= Not Applicable, NF4=4-bit NormalFloat). We report BLEU (‚Üë) and COMET (‚Üë) scores for <code>en ‚Üî x</code>. Additionally, we cover the average time per token in seconds (‚Üì) and the memory allocation in megabytes (MB) (‚Üì) after each compression technique is applied. </div> <h4 id="quantization">Quantization</h4> <p>All quantization methods preserved strong BLEU and COMET performance‚Äîonly slightly below the baseline‚Äîwhile reducing memory use by about 70%. This stability likely stems from ALMA‚Äôs already low BLEU score and highly variable multilingual weights, which allow better generalization after quantization. However, 4-bit quantization increased inference time, likely because GPU operations still rely on FP16 multiplications, which are more efficient on current hardware despite the smaller memory footprint of integer formats.</p> <p>The GPTQ study in <a href="#figure1">Figure 1</a> found that 4-bit quantization offers the best balance between performance and memory efficiency, while 2-bit causes major degradation due to limited representational capacity. However, GPTQ also led to increased inference time, likely because the fused kernel optimization from the original paper was not implemented (<a href="#tab-main-res">2nd Row Table 1</a>).</p> <p>The Q-LoRA-like quantization method showed minimal translation performance loss and faster inference than GPTQ (<a href="#tab-main-res">3rd Row Table 1</a>), thanks to its efficient block-wise floating-point quantization and de-quantization to FP16.</p> <p>Finally, combining SmoothQuant with Q-LoRA by smoothing activations and applying block-wise weight quantization‚Äîsurprisingly improved performance beyond the baseline ALMA-7B (<a href="#tab-main-res">4th Row Table 1</a>), suggesting that quantization can also act as a noise-suppressing mechanism that enhances translation quality.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure1"></a> <figure> <picture> <img src="/assets/img/alma-figure1.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Top grah shows average BLEU Score for both directions of <code>en ‚Üî x</code> across different GPTQ bitquantization levels. The bottom bar chart represents the memory allocation across different precision values. Both results are based on ALMA-7B. </div> <h4 id="pruning">Pruning</h4> <p>Wanda pruning achieved lower BLEU scores but offered faster inference and reduced memory use compared to the full ALMA-7B model (<a href="#tab-main-res">5th Row Table 1</a>). However, its memory savings were less substantial than quantization, since pruning zeros out weights rather than compressing them. The resulting sparsity speeds up computation but does not reduce the number of operations.</p> <p>Wanda + DSnoT improved perplexity (predictive accuracy) but had negligible gains in BLEU and COMET, indicating that it enhances word-level prediction rather than overall translation quality. Thus, DSnoT appears better suited for general LLM tasks and was excluded from the main results.</p> <p>Wanda + GPTQ, inspired by prior LLaMA experiments, combined pruning (50% sparsity) with 4-bit quantization, yielding slightly better performance and memory usage comparable to pure quantization, though with longer inference times (<a href="#tab-main-res">6th Row Table 1</a>). The method‚Äôs suitability ultimately depends on the hardware and performance trade-offs of the translation setup.</p> <h4 id="effects-of-calibration">Effects of Calibration</h4> <p>In <a href="#figure2">Figure 2</a> we report our expeirments on calibration data. These showed minimal performance differences across varying sample sizes, suggesting that translation models depend mainly on pattern recognition and word mappings, which smaller datasets can capture effectively. Since calibration is a one-time post-training step, using the best-performing setup (512 samples per language) is acceptable despite higher latency. However, as model size increases, calibration becomes computationally expensive, highlighting the advantage of methods like Q-LoRA, which bypass the calibration step entirely.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure2"></a> <figure> <picture> <img src="/assets/img/alma-figure2.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Figure 2: The top and bottom graphs display the BLEU and COMET scores, respectively, for both translation directions <code>en ‚Üî x</code> across varying calibration sizes and different compression methods. All results are based on the ALMA-7B model. </div> <h2 id="citation">Citation</h2> <p>If you use this work, please cite:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">vincenti2024modelcompression</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Vincenti, J., Nulli, M., Tragoudaras,A., Tzifa-Kratira, Z., and Gomez, A. B.}</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Model Compression for Machine Translation in Large Language Models}</span><span class="p">,</span>
  <span class="na">howpublished</span>  <span class="p">=</span> <span class="s">{https://matteonulli.github.io/blog/2024/alma/}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/ogllava/">Object-Guided Visual Tokens: Eliciting Compositional Reasoning<br>in Multimodal Language Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/confidently_exitingblogpostmd-at-main-joanveljaconfidently_exiting-github/">Confidently_Exiting/blogpost.md at main ¬∑ joanvelja/Confidently_Exiting ¬∑ GitHub</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Matteo Nulli. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?b7816bd189846d29eded8745f9c4cf77"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>