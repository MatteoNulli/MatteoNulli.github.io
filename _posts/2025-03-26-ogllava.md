---
layout: post
title: "Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models"
date: 2025-09-05 14:24:00
description: Addressing MLLMs shortcomings in Compositional Reasoning through CLIP-Segmentation fusion
tags: Multimodal-Learning Compositional-Reasoning Computer-Vision
# categories: Multimodal-Learning
thumbnail: assets/img/tumbn_ogllava.png
mathjax: true
math: true
importance: 1  # 1 will appear first
---


##### <b>M. Nulli</b>, I. Najdenkoska, M. M. Derakhshani, V. Orshulevich, M. Dorkenwald and Y. M. Asano
###### University of Amsterdam, eBay, University of Technology Nuremberg, ELLIS Unit Amsterdam
###### <img src="https://upload.wikimedia.org/wikipedia/commons/1/17/Uva%C2%AEmerken_ENG.png" alt="University of Amsterdam" height="24"/> &nbsp;<img src="https://upload.wikimedia.org/wikipedia/commons/1/1b/EBay_logo.svg" alt="eBay" height="24"/> &nbsp; <img src="https://www.utn.de/files/2022/07/UTN-Website-icon-512.png" alt="UTN" height="24"/> &nbsp; <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQhUVivV36BHYzHgtuCNL1TpI4jdOCG0W1h-A&s" alt="UTN" height="24"/> &nbsp; 
###### üìÑ [Paper](https://openreview.net/forum?id=yvY1T3hHEQ) | üìú [Full Thesis](https://github.com/MatteoNulli/og_llava/blob/main/paper/LongPaper.pdf) | üìù [Blogpost](https://matteonulli.github.io/blog/2025/ogllava/) | üßë‚Äçüíª [Code](https://github.com/MatteoNulli/og_llava/tree/main)
###### *Accepted to EurIPS, Workshop on Principles of Generative Modelling*
<br>


<script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['\\[', '\\]']],
    },
    svg: { fontCache: 'global' }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
</script>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js"
        onload="renderMathInElement(document.body, {
          delimiters: [
            {left: '$$', right: '$$', display: true},
            {left: '\\[', right: '\\]', display: true},
            {left: '$',  right: '$',  display: false},
            {left: '\\(', right: '\\)', display: false}
          ],
          // keep default ignore list so code/pre are skipped
        });"></script>


## Motivation


<div class="row mt-3">
    <a id="video2"></a>
    <div class="col-sm mt-3 mt-md-0">
        {% include video.liquid path="assets/video/ogllava.mp4" class="img-fluid rounded z-depth-1" controls=true autoplay=true %}
    </div>
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/eurips.JPG" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    <b>Matteo Nulli going through Figure 1 at the ELLIS Honours Presentation (left)</b>, <b>Matteo Nulli presenting the paper at EurIPS (right)</b>
</div>


Most Multimodal Large Language Models (MLLMs) ([2](#qwen2-5-vl-2025), [3](#internvl2-2024), [4](#gemma-3-2025), [5](#cambrian-1-2024), [6](#llava-next-2024)) use contrastively pre-trained vision encoders ([7](#clip-2021)). They work well on many tasks, but often struggle when it comes to **compositional understanding** and **reasoning** on understanding interndependencies between objetcs, as highlighted in [8](#bags-of-words-vlms-2023) and [9](#icl-compositional-vlms-2024). That‚Äôs because these encoders are mainly trained for image‚Äìcaption retrieval, not for truly breaking down and understanding all parts of a scene. Another issue is efficiency: state-of-the-art vision encoders generate **2‚Äì3x more visual tokens** (Any-Resolution in [6](#llava-next-2024) and Spatial Visual Aggregator in [5](#cambrian-1-2024)), which slow down both training and inference.  

To tackle these problems, we introduce **OG-LLaVA (Object-Guided [LLaVA](https://arxiv.org/pdf/2310.03744))**. With our new connector design, **`OG-Fusion`**, the model can reason about visual content more effectively‚Äîwithout adding lots of extra tokens or fine-tuning the vision encoder itself. At the core of `OG-Fusion` is a simple but powerful idea: combine **CLIP representations** with **segmentation masks**. This lets OG-LLaVA leverage the descriptive strength of segmentation models ([10](#sam-2-2024)) to better capture **object relationships** and **spatial arrangements**. The result? <br>
**OG-LLaVA outperforms comparable models on tasks demanding deep visual reasoning and grounding, while staying efficient.**


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
    <a id="mainfigure"></a>
        {% include figure.liquid loading="eager" path="./assets/img/ogllava_3.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 1: <b> OG-LLaVA </b> architecture with <code>OG-Fusion</code> internal proces
</div>


## Methodology

Given a single input image $\mathbf{X} \in \mathbb{R}^{C \times H \times W}$, we denote by
$\mathbf{M} = {\mathbf{m}_i \mid i = 1,\dots,N} \subset \mathbb{R}^{H \times W}$
the corresponding set of $N$ binary segmentation masks, where each mask satisfies $\mathbf{m}_i \in {0,1}^{H \times W}$.
Our objective is to construct a set of segmentation-aware visual tokens, such that each variable-length token segment is explicitly associated with one object mask.

For clarity, we describe the procedure assuming a batch size of one; a fully rigorous mathematical formulation is deferred to the Appendix of the [paper](https://openreview.net/forum?id=yvY1T3hHEQ) (see Section Object-Guided Visual Tokens).

### Masks and Features Extraction

We begin by extracting object-level structure from each image through a segmentation model, which produces a set of binary masks $\mathbf{M}$. During training, visual features are obtained from a Vision Encoder, yielding representations $\mathbf{X'} \in \mathbb{R}^{V \times F}$. These features are aligned with the corresponding segmentation masks and processed through an ad-hoc downsampling operator designed to preserve object-centric information.

### Downsampling Operator

We define a downsampling operator $\Phi_{\alpha}$ that maps a high-resolution binary mask to a lower-resolution representation. Each output bin aggregates neighboring pixels and is marked as foreground if it contains at least $\alpha$ foreground pixels. Applying this operator independently to all $N$ masks results in a set of downsampled binary masks:

<p align="center"> \[ \mathbf{M}' = \bigl\{ \Phi_{\alpha}(\mathbf{m}_i) \;\bigm|\; i = 1,\dots,N \bigr\} \subset \{0,1\}^{V} \] </p>

Further implementation details of $\Phi_{\alpha}$ are provided in Appendix A of the [paper](https://openreview.net/forum?id=yvY1T3hHEQ).

### Object-Guided Visual Tokens

Following preprocessing, the downsampled masks $\mathbf{M}'$ are applied to the visual feature matrix $\mathbf{X}'$ through an index-based row-selection matrix $P_i$. This operation extracts object-specific visual fragments:

<p align="center"> \[ \mathbf{Y}_i = P_i\,\mathbf{X}' \;\in\; \mathbb{R}^{t_i \times F}. \] </p>

The resulting fragments are then projected into the language-model embedding space via a Multi-Layer Perceptron (MLP), producing Object-Guided Visual Tokens (OGVT):

<p align="center"> \[ \boxed{ \textbf{OGVT} \;:=\; MLP(\mathbf{Y}) \;\in\; \mathbb{R}^{T \times D} } \] </p>

Here, $T$ denotes the total number of object-bearing bins retained after downsampling. While $T$ varies per image, its expected value remains close to the original token count ($T \approx V$). When multiple masks overlap on the same ViT bin, that bin is duplicated across different $\mathbf{Y}_i$. Due to mask thresholding and the use of rotary positional embeddings (RoPE), these duplicated tokens yield distinct projections and therefore do not collapse into identical attention keys. As a result, attention is naturally biased toward regions with higher object density, effectively reintroducing spatial grounding that is otherwise weakened in standard Transformer architectures. A formal analysis of token duplication effects is provided in Appendix B of the [paper](https://openreview.net/forum?id=yvY1T3hHEQ).

### Model Architecture, Training, and Inference

To preserve architectural compatibility with LLaVA-1.5, we adopt CLIP ViT-L/14@336 ([7](#clip-2021)) as the vision encoder. While CLIP is our primary choice, the proposed method is agnostic to the specific vision backbone.

We experiment with two large language models:
Llama 3.1-8B-Instruct ([11](#llama-3-herd-2024)) and Llama 3.2-3B-Instruct ([12](#llama-3-2-2024)), resulting in two variants OG-LLaVA-8B and OG-LLaVA-3B, respectively.
An overview of the full OG-Fusion pipeline is shown in [Figure&nbsp;1](#mainfigure).
The architecture integrates a frozen Segment Anything Model 2 (SAM2) ([10](#sam-2-2024)) backbone, followed by the object-guided token construction described above and a two-hidden-layer MLP with GeLU activations.

Training follows the visual instruction tuning paradigm of [13](#visual-instruction-tuning-2023) and proceeds in two stages:<br>
(i) Vision‚ÄìLanguage Alignment, where only OG-Fusion is unfrozen <br>
(ii) Supervised Fine-Tuning, where both the LLM and OG-Fusion are trained using LoRA ([14](#lora-2021)).

The **_OGVT_** are then given as input to a Large Language Model together with Textual Tokens to produce an output.  
The ‚ùÑÔ∏è (snowflake) and üî• (fire) symbols in [Figure&nbsp;1](#mainfigure) represent modules whose parameters are kept **frozen** or **trained**.  
LoRA emphasizes that not all parameters of the LLM are unfrozen, only the LoRA layers.

Although **_OGVT_**s are constructed using segmentation masks during training, the model can be evaluated both with and without mask infusion‚Äîdemonstrating robustness by preserving the semantic structure of the original visual features $\mathbf{X}'$.

## Results

Our results on compositional reasoning and vision-centric benchmarks [Table&nbsp;1](#tab-main-res), show that **OG-LLaVA** consistently outperforms its baselines, across both *[LLaVA-1.5](https://arxiv.org/pdf/2310.03744)* and *[Cambrian-1](https://arxiv.org/pdf/2406.16860)* training setups. The improvements are not marginal‚Äîthey‚Äôre large and systematic.  

- **Compositional understanding**  
  - **ARO**: 
    - +21% on *Coco-Order* (38.2 ‚Üí 82.6) and +16% on *Flickr-Order* (49.1 ‚Üí 84.0). 
    - *Visual Genome Attribution* on average +10% across backbones and on *Visual Genome Relation* +20% across training data and model sizes.  
  - **ConME**: steady +2% gains, peaking at 65.2 in the 8B setting (+3.6 over the strongest baseline).  

- **Vision-centric reasoning**  
  - **MMVP**: about +3 points on average (e.g. 32.0 ‚Üí 37.0 in 8B, 61.6 ‚Üí 66.0 with *Cambrian-1* data).  
  - **CVBench**: stable performance, with only ¬±1 point fluctuations.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
    <a id="tab-main-res"></a>
        {% include figure.liquid loading="eager" path="./assets/img/main-table.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Table 1: <b> OG-LLaVA </b> performance on Compositional Reasoning and Vision Centric tasks compared with LLaVA baselines.
</div>


In [Figure&nbsp;7](#fig-sit-vs-ours), we compare **OG-LLaVA-8B** with SIT-8B, and LLaVA-1.5-8B under the same backbone. SIT-8B stands for *[Subobject-level Image Tokenization (SIT)](https://arxiv.org/pdf/2402.14327)* a new study employing a comparable segmentation-infusion method. The results are clear: **OG-LLaVA** consistently outperforms SIT, with more than a 25% advantage on compositional reasoning and a 10% edge in visual grounding.  

There‚Äôs also a key difference in usability. **OG-LLaVA** works flexibly both with and without segmentation masks at inference, while SIT requires pre-computed masks every time. This not only adds non-trivial overhead‚Äîsince a separate segmentation model must run first‚Äîbut also makes the system less adaptable. In practice, the reduced token count doesn‚Äôt outweigh the complexity introduced, whereas **OG-LLaVA** preserves efficiency without imposing such constraints.  


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
    <a id="fig-sit-vs-ours"></a>
        {% include figure.liquid loading="eager" path="./assets/img/og_llava_vs_rivals_weighted.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 7: <b> OG-LLaVA </b> vs Subobject Level Image Tokenization and LLaVA-1.5 on Compositional Reasoning and Vision Centric tasks.
</div>



## Qualitative Results


<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/conme_visual.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 2: <b> OG-LLaVA </b> vs LLaVA-1.5 on Compositional Reasoning Benchmark ConMe.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/mmvp_visual.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 3: <b> OG-LLaVA </b> vs LLaVA-1.5 on Vision Grounding benchmark MMVP.
</div>


The images we picked cover all kinds of tricky challenges‚Äîspotting tiny details, telling apart subtle colors, reading depth cues, recognizing materials, making sense of spatial layouts, and even detecting small objects. They‚Äôre designed to push visual‚Äìlanguage reasoning to its limits. What‚Äôs key is that these examples are tested at inference time with no extra fine-tuning, so any boost (or drop) in performance comes purely from the Object-Guided priors built into **OG-LLaVA**.


In [Figure&nbsp;4](#fig-conme-rel-2), [&nbsp;5](#fig-conme-obj) and [&nbsp;6](#fig-conme-rel-1) we highlight a range of cases where **OG-LLaVA** consistently demonstrates sharper perception and more grounded reasoning, from subtle posture cues to tricky color judgments and material recognition.  

- **Fine-grained human pose**: correctly reads a batter‚Äôs stance, placing the bat *up and behind* instead of *in front*. ([Figure&nbsp;4 - Picture1](#fig-conme-rel-2))  
- **Precise color & reflection reasoning**: rules out red in umbrellas, confining it to apples/plate, while the baseline gets misled, as well as captures realistic color reflections on materials and disambiguates different hues. ([Figure&nbsp;4 - Picture2](#fig-conme-rel-2)), ([Figure&nbsp;5 - Picture8](#fig-conme-obj)) & ([Figure&nbsp;6 - Picture2](#fig-conme-rel-1))  
- **Depth-of-field understanding**: detects focus fading *front-to-back* instead of mistakenly *left-to-right*. ([Figure&nbsp;4 - Picture3](#fig-conme-rel-2))    
- **Material recognition**: identifies a skate-park surface as *concrete*, not asphalt and glass instead of curtains. ([Figure&nbsp;4 - Picture4](#fig-conme-rel-2)) & ([Figure&nbsp;5 - Picture1](#fig-conme-rel-2))  
- **Font recognition**: picks up subtle font characteristics, showing solid OCR ability. ([Figure&nbsp;5 - Picture2](#fig-conme-obj))  
- **Spatial reasoning**: accurately locates people and objects in complex scenes. ([Figure&nbsp;5 - Picture3](#fig-conme-obj)) & ([Figure&nbsp;5 - Picture6](#fig-conme-obj))  
- **Object counting & detection**: counts giraffes correctly where the baseline stumbles and spots a distant coffee maker amid clutter, avoiding confusion with a blender. ([Figure&nbsp;5 - Picture4](#fig-conme-obj)) & ([Figure&nbsp;6 - Picture3](#fig-conme-rel-1))  
- **Fashion understanding**: distinguishes between short sleeves and cap sleeves. ([Figure&nbsp;5 - Picture7](#fig-conme-obj))    
- **Dynamic cues**: understands a distant sail is *inflated with strong breeze*, matching the surfing context. ([Figure&nbsp;6 - Picture1](#fig-conme-rel-1)) 
- **Shape recognition**: correctly identifies train tracks in the background. ([Figure&nbsp;6 - Picture4](#fig-conme-rel-1)) 

Together, these examples underline how **OG-LLaVA** moves beyond surface-level cues. It pays attention to fine details, adapts across diverse tasks, and reasons about entire scenes in a way that more closely reflects human understanding.  

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <a id="fig-conme-rel-2"></a>
        {% include figure.liquid loading="eager" path="./assets/img/conme_additional_rel_2.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 4: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Relation examples.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        <a id="fig-conme-obj"></a>
        {% include figure.liquid loading="eager" path="./assets/img/conme_additional_obj.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 5: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Object examples.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
      <a id="fig-conme-rel-1"></a>
        {% include figure.liquid loading="eager" path="./assets/img/conme_additional_rel_1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 6: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Relation examples.
</div>

## Citation

If you use this work, please cite:

```bibtex
@inproceedings{nulli2025objectguided,
title={Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models},
author={Matteo Nulli and Ivona Najdenkoska and Mohammad Mahdi Derakhshani and Yuki M Asano},
booktitle={EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)},
year={2025},
url={https://openreview.net/forum?id=yvY1T3hHEQ}
}
```

**References**
<div id="references-section">

<a id="conme-2024" class="bib-item">Huang Irene, Lin Wei, Mirza M. Jehanzeb, Hansen Jacob A., Doveh Sivan, Butoi Victor Ion, Herzig Roei, Arbelle Assaf, Kuehne Hilde, Darrell Trevor, Gan Chuang, Oliva Aude, Feris Rogerio, Karlinsky Leonid. (2024). Conme: Rethinking Evaluation of Compositional Reasoning for Modern VLMs. arXiv preprint arXiv:2406.08164.</a>

<a id="eyes-wide-shut-2024" class="bib-item">Tong Shengbang, Liu Zhuang, Zhai Yuexiang, Ma Yi, LeCun Yann, Xie Saining. (2024). Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. arXiv preprint arXiv:2401.06209.</a>

<a id="visual-instruction-tuning-2023" class="bib-item">Liu Haotian, Li Chunyuan, Wu Qingyang, Lee Yong Jae. (2023). Visual Instruction Tuning. arXiv preprint arXiv:2304.08485.</a>

<a id="llavonevision-2024" class="bib-item">Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llava-onevision: Easy visual task transfer. Preprint, arXiv:2408.03326.</a>

<a id="qwen2-5-vl-2025" class="bib-item">Bai Shuai, Chen Keqin, Liu Xuejing, Wang Jialin, Ge Wenbin, Song Sibo, Dang Kai, Wang Peng, Wang Shijie, Tang Jun, Zhong Humen, Zhu Yuanzhi, Yang Mingkun, Li Zhaohai, Wan Jianqiang, Wang Pengfei, Ding Wei, Fu Zheren, Xu Yiheng, Ye Jiabo, Zhang Xi, Xie Tianbao, Cheng Zesen, Zhang Hang, Yang Zhibo, Xu Haiyang, Lin Junyang. (2025). Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923.</a>

<a id="qwen3-vl-2025" class="bib-item">QwenTeam. 2025. Qwen3-vl: Sharper vision, deeper thought, broader action.</a>

<a id="internvl2-2024" class="bib-item">OpenGVLab-Team. (2024). InternVL2: Better Than the Best‚ÄîExpanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy. Blog post. URL https://internvl.github.io/blog/2024-07-02-InternVL-2.0/.</a>

<a id="gemma-3-2025" class="bib-item">Gemma-Team. (2025). Gemma 3 Technical Report. arXiv preprint arXiv:2503.19786.</a>

<a id="bags-of-words-vlms-2023" class="bib-item">Yuksekgonul Mert, Bianchi Federico, Kalluri Pratyusha, Jurafsky Dan, Zou James. (2023). When and Why Vision-Language Models Behave Like Bags-of-Words, and What to Do About It? arXiv preprint arXiv:2210.01936.</a>

<a id="icl-compositional-vlms-2024" class="bib-item">Nulli Matteo, Ibrahimi Anesa, Pal Avik, Lee Hoshe, Najdenkoska Ivona. (2024). In-Context Learning Improves Compositional Understanding of Vision-Language Models. In ICML 2024 Workshop on Foundation Models in the Wild. arXiv preprint arXiv:2407.15487.</a>

<a id="nulliobjectguided-2025" class="bib-item">Matteo Nulli, Ivona Najdenkoska, Mohammad Mahdi Derakhshani, and Yuki M Asano. 2025. Objectguided visual tokens: Eliciting compositional reasoning in multimodal language models. In EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)</a>

<a id="vismin-2025" class="bib-item">Awal Rabiul, Ahmadi Saba, Zhang Le, Agrawal Aishwarya. (2025). Vismin: Visual Minimal-Change Understanding. arXiv preprint arXiv:2407.16772.</a>

<a id="cambrian-1-2024" class="bib-item">Tong Shengbang, Brown Ellis, Wu Penghao, Woo Sanghyun, Middepogu Manoj, Akula Sai Charitha, Yang Jihan, Yang Shusheng, Iyer Adithya, Pan Xichen, Wang Austin, Fergus Rob, LeCun Yann, Xie Saining. (2024). Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs. arXiv preprint arXiv:2406.16860.</a>

<a id="llava-next-2024" class="bib-item">Liu Haotian, Li Chunyuan, Li Yuheng, Li Bo, Zhang Yuanhan, Shen Sheng, Lee Yong Jae. (2024). LLaVA-NeXT: Improved Reasoning, OCR, and World Knowledge. Blog post (January 2024). URL https://llava-vl.github.io/blog/2024-01-30-llava-next/.</a>

<a id="sam-2-2024" class="bib-item">Ravi Nikhila, Gabeur Valentin, Hu Yuan-Ting, Hu Ronghang, Ryali Chaitanya, Ma Tengyu, Khedr Haitham, R√§dle Roman, Rolland Chloe, Gustafson Laura, Mintun Eric, Pan Junting, Alwala Kalyan Vasudev, Carion Nicolas, Wu Chao-Yuan, Girshick Ross, Doll√°r Piotr, Feichtenhofer Christoph. (2024). SAM 2: Segment Anything in Images and Videos. arXiv preprint arXiv:2408.00714.</a>

<a id="omg-seg-cvpr-2024" class="bib-item">Li Xiangtai, Yuan Haobo, Li Wei, Ding Henghui, Wu Size, Zhang Wenwei, Li Yining, Chen Kai, Loy Chen Change. (2024). OMG-Seg: Is One Model Good Enough for All Segmentation? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 27948‚Äì27959.</a>

<a id="eagle-2-5-2025" class="bib-item">Chen Guo, Li Zhiqi, Wang Shihao, Jiang Jindong, Liu Yicheng, Lu Lidong, Huang De-An, Byeon Wonmin, Le Matthieu, Rintamaki Tuomas, Poon Tyler, Ehrlich Max, Lu Tong, Wang Limin, Catanzaro Bryan, Kautz Jan, Tao Andrew, Yu Zhiding, Liu Guilin. (2025). EAGLE 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models. arXiv preprint arXiv:2504.15271.</a>

<a id="omg-llava-2024" class="bib-item">Zhang Tao, Li Xiangtai, Fei Hao, Yuan Haobo, Wu Shengqiong, Ji Shunping, Loy Chen Change, Yan Shuicheng. (2024). OMG-LLaVA: Bridging Image-Level, Object-Level, Pixel-Level Reasoning and Understanding. arXiv preprint arXiv:2406.19389.</a>

<a id="sa2va-2025" class="bib-item">Yuan Haobo, Li Xiangtai, Zhang Tao, Huang Zilong, Xu Shilin, Ji Shunping, Tong Yunhai, Qi Lu, Feng Jiashi, Yang Ming-Hsuan. (2025). SA2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos. arXiv preprint arXiv:2501.04001.</a>

<a id="clip-2021" class="bib-item">Radford Alec, Kim Jong Wook, Hallacy Chris, Ramesh Aditya, Goh Gabriel, Agarwal Sandhini, Sastry Girish, Askell Amanda, Mishkin Pamela, Clark Jack, Krueger Gretchen, Sutskever Ilya. (2021). Learning Transferable Visual Models from Natural Language Supervision. arXiv preprint arXiv:2103.00020.</a>

<a id="improved-vit-baselines-2024" class="bib-item">Liu Haotian, Li Chunyuan, Li Yuheng, Lee Yong Jae. (2024). Improved Baselines with Visual Instruction Tuning. arXiv preprint arXiv:2310.03744.</a>

<a id="vit-2021" class="bib-item">Dosovitskiy Alexey, Beyer Lucas, Kolesnikov Alexander, Weissenborn Dirk, Zhai Xiaohua, Unterthiner Thomas, Dehghani Mostafa, Minderer Matthias, Heigold Georg, Gelly Sylvain, Uszkoreit Jakob, Houlsby Neil. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010.11929.</a>

<a id="llama-2-2023" class="bib-item">Touvron Hugo, Martin Louis, Stone Kevin, Albert Peter, Almahairi Amjad, Babaei Yasmine, Bashlykov Nikolay, Batra Soumya, Bhargava Prajjwal, Bhosale Shruti, Bikel Dan, Blecher Lukas, Canton Ferrer Cristian, Chen Moya, Cucurull Guillem, Esiobu David, Fernandes Jude, Fu Jeremy, Fu Wenyin, Fuller Brian, Gao Cynthia, Goswami Vedanuj, Goyal Naman, Hartshorn Anthony, Hosseini Saghar, Hou Rui, Inan Hakan, Kardas Marcin, Kerkez Viktor, Khabsa Madian, Kloumann Isabel, Korenev Artem, Koura Punit Singh, Lachaux Marie-Anne, Lavril Thibaut, Lee Jenya, Liskovich Diana, Lu Yinghai, Mao Yuning, Martinet Xavier, Mihaylov Todor, Mishra Pushkar, Molybog Igor, Nie Yixin, Poulton Andrew, Reizenstein Jeremy, Rungta Rashi, Saladi Kalyan, Schelten Alan, Silva Ruan, Smith Eric Michael, Subramanian Ranjan, Tan Xiao-qing Ellen, Tang Binh, Taylor Ross, Williams Adina, Kuan Jian Xiang, Xu Puxin, Yan Zheng, Zarov Iliyan, Zhang Yuchen, Fan Angela, Kambadur Melanie, Narang Sharan, Rodriguez Aurelien, Stojnic Robert, Edunov Sergey, Scialom Thomas. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models.</a>

<a id="llama-3-2-2024" class="bib-item">Meta. (2024). Llama 3.2: Revolutionizing Edge AI and Vision with Open, Customizable Models. Blog post. URL https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.</a>

<a id="lora-2021" class="bib-item">Hu Edward J., Shen Yelong, Wallis Phillip, Allen-Zhu Zeyuan, Li Yuanzhi, Wang Shean, Wang Lu, Chen Weizhu. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.</a>

<a id="coco-2014" class="bib-item">Lin Tsung-Yi, Maire Michael, Belongie Serge, Hays James, Perona Pietro, Ramanan Deva, Doll√°r Piotr, Zitnick C. Lawrence. (2014). Microsoft COCO: Common Objects in Context. In Computer Vision ‚Äì ECCV 2014, pages 740‚Äì755. Springer.</a>

<a id="image-descriptions-2014" class="bib-item">Young Peter, Lai Alice, Hodosh Micah, Hockenmaier Julia. (2014). From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference Over Event Descriptions. Transactions of the Association for Computational Linguistics, 2:67‚Äì78.</a>

<a id="visual-genome-2017" class="bib-item">Krishna Ranjay, Zhu Yuke, Groth Oliver, Johnson Justin, Hata Kenji, Kravitz Joshua, Chen Stephanie, Kalantidis Yannis, Li Li-Jia, Shamma David A., et al. (2017). Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision, 123:32‚Äì73.</a>

<a id="gqa-2019" class="bib-item">Hudson Drew A., Manning Christopher D. (2019). GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6700‚Äì6709.</a>

<a id="sugar-crepe-2023" class="bib-item">Hsieh Cheng-Yu, Zhang Jieyu, Ma Zixian, Kembhavi Aniruddha, Krishna Ranjay. (2023). SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality. Advances in Neural Information Processing Systems, 36:31096‚Äì31116.</a>

<a id="gpt-4-technical-report-2024" class="bib-item">OpenAI. (2024). GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.</a>

<a id="scaling-instruction-finetuned-2024" class="bib-item">Chung Hyung Won, Hou Le, Longpre Shayne, Zoph Barret, Tay Yi, Fedus William, Li Yunxuan, Wang Xuezhi, Dehghani Mostafa, Brahma Siddhartha, et al. (2024). Scaling Instruction-Finetuned Language Models. Journal of Machine Learning Research, 25(70):1‚Äì53.</a>

<a id="diagram-2016" class="bib-item">Kembhavi Aniruddha, Salvato Mike, Kolve Eric, Seo Minjoon, Hajishirzi Hannaneh, Farhadi Ali. (2016). A Diagram is Worth a Dozen Images. arXiv preprint arXiv:1603.07396.</a>

<a id="mme-2024" class="bib-item">Fu Chaoyou, Bird Peixian, Shen Yunhang, Qin Yulei, Zhang Mengdan, Lin Xu, Yang Jinrui, Zheng Xiawu, Li Ke, Sun Xing, Wu Yunsheng, Ji Rongrong. (2024). MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv preprint arXiv:2306.13394.</a>

<a id="evaluating-vlms-right-way-2024" class="bib-item">Chen Lin, Li Jinsong, Dong Xiaoyi, Zhang Pan, Zang Yuhang, Chen Zehui, Duan Haodong, Wang Jiaqi, Qiao Yu, Lin Dahua, Zhao Feng. (2024). Are We on the Right Way for Evaluating Large Vision-Language Models? arXiv preprint arXiv:2403.20330.</a>

<a id="mmbench-2024" class="bib-item">Liu Yuan, Duan Haodong, Zhang Yuanhan, Li Bo, Zhang Songyang, Zhao Wangbo, Yuan Yike, Wang Jiaqi, He Conghui, Liu Ziwei, Chen Kai, Lin Dahua. (2024). MMBench: Is Your Multi-Modal Model an All-Around Player? arXiv preprint arXiv:2307.06281.</a>

<a id="subobject-tokenization-2025" class="bib-item">Chen Delong, Cahyawijaya Samuel, Liu Jianfeng, Wang Baoyuan, Fung Pascale. (2025). Subobject-Level Image Tokenization. arXiv preprint arXiv:2402.14327.</a>

<a id="deepspeed-2020" class="bib-item">Rasley Jeff, Rajbhandari Samyam, Ruwase Olatunji, He Yuxiong. (2020). DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (KDD ‚Äô20), pages 3505‚Äì3506. doi:10.1145/3394486.3406703.</a>

<a id="zero-2020" class="bib-item">Rajbhandari Samyam, Rasley Jeff, Ruwase Olatunji, He Yuxiong. (2020). ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1‚Äì16. doi:10.1109/SC41405.2020.00024.</a>

<a id="adam-2017" class="bib-item">Kingma Diederik P., Ba Jimmy. (2017). Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980.</a>

<a id="adamw-2019" class="bib-item">Loshchilov Ilya, Hutter Frank. (2019). Decoupled Weight Decay Regularization. arXiv preprint arXiv:1711.05101.</a>

<a id="bert-2019" class="bib-item">Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina. (2019). BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT 2019, pages 4171‚Äì4186.</a>

<a id="attention-is-all-you-need-2017" class="bib-item">Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob, Jones Llion, Gomez Aidan N., Kaiser ≈Åukasz, Polosukhin Illia. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems, 30.</a>

<a id="pixtral-12b-2024" class="bib-item">Agrawal Pravesh, Antoniak Szymon, Bou Hanna Emma, Bout Baptiste, Chaplot Devendra, Chudnovsky Jessica, Costa Diogo, De Monicault Baudouin, Garg Saurabh, Gervet Theophile, Ghosh Soham, H√©liou Am√©lie, Jacob Paul, Jiang Albert Q., Khandelwal Kartik, Lacroix Timoth√©e, Lample Guillaume, Las Casas Diego, Lavril Thibaut, Le Scao Teven, Lo Andy, Marshall Louis, Martin Arthur, Mensch Arthur, Muddireddy Pavankumar, Nemychnikova Valera, Pellat Marie, Von Platen Patrick, Raghuraman Nikhil, Bout Rozi√®re Baptiste, Sablayrolles Alexandre, Saulnier Lucile, Sauvestre Romain, Rozi√®re Baptiste, Shang Wendy, Soletskyi Roman, Stewart Lawrence, Stock Pierre, Studnia Joachim, Subramanian Sandeep, Vaze Sagar, Wang Thomas, Yang Sophia. (2024). Pixtral 12B. arXiv preprint arXiv:2410.07073.</a>

<a id="roformer-2023" class="bib-item">Su Jianlin, Lu Yu, Pan Shengfeng, Murtadha Ahmed, Wen Bo, Liu Yunfeng. (2023). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint arXiv:2104.09864.</a>

<a id="blip2-2023" class="bib-item">Li J, Li D, Savarese S, Hoi S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. InInternational conference on machine learning 2023</a>

<a id="llama-3-herd-2024" class="bib-item">Dubey Abhimanyu, et al. (2024). The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783.</a>

<a id="reproducible-scaling-laws-2023" class="bib-item">Cherti Mehdi, Beaumont Romain, Wightman Ross, Wortsman Mitchell, Ilharco Gabriel, Gordon Cade, Schuhmann Christoph, Schmidt Ludwig, Jitsev Jenia. (2023). Reproducible Scaling Laws for Contrastive Language-Image Learning. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818‚Äì2829. doi:10.1109/CVPR52729.2023.00276.</a>

<a id="sigmoid-loss-2023" class="bib-item">Zhai Xiaohua, Mustafa Basil, Kolesnikov Alexander, Beyer Lucas. (2023). Sigmoid Loss for Language Image Pre-Training. arXiv preprint arXiv:2303.15343.</a>

<a id="dinov2-2024" class="bib-item">Oquab Maxime, Darcet Timoth√©e, Moutakanni Th√©o, Vo Huy, Szafraniec Marc, Khalidov Vasil, Fernandez Pierre, Haziza Daniel, Massa Francisco, El-Nouby Alaaeldin, Assran Mahmoud, Ballas Nicolas, Galuba Wojciech, Misra Ishan, Rabbat Michael, Sharma Vasu, Synnaeve Gabriel, Xu Hu, Jegou Herv√©, Mairal Julien, Labatut Patrick, Joulin Armand, Bojanowski Piotr. (2024). DINOv2: Learning Robust Visual Features Without Supervision. arXiv preprint arXiv:2304.07193.</a>

<a id="internlm2-2024" class="bib-item">Cai Zheng, Cao Maosong, Chen Haojiong, Chen Kai, Chen Keyu, Chen Xin, Chen Xun, Chen Zehui, Chen Zhi, Chu Pei, Dong Xiaoyi, Duan Haodong, Fan Qi, Fei Zhaoye, Gao Yang, Ge Jiaye, Gu Chenya, Gu Yuzhe, Gui Tao, Guo Aijia, Guo Qipeng, He Conghui, Hu Yingfan, Huang Ting, Jiang Tao, Jiao Penglong, Jin Zhenjiang, Lei Zhikai, Li Jiaxing, Li Jingwen, Li Linyang, Li Shuaibin, Li Wei, Li Yining, Liu Hongwei, Liu Jiawei, Liu Kaiwen, Liu Kuikun, Liu Xiaoran, Lv Chengqi, Lv Haijun, Lv Kai, Ma Li, Ma Runyuan, Ma Zerun, Ning Wenchang, Ouyang Linke, Qiu Jiantao, Qu Yuan, Shang Fukai, Shao Yunfan, Song Demin, Song Zifan, Sui Zhihao, Sun Peng, Sun Yu, Tang Huanze, Wang Bin, Wang Guoteng, Wang Jiaqi, Wang Jiayu, Wang Rui, Wang Yudong, Wang Ziyi, Wei Xingjian, Weng Qizhen, Wu Fan, Xiong Yingtong, Xu Chao, Xu Ruiliang, Yan Hang, Yan Yirong, Yang Xiaogui, Ye Haochen, Ying Huaiyuan, Yu Jia, Yu Jing, Zang Yuhang, Zhang Chuyu, Zhang Li, Zhang Pan, Zhang Peng, Zhang Ruijie, Zhang Shuo, Zhang Songyang, Zhang Wenjian, Zhang Wenwei, Zhang Xingcheng, Zhang Xinyue, Zhao Hui, Zhao Qian, Zhao Xiaomeng, Zhao Fengzhe, Zhou Zaida, Zhou Jingming, Zhuo Jingming, Zou Yicheng, Qiu Xipeng, Qiao Yu, Lin Dahua. (2024). InternLM2 Technical Report. arXiv preprint arXiv:2403.17297.</a>

<a id="omg-seg-arxiv-2024" class="bib-item">Li Xiangtai, Yuan Haobo, Li Wei, Ding Henghui, Wu Size, Zhang Wenwei, Li Yining, Chen Kai, Loy Chen Change. (2024). OMG-Seg: Is One Model Good Enough for All Segmentation? arXiv preprint arXiv:2401.10229.</a>

<a id="seem-2023" class="bib-item">Zou Xueyan, Yang Jianwei, Zhang Hao, Li Feng, Li Linjie, Wang Jianfeng, Wang Lijuan, Gao Jianfeng, Lee Yong Jae. (2023). Segment Everything Everywhere All at Once. arXiv preprint arXiv:2304.06718.</a>

<a id="siglip-2024" class="bib-item">Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer. Sigmoid Loss for Language Image Pre-Training, 2024. URL https://arxiv.org/abs/2303.15343.</a>

<a id="fastvlms-2025" class="bib-item">Vasu, Pavan Kumar Anasosalu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam et al. "Fastvlm: Efficient vision encoding for vision language models." In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 19769-19780. 2025.</a>

</div>


<style>
  /* Hide all references by default */
  .bib-item { display: none; }
  /* Show only the ones with the 'cited' class */
  .bib-item.cited { display: block; margin-bottom: 10px; }
</style>

<script>
document.addEventListener("DOMContentLoaded", function() {
    // 1. Find all internal links in the post (usually starting with #)
    const links = document.querySelectorAll('a[href^="#"]');
    const citedIds = new Set();

    links.forEach(link => {
        // Get the ID being linked to (remove the # character)
        const id = link.getAttribute('href').substring(1);
        if (id) citedIds.add(id);
    });

    // 2. Loop through all reference items
    const refItems = document.querySelectorAll('.bib-item');
    refItems.forEach(item => {
        if (citedIds.has(item.id)) {
            item.classList.add('cited'); // This makes it visible via CSS
        }
    });
});
</script>