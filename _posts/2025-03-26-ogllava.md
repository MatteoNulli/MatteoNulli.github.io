---
layout: post
title: "Object-Guided Visual Tokens:<br> Eliciting Compositional Reasoning<br>in Multimodal Language Models"
date: 2025-09-05 14:24:00
description: Addressing shortcomings of MLLMs in Compositional Reasoning through Segmentation
tags: Multimodal-Learning Compositional-Reasoning Visual-Grounding
# categories: Multimodal-Learning
---


##### M. Nulli, I. Najdenkoska, M. M. Derakhshani, M. Dorkenwald, V. Orshulevich, Y. M. Asano
###### Link to [Paper](https://github.com/MatteoNulli/og_llava/blob/main/paper/LongPaper.pdf)
<br>

## Motivation

Most Multimodal Large Language Models (MLLMs) use contrastively pre-trained vision encoders.  
They work well on many tasks, but often struggle when it comes to **compositional understanding** and **reasoning** about what‚Äôs actually in an image.  
That‚Äôs because these encoders are mainly trained for image‚Äìcaption retrieval, not for truly breaking down and understanding all parts of a scene.  

Another issue is efficiency: state-of-the-art vision encoders generate **2‚Äì3x more visual tokens**, which slows down both training and inference.  

To tackle these problems, we introduce **OG-LLaVA (Object-Guided LLaVA)**.  
With our new connector design, **`OG-Fusion`**, the model can reason about visual content more effectively‚Äîwithout adding lots of extra tokens or fine-tuning the vision encoder itself.  

At the core of `OG-Fusion` is a simple but powerful idea: combine **CLIP representations** with **segmentation masks**.  
This lets OG-LLaVA leverage the descriptive strength of segmentation models to better capture **object relationships** and **spatial arrangements**.  

The result?  
**OG-LLaVA outperforms existing models on tasks that demand deeper visual reasoning and grounding**, all while staying efficient.

## Main Process

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/ogllava_3.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 1: <b> OG-LLaVA </b> architecture with <code>OG-Fusion</code> internal proces
</div>


We extract visual features from the input image through a Vision Encoder.  
Concurrently, we pass the input image through `OG-Fusion`. Here we:  
1. Use a Segmentation model to retrieve the masks,  
2. Downsample the segmentations, and  
3. Apply these masks onto the visual features.  
4. Concatenated together and passed through a Multi-Layer Perceptron to produce Object-Guided Visual Tokens (**_OGVT_**).  

The **_OGVT_** are then given as input to a Large Language Model together with Textual Tokens to produce an output.  
The ‚ùÑÔ∏è (snowflake) and üî• (fire) represent modules whose parameters are kept **frozen** or **turned on**.  
LoRA emphasizes that not all parameters of the LLM are unfrozen, only the LoRA layers.

## Visualizations

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/conme_visual.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 2: OG-LLaVA vs LLaVA-1.5 on ConMe Replace-Attribute examples.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/mmvp_visual.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 3: OG-LLaVA vs LLaVA-1.5 on MMVP examples.
</div>

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/conme_additional_rel_1.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 4: OG-LLaVA vs LLaVA-1.5 on ConMe Replace-Relation examples.
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/conme_additional_obj.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 5: OG-LLaVA vs LLaVA-1.5 on ConMe Replace-Object examples.
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/conme_additional_rel_2.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 6: OG-LLaVA vs LLaVA-1.5 on ConMe Replace-Relation examples.
</div>


## Results

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/main-table.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Table 1: OG-LLaVA performance on Compositional Reasoning and Vision Centric tasks compared with LLaVA baselines.
</div>
<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="./assets/img/og_llava_vs_rivals_weighted.png" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 7: OG-LLaVA vs Subobject Level Image Tokenization and LLaVA-1.5 on Compositional Reasoning and Vision Centric tasks.
</div>


## Citation

If you use this work, please cite:

```bibtex
@online{nulli2025ogllava,
  author  = {Nulli, Matteo and Najdenkoska, I., and Derakhshani, M. M., and Dorkenwald, M., and Asano, Y. M.},
  title   = {Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models},
  url     = {https://matteonulli.github.io/blog/2025/ogllava/},
  year    = {2025},
  urldate = {2025-09-05}
}
```
