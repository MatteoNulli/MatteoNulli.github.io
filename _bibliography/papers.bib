@inproceedings{nulli2025objectguided,
title={Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models},
author={Matteo Nulli and Ivona Najdenkoska and Mohammad Mahdi Derakhshani and Yuki M Asano},
booktitle={EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)},
year={2025},
selected={true},
abbr={PriGM@EurIPS},
bibtex_show={true},
url={https://openreview.net/forum?id=yvY1T3hHEQ},
html={https://openreview.net/forum?id=yvY1T3hHEQ},
preview={ogllava_3.png},
pdf={https://openreview.net/pdf?id=yvY1T3hHEQ},
abstract={Multimodal Large Language Models (MLLMs) employ contrastive pre-trained Vision Encoders whose performance falls short in compositional understanding and visual reasoning. This is mostly due to their pre-training objective aimed at retrieval between similar images or captions rather than an in-depth understanding of all components of an image. Moreover, while state-of-the-art image encoding methods yield strong performance, they inflate the number of visual input tokens by roughly two to three times, thereby significantly lengthening both training and inference times. To alleviate these issues, we present OG-LLaVA (Object-Guided LLaVA), a novel multimodal architecture which, through an innovative connector design OG-Fusion, enhances the model’s ability to understand and reason about visual content without substantially increasing the number of tokens or unfreezing the Vision Encoder. A core element of OG-Fusion is the combination of CLIP representations with segmentations. By leveraging the descriptive power of advanced segmentation models, OG-LLaVA attains superior performance at tasks that require a deeper understanding of object relationships and spatial arrangements, within the domains of compositional reasoning and visual grounding. The code is available at https://github.com/MatteoNulli/og_llava/tree/main.}
}
@article{vincenti2024dynamic,
title={Dynamic Vocabulary Pruning in Early-Exit LLMs}, 
author={Jort Vincenti* and Karim Abdel Sadek* and Joan Velja* and Matteo Nulli* and Metod Jazbec},
year={2024},
bibtex_show={true},
eprint={2410.18952},
archivePrefix={arXiv},
primaryClass={cs.CL},
journal={NeurIPS Efficient Natural Language and Speech Processing},
url={https://arxiv.org/abs/2410.18952},
html={https://arxiv.org/abs/2410.18952},
selected={true},
abbr={ENLSP@NeurIPS},
preview={ee-nips.png},
pdf={https://arxiv.org/pdf/2410.18952}, 
abstract={Increasing the size of large language models (LLMs) has been shown to lead to better performance. However, this comes at the cost of slower and more expensive inference. Early-exiting is a promising approach for improving the efficiency of LLM inference by enabling next token prediction at intermediate layers. Yet, the large vocabulary size in modern LLMs makes the confidence estimation required for exit decisions computationally expensive, diminishing the efficiency gains. To address this, we propose dynamically pruning the vocabulary at test time for each token. Specifically, the vocabulary is pruned at one of the initial layers, and the smaller vocabulary is then used throughout the rest of the forward pass. Our experiments demonstrate that such post-hoc dynamic vocabulary pruning improves the efficiency of confidence estimation in early-exit LLMs while maintaining competitive performance.}
}
@inproceedings{nulli2024context,
title={In-Context Learning Improves Compositional Understanding of Vision-Language Models},
author={Nulli, Matteo and Ibrahimi, Anesa and Pal, Avik and Lee, Hoshe and Najdenkoska, Ivona},
bibtex_show={true},
booktitle={ICML 2024 Workshop on Foundation Models in the Wild},
year={2024},
eprint={2407.15487},
archivePrefix={arXiv},
primaryClass={cs.CV},
url={https://arxiv.org/abs/2407.15487}, 
html={https://arxiv.org/abs/2407.15487}, 
selected={true},
preview={vlm_comp.png},
video={https://www.linkedin.com/feed/update/urn:li:activity:7238837758444601344/},
abbr={FMW@ICML},
pdf={https://arxiv.org/pdf/2407.15487},
abstract={Vision-Language Models (VLMs) have shown remarkable capabilities in a large number of downstream tasks. Nonetheless, compositional image understanding remains a rather difficult task due to the object bias present in training data. In this work, we investigate the reasons for such a lack of capability by performing an extensive bench-marking of compositional understanding in VLMs. We compare contrastive models with generative ones and analyze their differences in architecture, pre-training data, and training tasks and losses. Furthermore, we leverage In-Context Learning (ICL) as a way to improve the ability of VLMs to perform more complex reasoning and understanding given an image. Our extensive experiments demonstrate that our proposed approach outperforms baseline models across multiple compositional understanding datasets.}
}

@article{sadek2024explaining,
title={'Explaining {RL} Decisions with Trajectories{\textquoteright}: A Reproducibility Study},
author={Karim Abdel Sadek* and Matteo Nulli* and Joan Velja* and Jort Vincenti*},
journal={Transactions on Machine Learning Research},
issn={2835-8856},
year={2024},
url={https://arxiv.org/abs/2411.07200},
html={https://arxiv.org/abs/2411.07200},
note={Reproducibility Certification},
pdf={https://arxiv.org/pdf/2411.07200},
bibtex_show={true},
selected={true},
abbr={TMLR},
preview={xrl_repro.png},
abstract={This work investigates the reproducibility of the paper "Explaining RL decisions with trajectories“ by Deshmukh et al. (2023). The original paper introduces a novel approach in explainable reinforcement learning based on the attribution decisions of an agent to specific clusters of trajectories encountered during training. We verify the main claims from the paper, which state that (i) training on less trajectories induces a lower initial state value, (ii) trajectories in a cluster present similar high-level patterns, (iii) distant trajectories influence the decision of an agent, and (iv) humans correctly identify the attributed trajectories to the decision of the agent. We recover the environments used by the authors based on the partial original code they provided for one of the environments (Grid-World), and implemented the remaining from scratch (Seaquest and HalfCheetah, Breakout, Q*Bert). While we confirm that (i), (ii), and (iii) partially hold, we extend on the largely qualitative experiments from the authors by introducing a quantitative metric to further support (iii), and new experiments and visual results for (i). Moreover, we investigate the use of different clustering algorithms and encoder architectures to further support (ii). We could not support (iv), given the limited extent of the original experiments. We conclude that, while some of the claims can be supported, further investigations and experiments could be of interest. We recognize the novelty of the work from the authors and hope that our work paves the way for clearer and more transparent approaches.}
}


