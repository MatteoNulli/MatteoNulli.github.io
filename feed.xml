<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://matteonulli.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://matteonulli.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-10-26T11:59:08+00:00</updated><id>https://matteonulli.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Object-Guided Visual Tokens: Eliciting Compositional Reasoningin Multimodal Language Models</title><link href="https://matteonulli.github.io/blog/2025/ogllava/" rel="alternate" type="text/html" title="Object-Guided Visual Tokens: Eliciting Compositional Reasoningin Multimodal Language Models"/><published>2025-09-05T14:24:00+00:00</published><updated>2025-09-05T14:24:00+00:00</updated><id>https://matteonulli.github.io/blog/2025/ogllava</id><content type="html" xml:base="https://matteonulli.github.io/blog/2025/ogllava/"><![CDATA[<h5 id="m-nulli-i-najdenkoska-m-m-derakhshani-v-orshulevich-y-m-asano">M. Nulli, I. Najdenkoska, M. M. Derakhshani, V. Orshulevich, Y. M. Asano</h5> <h6 id="university-of-amsterdam-ebay-university-of-technology-nuremberg">University of Amsterdam, eBay, University of Technology Nuremberg</h6> <h6 id="links--long-paper---blogpost---code">Links: 📄 <a href="https://github.com/MatteoNulli/og_llava/blob/main/paper/LongPaper.pdf">Long Paper</a> | 📝 <a href="https://matteonulli.github.io/blog/2025/ogllava/">Blogpost</a> | 🧑‍💻 <a href="https://github.com/MatteoNulli/og_llava/tree/main">Code</a></h6> <p><br/></p> <h2 id="motivation">Motivation</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="mainfigure"></a> <figure> <picture> <img src="/assets/img/ogllava_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: <b> OG-LLaVA </b> architecture with <code>OG-Fusion</code> internal proces </div> <p>Most Multimodal Large Language Models (MLLMs) use contrastively pre-trained vision encoders.<br/> They work well on many tasks, but often struggle when it comes to <strong>compositional understanding</strong> and <strong>reasoning</strong> about what’s actually in an image. That’s because these encoders are mainly trained for image–caption retrieval, not for truly breaking down and understanding all parts of a scene. Another issue is efficiency: state-of-the-art vision encoders generate <strong>2–3x more visual tokens</strong>, which slows down both training and inference.</p> <p>To tackle these problems, we introduce <strong>OG-LLaVA (Object-Guided <a href="https://arxiv.org/pdf/2310.03744">LLaVA</a>)</strong>. With our new connector design, <strong><code class="language-plaintext highlighter-rouge">OG-Fusion</code></strong>, the model can reason about visual content more effectively—without adding lots of extra tokens or fine-tuning the vision encoder itself. At the core of <code class="language-plaintext highlighter-rouge">OG-Fusion</code> is a simple but powerful idea: combine <strong>CLIP representations</strong> with <strong>segmentation masks</strong>. This lets OG-LLaVA leverage the descriptive strength of segmentation models to better capture <strong>object relationships</strong> and <strong>spatial arrangements</strong>. The result? <strong>OG-LLaVA outperforms existing comparable models on tasks that demand deeper visual reasoning and grounding</strong>, all while staying efficient.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/conme_visual.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: <b> OG-LLaVA </b> vs LLaVA-1.5 on Compositional Reasoning Benchmark ConMe. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/mmvp_visual.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3: <b> OG-LLaVA </b> vs LLaVA-1.5 on Vision Grounding benchmark MMVP. </div> <h2 id="underlying-procedure">Underlying Procedure</h2> <p>As illustrated in <a href="#mainfigure">Figure 1</a>, we extract visual features from the input image through a Vision Encoder. Concurrently, we pass the input image through <code class="language-plaintext highlighter-rouge">OG-Fusion</code>. Here we:</p> <ol> <li>Use a Segmentation model to retrieve the masks,</li> <li>Downsample the segmentations, and</li> <li>Apply these masks onto the visual features.</li> <li>Concatenated together and passed through a Multi-Layer Perceptron to produce Object-Guided Visual Tokens (<strong><em>OGVT</em></strong>).</li> </ol> <p>The <strong><em>OGVT</em></strong> are then given as input to a Large Language Model together with Textual Tokens to produce an output.<br/> The ❄️ (snowflake) and 🔥 (fire) symbols in <a href="#mainfigure">Figure 1</a> represent modules whose parameters are kept <strong>frozen</strong> or <strong>turned on</strong>.<br/> LoRA emphasizes that not all parameters of the LLM are unfrozen, only the LoRA layers.</p> <h2 id="visualizations">Visualizations</h2> <p>The images we picked cover all kinds of tricky challenges—spotting tiny details, telling apart subtle colors, reading depth cues, recognizing materials, making sense of spatial layouts, and even detecting small objects. They’re designed to push visual–language reasoning to its limits. What’s key is that these examples are tested at inference time with no extra fine-tuning, so any boost (or drop) in performance comes purely from the Object-Guided priors built into <strong>OG-LLaVA</strong>.</p> <p>In <a href="#fig-conme-rel-2">Figure 4</a>, <a href="#fig-conme-obj"> 5</a> and <a href="#fig-conme-rel-1"> 6</a> we highlight a range of cases where <strong>OG-LLaVA</strong> consistently demonstrates sharper perception and more grounded reasoning, from subtle posture cues to tricky color judgments and material recognition.</p> <ul> <li><strong>Fine-grained human pose</strong>: correctly reads a batter’s stance, placing the bat <em>up and behind</em> instead of <em>in front</em>. (<a href="#fig-conme-rel-2">Figure 4 - Picture1</a>)</li> <li><strong>Precise color &amp; reflection reasoning</strong>: rules out red in umbrellas, confining it to apples/plate, while the baseline gets misled, as well as captures realistic color reflections on materials and disambiguates different hues. (<a href="#fig-conme-rel-2">Figure 4 - Picture2</a>), (<a href="#fig-conme-obj">Figure 5 - Picture8</a>) &amp; (<a href="#fig-conme-rel-1">Figure 6 - Picture2</a>)</li> <li><strong>Depth-of-field understanding</strong>: detects focus fading <em>front-to-back</em> instead of mistakenly <em>left-to-right</em>. (<a href="#fig-conme-rel-2">Figure 4 - Picture3</a>)</li> <li><strong>Material recognition</strong>: identifies a skate-park surface as <em>concrete</em>, not asphalt and glass instead of curtains. (<a href="#fig-conme-rel-2">Figure 4 - Picture4</a>) &amp; (<a href="#fig-conme-rel-2">Figure 5 - Picture1</a>)</li> <li><strong>Font recognition</strong>: picks up subtle font characteristics, showing solid OCR ability. (<a href="#fig-conme-obj">Figure 5 - Picture2</a>)</li> <li><strong>Spatial reasoning</strong>: accurately locates people and objects in complex scenes. (<a href="#fig-conme-obj">Figure 5 - Picture3</a>) &amp; (<a href="#fig-conme-obj">Figure 5 - Picture6</a>)</li> <li><strong>Object counting &amp; detection</strong>: counts giraffes correctly where the baseline stumbles and spots a distant coffee maker amid clutter, avoiding confusion with a blender. (<a href="#fig-conme-obj">Figure 5 - Picture4</a>) &amp; (<a href="#fig-conme-rel-1">Figure 6 - Picture3</a>)</li> <li><strong>Fashion understanding</strong>: distinguishes between short sleeves and cap sleeves. (<a href="#fig-conme-obj">Figure 5 - Picture7</a>)</li> <li><strong>Dynamic cues</strong>: understands a distant sail is <em>inflated with strong breeze</em>, matching the surfing context. (<a href="#fig-conme-rel-1">Figure 6 - Picture1</a>)</li> <li><strong>Shape recognition</strong>: correctly identifies train tracks in the background. (<a href="#fig-conme-rel-1">Figure 6 - Picture4</a>)</li> </ul> <p>Together, these examples underline how <strong>OG-LLaVA</strong> moves beyond surface-level cues. It pays attention to fine details, adapts across diverse tasks, and reasons about entire scenes in a way that more closely reflects human understanding.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-conme-rel-2"></a> <figure> <picture> <img src="/assets/img/conme_additional_rel_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Relation examples. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-conme-obj"></a> <figure> <picture> <img src="/assets/img/conme_additional_obj.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Object examples. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-conme-rel-1"></a> <figure> <picture> <img src="/assets/img/conme_additional_rel_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Relation examples. </div> <h2 id="results">Results</h2> <p>Our results on compositional reasoning and vision-centric benchmarks <a href="#tab-main-res">Table 1</a>, show that <strong>OG-LLaVA</strong> consistently outperforms its baselines, across both <em><a href="https://arxiv.org/pdf/2310.03744">LLaVA-1.5</a></em> and <em><a href="https://arxiv.org/pdf/2406.16860">Cambrian-1</a></em> training setups. The improvements are not marginal—they’re large and systematic.</p> <ul> <li><strong>Compositional understanding</strong> <ul> <li><strong>ARO</strong>: <ul> <li>+21% on <em>Coco-Order</em> (38.2 → 82.6) and +16% on <em>Flickr-Order</em> (49.1 → 84.0).</li> <li><em>Visual Genome Attribution</em> on average +10% across backbones and on <em>Visual Genome Relation</em> +20% across training data and model sizes.</li> </ul> </li> <li><strong>ConME</strong>: steady +2% gains, peaking at 65.2 in the 8B setting (+3.6 over the strongest baseline).</li> </ul> </li> <li><strong>Vision-centric reasoning</strong> <ul> <li><strong>MMVP</strong>: about +3 points on average (e.g. 32.0 → 37.0 in 8B, 61.6 → 66.0 with <em>Cambrian-1</em> data).</li> <li><strong>CVBench</strong>: stable performance, with only ±1 point fluctuations.</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="tab-main-res"></a> <figure> <picture> <img src="/assets/img/main-table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 1: <b> OG-LLaVA </b> performance on Compositional Reasoning and Vision Centric tasks compared with LLaVA baselines. </div> <p>In <a href="#fig-sit-vs-ours">Figure 7</a>, we compare <strong>OG-LLaVA-8B</strong> with SIT-8B, and LLaVA-1.5-8B under the same backbone. SIT-8B stands for <em><a href="https://arxiv.org/pdf/2402.14327">Subobject-level Image Tokenization (SIT)</a></em> a new study employing a comparable segmentation-infusion method. The results are clear: <strong>OG-LLaVA</strong> consistently outperforms SIT, with more than a 25% advantage on compositional reasoning and a 10% edge in visual grounding.</p> <p>There’s also a key difference in usability. <strong>OG-LLaVA</strong> works flexibly both with and without segmentation masks at inference, while SIT requires pre-computed masks every time. This not only adds non-trivial overhead—since a separate segmentation model must run first—but also makes the system less adaptable. In practice, the reduced token count doesn’t outweigh the complexity introduced, whereas <strong>OG-LLaVA</strong> preserves efficiency without imposing such constraints.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-sit-vs-ours"></a> <figure> <picture> <img src="/assets/img/og_llava_vs_rivals_weighted.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 7: <b> OG-LLaVA </b> vs Subobject Level Image Tokenization and LLaVA-1.5 on Compositional Reasoning and Vision Centric tasks. </div> <h2 id="citation">Citation</h2> <p>If you use this work, please cite:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">nulli2025ogllava</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Nulli, M. and Najdenkoska, I., and Derakhshani, M. M., and Dorkenwald, M., Orshulevich, V., and Asano, Y. M.}</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models}</span><span class="p">,</span>
  <span class="na">howpublished</span>  <span class="p">=</span> <span class="s">{https://matteonulli.github.io/blog/2025/ogllava/}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
  <span class="na">note</span> <span class="p">=</span> <span class="s">{Accessed: 2025-09-05}</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="Multimodal-Learning"/><category term="Compositional-Reasoning"/><category term="Visual-Grounding"/><summary type="html"><![CDATA[Addressing shortcomings of MLLMs in Compositional Reasoning through Segmentation]]></summary></entry><entry><title type="html">Model Compression for Machine Translation in Large Language Models</title><link href="https://matteonulli.github.io/blog/2024/alma/" rel="alternate" type="text/html" title="Model Compression for Machine Translation in Large Language Models"/><published>2024-11-05T14:24:00+00:00</published><updated>2024-11-05T14:24:00+00:00</updated><id>https://matteonulli.github.io/blog/2024/alma</id><content type="html" xml:base="https://matteonulli.github.io/blog/2024/alma/"><![CDATA[<h5 id="j-vincenti-m-nulli-a-tragoudaras-z-tzifa-kratira-and-a-b-gomez">J. Vincenti, M. Nulli, A. Tragoudaras, Z. Tzifa-Kratira, and A. B. Gomez</h5> <h6 id="university-of-amsterdam">University of Amsterdam</h6> <h6 id="links--paper---blogpost---code">Links: 📄 <a href="https://www.overleaf.com/read/dkkcbhpycncf#bec4c7">Paper</a> | 📝 <a href="https://matteonulli.github.io/blog/2024/alma/">Blogpost</a> | 🧑‍💻 <a href="https://github.com/JortVincenti/ALMA">Code</a></h6> <p><br/></p> <h2 id="motivation">Motivation</h2> <p>Small traditional machine translation models can produce high-quality translations in a one-to-one language setting [<a href="https://arxiv.org/pdf/2207.04672">1</a>]. However, when scaling these models to handle multiple languages, they often struggle to perform well due to the limited amount of parallel data and the complex architectures required to fully understand the task. In contrast, Large Language Models (LLMs) [<a href="https://arxiv.org/pdf/2005.14165">2</a>] can handle complex settings due to their large architectures and overcome smaller datasets by leveraging their large English training data. As a result, ALMA is a fine-tuned LLM designed for machine translation across multiple language directions from and to English and is the first LLM to become competitive with traditional machine translation models [<a href="https://arxiv.org/pdf/2309.11674">3</a>].</p> <p>Despite performing remarkably well in translation, ALMA incurs significant computational and memory costs, making even inference prohibitively expensive to scale to consumer hardwar [<a href="https://arxiv.org/pdf/2308.07633">4</a>]. To address these challenges, several compression techniques have been proposed for LLMs. Among many others, these include quantization [<a href="https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/720541/&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=4619659333798904151&amp;ei=4Pn9aLLLGNyOieoP1t-zoQM&amp;scisig=ABGrvjJbo0ZiWOaWn3QOaSjwLpbc">5</a>, <a href="https://scholar.google.com/scholar_url?url=http://proceedings.mlr.press/v202/xiao23c.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=17814731842610185667&amp;ei=-vn9aImlILmAieoPlr2tqAg&amp;scisig=ABGrvjIl4KLQN2x_Kith27oHWlKF">6</a>] and pruning [<a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=4588992562541514941&amp;ei=Efr9aNeCM_rUieoPvMbB8Qc&amp;scisig=ABGrvjKlhw7h_hc2pm1eDaB_57Jp">7</a>, <a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=6338930303179684776&amp;ei=I_r9aIX5F77WieoPn4GZgQE&amp;scisig=ABGrvjJDL40cEEu6_3RiqtrpmeNY">8</a>, <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/1510.00149&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=7860777411990654691&amp;ei=Mfr9aKfnEv2lieoP1Jf4uAw&amp;scisig=ABGrvjKoCGzYeTPmCdCI6xO9Hgxo">9</a>] methods. However, none of these techniques have been applied to ALMA and evaluated in terms of translation quality.</p> <p>In this work, we explore applying compression techniques to ALMA to preserve its translation quality. We experiment with five distinct compression methods: GPTQ [<a href="https://scholar.google.com/scholar_url?url=https://proceedings.mlr.press/v202/frantar23a&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=2009267211619482754&amp;ei=Rfr9aKK0DYGpieoPyfaRyA4&amp;scisig=ABGrvjJl-us59RdHA3e_MXFWxM8X">10</a>], Q-LoRA , [<a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=8079217396100949644&amp;ei=gvr9aPPOK_2lieoP1Jf4uAw&amp;scisig=ABGrvjIqYBQ-eh1JEPCyzNEGJAtt">11</a>], SmoothQuant [<a href="https://scholar.google.com/scholar_url?url=http://proceedings.mlr.press/v202/xiao23c.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=17814731842610185667&amp;ei=jfr9aLrzIbmAieoPlr2tqAg&amp;scisig=ABGrvjIl4KLQN2x_Kith27oHWlKF">12</a>], Wanda [<a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2306.11695&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=8181806601256870843&amp;ei=p_r9aLD8JNrZzwKX7-SYDw&amp;scisig=ABGrvjJYsdHWsU9e_-K7BUqRU9W8">13</a>], and DSnot [<a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2310.08915&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=6652760928495592428&amp;ei=v_r9aOaYCe2ZieoP7ueJyQk&amp;scisig=ABGrvjLWrVul2-STCvxcctLhFb5d">14</a>]. To evaluate the effectiveness of these techniques, we assess their translation quality, memory usage, and inference time, providing a comprehensive analysis of the trade-offs involved in compressing LLMs fine-tuned for translation.</p> <h2 id="results">Results</h2> <p>Table 1 presents the results for the five different compression techniques. A noticeable performance gap can be observed between the <code class="language-plaintext highlighter-rouge">en→xx</code> and <code class="language-plaintext highlighter-rouge">xx→en</code> translation directions. As noted in [<a href="https://arxiv.org/pdf/2309.11674">3</a>], this disparity is likely due to the large amount of English data used during the training of LLAMA 2 [<a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2307.09288&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=10722545090167785628&amp;ei=Jfv9aPKCF-2ZieoP7ueJyQk&amp;scisig=ABGrvjIKbmSm-weoKpHx13AzXNT6">15</a>], resulting in a bias towards better performance when translating into English.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="tab-main-res"></a> <figure> <picture> <img src="/assets/img/alma-table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 1: Summary of calculated metrics across all analyzed model compression techniques. All results are calculated on ALMA-7B and the Method column highlights the method used along with a precision/sparsity level (NA= Not Applicable, NF4=4-bit NormalFloat). We report BLEU (↑) and COMET (↑) scores for <code>en ↔ x</code>. Additionally, we cover the average time per token in seconds (↓) and the memory allocation in megabytes (MB) (↓) after each compression technique is applied. </div> <h4 id="quantization">Quantization</h4> <p>All quantization methods preserved strong BLEU and COMET performance—only slightly below the baseline—while reducing memory use by about 70%. This stability likely stems from ALMA’s already low BLEU score and highly variable multilingual weights, which allow better generalization after quantization. However, 4-bit quantization increased inference time, likely because GPU operations still rely on FP16 multiplications, which are more efficient on current hardware despite the smaller memory footprint of integer formats.</p> <p>The GPTQ study in <a href="#figure1">Figure 1</a> found that 4-bit quantization offers the best balance between performance and memory efficiency, while 2-bit causes major degradation due to limited representational capacity. However, GPTQ also led to increased inference time, likely because the fused kernel optimization from the original paper was not implemented (<a href="#tab-main-res">2nd Row Table 1</a>).</p> <p>The Q-LoRA-like quantization method showed minimal translation performance loss and faster inference than GPTQ (<a href="#tab-main-res">3rd Row Table 1</a>), thanks to its efficient block-wise floating-point quantization and de-quantization to FP16.</p> <p>Finally, combining SmoothQuant with Q-LoRA by smoothing activations and applying block-wise weight quantization—surprisingly improved performance beyond the baseline ALMA-7B (<a href="#tab-main-res">4th Row Table 1</a>), suggesting that quantization can also act as a noise-suppressing mechanism that enhances translation quality.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure1"></a> <figure> <picture> <img src="/assets/img/alma-figure1.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Top grah shows average BLEU Score for both directions of <code>en ↔ x</code> across different GPTQ bitquantization levels. The bottom bar chart represents the memory allocation across different precision values. Both results are based on ALMA-7B. </div> <h4 id="pruning">Pruning</h4> <p>Wanda pruning achieved lower BLEU scores but offered faster inference and reduced memory use compared to the full ALMA-7B model (<a href="#tab-main-res">5th Row Table 1</a>). However, its memory savings were less substantial than quantization, since pruning zeros out weights rather than compressing them. The resulting sparsity speeds up computation but does not reduce the number of operations.</p> <p>Wanda + DSnoT improved perplexity (predictive accuracy) but had negligible gains in BLEU and COMET, indicating that it enhances word-level prediction rather than overall translation quality. Thus, DSnoT appears better suited for general LLM tasks and was excluded from the main results.</p> <p>Wanda + GPTQ, inspired by prior LLaMA experiments, combined pruning (50% sparsity) with 4-bit quantization, yielding slightly better performance and memory usage comparable to pure quantization, though with longer inference times (<a href="#tab-main-res">6th Row Table 1</a>). The method’s suitability ultimately depends on the hardware and performance trade-offs of the translation setup.</p> <h4 id="effects-of-calibration">Effects of Calibration</h4> <p>In <a href="#figure2">Figure 2</a> we report our expeirments on calibration data. These showed minimal performance differences across varying sample sizes, suggesting that translation models depend mainly on pattern recognition and word mappings, which smaller datasets can capture effectively. Since calibration is a one-time post-training step, using the best-performing setup (512 samples per language) is acceptable despite higher latency. However, as model size increases, calibration becomes computationally expensive, highlighting the advantage of methods like Q-LoRA, which bypass the calibration step entirely.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure2"></a> <figure> <picture> <img src="/assets/img/alma-figure2.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: The top and bottom graphs display the BLEU and COMET scores, respectively, for both translation directions <code>en ↔ x</code> across varying calibration sizes and different compression methods. All results are based on the ALMA-7B model. </div> <h2 id="citation">Citation</h2> <p>If you use this work, please cite:</p> <div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@misc</span><span class="p">{</span><span class="nl">vincenti2024modelcompression</span><span class="p">,</span>
  <span class="na">author</span>  <span class="p">=</span> <span class="s">{Vincenti, J., Nulli, M., Tragoudaras,A., Tzifa-Kratira, Z., and Gomez, A. B.}</span><span class="p">,</span>
  <span class="na">title</span>   <span class="p">=</span> <span class="s">{Model Compression for Machine Translation in Large Language Models}</span><span class="p">,</span>
  <span class="na">howpublished</span>  <span class="p">=</span> <span class="s">{https://matteonulli.github.io/blog/2024/alma/}</span><span class="p">,</span>
  <span class="na">year</span>    <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="Inference-Optimization"/><category term="LLMs"/><summary type="html"><![CDATA[Model Compression for Machine Translation in Large Language Models]]></summary></entry><entry><title type="html">Confidently_Exiting/blogpost.md at main · joanvelja/Confidently_Exiting · GitHub</title><link href="https://matteonulli.github.io/blog/2024/confidently_exitingblogpostmd-at-main-joanveljaconfidently_exiting-github/" rel="alternate" type="text/html" title="Confidently_Exiting/blogpost.md at main · joanvelja/Confidently_Exiting · GitHub"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://matteonulli.github.io/blog/2024/confidently_exitingblogpostmd-at-main--joanveljaconfidently_exiting--github</id><content type="html" xml:base="https://matteonulli.github.io/blog/2024/confidently_exitingblogpostmd-at-main-joanveljaconfidently_exiting-github/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[Optimizing Predictions: Vocabulary Reduction and Contrastive Decoding in LLMs. Work done as a reserch project for the MSc AI at the University of Amsterdam - Confidently_Exiting/blogpost.md at main · joanvelja/Confidently_Exiting]]></summary></entry></feed>