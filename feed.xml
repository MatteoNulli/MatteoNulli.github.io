<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://matteonulli.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://matteonulli.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-22T15:10:14+01:00</updated><id>https://matteonulli.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">De-mystifying Multimodal Learning: Enabling Vision in Language Models</title><link href="https://matteonulli.github.io/blog/2026/demystifying0/" rel="alternate" type="text/html" title="De-mystifying Multimodal Learning: Enabling Vision in Language Models"/><published>2026-02-17T15:14:00+01:00</published><updated>2026-02-17T15:14:00+01:00</updated><id>https://matteonulli.github.io/blog/2026/demystifying0</id><content type="html" xml:base="https://matteonulli.github.io/blog/2026/demystifying0/"><![CDATA[<h5 id="matteo-nulli"><b>Matteo Nulli</b></h5> <h6 id="-comunity-article--blogpost">ü§ó <a href="https://huggingface.co/blog/MatteoNulli/de-mystifying-multimodal-learning-enabiling-vision/">Comunity Article</a>, üìù <a href="https://matteonulli.github.io/blog/2025/demystifying0/">Blogpost</a></h6> <p><br/></p> <script>
  window.MathJax = {
    tex: {
      // Configuration for delimiters
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']]
    },
    svg: { fontCache: 'global' }
  };
</script> <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <h2 id="introduction">Introduction</h2> <p>In this first installment of our series, <code>De-mystifying Multimodal Learning</code>, we break down the mechanics of how images become language-compatible vectors. To truly understand how a Large Language Model (LLM) ‚Äúsees‚Äù, we must look at the mathematics defining the problem, the training objectives that align vision and text, and the specific architectural steps that process raw pixels, introducing Vision Language Models (VLMs).</p> <p><a id="figure-1"></a></p> <figure style="width: 70%; margin: auto; text-align: center;"> <img src="https://cdn-uploads.huggingface.co/production/uploads/661d4e74b8f13412f6d48a50/_jq7A-PHTiLCnEo3Sqx7P.png" alt="VLM Architecture" style="width: 100%;"/> <figcaption style="margin-top: 10px; font-style: italic; color: #555;"> Figure 1: Adaptation of Figure from LLaVA-OneVision <a href="#llavonevision-2024">(Li et al., 2024)</a>, serving as an overview the VLM architectural process. </figcaption> </figure> <p>We will therefore cover:<br/></p> <p><a href="#mathematical-formulation">Mathematical Formulation</a>: The theoretical foundation and formal definitions of VLMs.</p> <p><a href="#vision-enocoder-breakdown">Vision Enocoder Breakdown</a>: A detailed overview of the image processing operated by the ViT-CLIP based Vision Encoders.</p> <p><a href="#contrastive-learning">Contrastive Learning</a>: Uncovering how CLIP models learn to algin the images and text representations into the same space.</p> <p><a href="#vlm-architecture-and-flow">VLM Architecture and Flow</a>: Putting it all together, diving deep in the architectural components of VLMs, detailing the birth of Visual Tokens, the source of sigths for LLMs.</p> <h2 id="mathematical-formulation">Mathematical Formulation</h2> <p>To understand Vision-Language Models (VLMs), we first need to define the notation and the transformation pipeline formally.</p> <p>Let \( \mathbf{X} \in \mathbb{R}^{C \times H\times W} \) be an image and \( t \in \Sigma \) be a language instruction input, where \( \Sigma \) is the input space of character sequences. Let \( s_{\theta, \gamma, \phi} \) be an VLM parametrized by \( \theta, \gamma, \phi \). We define \( f_{v\theta} \) as a contrastively pre-trained Vision Encoder model:</p> \[f_{v\theta}: \mathbb{R}^{C \times H \times W} \rightarrow \mathbb{R}^{V \times F},\] <p>where \( V \) is the number of visual tokens and \( F \) is their hidden size. \( f_{t\theta‚Äô} \) represents the corresponding Text Encoder used during the pre-training phase.</p> <p>To bridge the gap between vision and language, we use a connector \( m_\gamma: \mathbb{R}^{V \times F} \rightarrow \mathbb{R}^{V \times D} \), typically a Multi-Layer Perceptron (MLP). The token vocabulary for the model is defined as:</p> \[\mathcal{V}\;=\;\mathcal{V}_{\text{vision}}\;\cup\;\mathcal{V}_{\text{text}}\] <p>The Large Language Model itself is defined as:</p> \[g_{\phi}\;:=\;\mathcal{D}_d\;\circ\;\operatorname{softmax}\;\circ\;F_{\phi'}\;\;:\;\mathbb{R}^{J\times D}\;\longrightarrow\;\mathcal{V}^{J}, \qquad \phi=\bigl(\phi',d\bigr),\] <p>where \( F_{\phi‚Äô} \) is the transformer that produces logits, and \( \mathcal{D}<em>d \) is a decoding operator (such as greedy, top- \( k \), or nucleus sampling) with hyper-parameters \( d \). Thus, \( g</em>{\phi} \) maps an embedded input token sequence to an output token sequence.</p> <h2 id="vision-enocoder-breakdown">Vision Enocoder Breakdown</h2> <p>Now that we have established the mathematical setting, let‚Äôs look at the architectural implementation of the Vision Encoder \( f_{v\theta} \), visually represented in <a href="#figure-2">Figure 2</a>. Practically, the processing flow of \( f_{v\theta} \) is broken down into the following steps:</p> <h4 id="1-patch-partitioning">1. Patch Partitioning</h4> <p>The first step is breaking the high-resolution image \( \mathbf{X} \) into a grid of fixed-size patches.Assuming our image has \( 336 \times 336 \) pixels and we use a patch size of \( P=14 \), standard \( ^{*} \) vision encoders divide the image into \( 24 \times 24 = 576 \) distinct squares. Mathematically, the image is reshaped from \( \mathbf{X} \in \mathbb{R}^{C \times H \times W} \) into a sequence of flattened 2D patches \( \mathbf{x}_p \in \mathbb{R}^{N \times (P^2 \cdot C)} \), where \( N \) is the total number of patches.</p> <p><small> \( ^* \) Standard stands for CLIP-like Vision Encoders (<a href="#clip-2021">Radford et al., 2021</a>, <a href="#siglip-2024">Zhai et al., 2024</a>).</small></p> <h4 id="2-linear-projection-and-position-embeddings">2. Linear Projection and Position Embeddings</h4> <p>These patches are simply raw pixel values. To convert them into vectors, \( f_{v\theta} \) projects each flattened patch into a latent representation through a linear layer. Given the lack of spatial priors in Vision Transformers (ViT) (<a href="#vit-2021">Dosovitskiy et al., 2021</a>), these vectors are equipped with learnable positional encodings, injecting ‚ÄúGPS-like‚Äù coordinates so the model knows where each patch belongs in the original image.</p> <p><a id="figure-2"></a></p> <figure style="width: 80%; margin: auto; text-align: center;"> <img src="https://cdn-uploads.huggingface.co/production/uploads/661d4e74b8f13412f6d48a50/n94F6JJHFFF50QPO53P-k.png" alt="ViT Architecture" style="width: 100%;"/> <figcaption style="margin-top: 10px; font-style: italic; color: #555;"> Figure 2: Architecture of Visision Transfomers (ViT) <a href="#vit-2021">(Dosovitskiy et al., 2021)</a>, serving as an overview the VLM architectural process. </figcaption> </figure> <h4 id="3-transformer-layers">3. Transformer Layers</h4> <p>The resulting vectors are passed through several Transformer Layers consisting of Multi-Head Self-Attention and MLPs. The output is a sequence of vectors where each vector represents a patch within the context of the whole image. This full process produces the representations \( \mathbf{X‚Äô} = f_{v\theta}(\mathbf{X}) \in \mathbb{R}^{V\times F} \).</p> <h2 id="contrastive-learning">Contrastive Learning</h2> <p>Before the Vision Encoder \( f_{v\theta} \) can be used in the VLM pipeline, it must learn to extract features that are semantically aligned with text. This is achieved through Contrastive Learning, (extra sources <a href="https://arxiv.org/pdf/2103.00020">here</a>, <a href="https://medium.com/rectlabs/clip-contrastive-language-image-pre-training-dce66ae18fe1">here</a> and <a href="https://github.com/openai/CLIP">here</a>) a learning process through which Vision Encoders learn to be powerful feature extractors, compressing visual information into vectors (tokens) semantically aligned with language.<br/> Mathematically, during this pre-training phase, each encoder ( \( f_{v\theta} \), \( f_{t\theta‚Äô} \)) extracts feature representations for a batch of image-text pairs. Let \( t‚Äô = f_{t\theta‚Äô}(t) \) be the text features and \( \mathbf{X}‚Äô = f_{v\theta}(\mathbf{X}) \) be the image features. These are normalized as follows</p> \[\mathbf{X}'_{e} = \frac{\mathbf{X}'}{\|\mathbf{X}'\|_2}, \quad t'_{e} = \frac{t'}{\|t'\|_2}\] <p>These normalized features are used to compute the pairwise cosine similarities:</p> \[\textit{logits} = (\mathbf{X}_e' \cdot t_e'^T ) \cdot e^{\tau}\] <p>where \( t_e‚Äô^{T} \) is the transpose of \( t_e‚Äô \), and \( \tau \) is a learnable temperature parameter.These logits are finally used to compute the joint loss function using cross-entropy (CE). The model attempts to maximize the similarity of correct image-text pairs (the diagonal of the matrix) while minimizing others:</p> \[\begin{aligned} \mathcal{L}_{\mathbf X} &amp;= \operatorname{CE}(\textit{logits}, \textit{labels}, \text{axis}=0), \\[4pt] \mathcal{L}_{t} &amp;= \operatorname{CE}(\textit{logits}, \textit{labels}, \text{axis}=1), \\[4pt] \mathcal{L} &amp;= \tfrac{1}{2}\,\bigl(\mathcal{L}_{\mathbf X} + \mathcal{L}_{t}\bigr). \end{aligned}\] <p>Here, <em>labels</em> are the ground truths for that sample, and \( \text{axis}=i, \text{with } i \in {0,1} \) represents the dimension along which the loss is computed.</p> <h2 id="vlm-architecture-and-flow">VLM Architecture and Flow</h2> <p>Once the Vision Encoder is pre-trained, we can assemble the full model. Architecturally, Vision Language Models are constituted by three major components:</p> <ul> <li>Vision Encoders ( \( f_{v\theta} \)), usually a CLIP-like image encoder (<a href="#vit-2021">Dosovitskiy et al., 2021</a>,<a href="#clip-2021">Radford et al., 2021</a>, <a href="#sigmoid-loss-2023">Zhai et al., 2023</a>, <a href="#qwen2-5-vl-2025">Bai et al., 2025</a>), but it can vary in architecture and training style. See <a href="https://jina.ai/vision-encoder-survey.pdf">this</a> extensive survey for more information.</li> <li>Modality Connectors ( \( m_\gamma \)), often simple Multi-Layer Perceptron, with some architectures employing attention blocks (<a href="#blip2-2023">Li et al., 2023</a>) and other alternatives (<a href="#cambrian-1-2024">Tong et al., 2024</a>, <a href="#nulliobjectguided-2025">Nulli et al,. 2025</a>).</li> <li>Large Language Models ( \( g_\phi \)) like Qwen3 <a href="#qwen3-2025">Yang An, et al. 2025</a>, Llama3 <a href="#llama-3-herd-2024">Abhimanyu, et al. 2024</a>, Vicuna <a href="#vicuna-2023">Wei-Lin, et al. 2023</a> and more.</li> </ul> <h3 id="vision-language-modeling-pipeline">Vision-Language Modeling Pipeline</h3> <p>Putting everything together, we can finally describe the classic VLM pipeline during inference, as depicted in <a href="#figure-1">Figure 1</a>. In our calculations below we assume:</p> <ul> <li>A fixed token count. We defer to our next blogpost <big>‚ÄúThe Hidden Inefficiency in Vision Language Modelling‚Äù</big> (coming soon), for an analysis of image pre-processing (<a href="#llavonevision-2024">Li et al., 2024</a>) or other kinds of spatial merging (<a href="#qwen3-vl-2025">QwenTeam, 2025</a>, <a href="#gemma-3-2025">Gemma-Team, 2025</a>) impacting the total visual token count.</li> <li>A batch size of 1.</li> </ul> <p>As per <a href="#vision-enocoder-breakdown">earlier</a>, Vision Encoders \( f_{v\theta} \) are used to encode an image \( \mathbf{X} \) into a representation:</p> \[\mathbf{X}' = f_{v\theta}(\mathbf{X}) \in \mathbb{R}^{V \times F}\] <p>Here, \( F \) is the feature dimension and \( V \) is the vision encoder hidden dimension, calculated as <br/> \( V = (\frac{\textit{image resolution}}{\textit{patch size}})^2 \) \( ^{**} \).</p> <p>Subsequently, \( \mathbf{X}‚Äô \) is transformed through the connector \( m_\gamma \) into Visual Tokens ( \( \mathbf{VT} \)):</p> \[\mathbf{VT} = m_\gamma(\mathbf{X}') \in \mathbb{R}^{V \times D}\] <p>Crucially, these tokens now exist in the input embedding space of the Large Language Model. In parallel, a Tokenizer \( \mathcal{T}: \Sigma \rightarrow \mathcal{V}^{J} \) and a learned embedding \( E:\mathcal{V}^{J}\;\longrightarrow\;\mathbb{R}^{D} \) turn the text input \( t \) into textual tokens: \( \mathit{TT} = E^{\otimes}(\mathcal{T}(t)) \in \mathbb{R}^{J \times D} \), where \( E^{\otimes} \) is the sequence-wise lifting of operator \( E \). Lastly, the visual tokens \( \mathbf{VT} \) are concatenated with the textual tokens \( \mathit{TT} \) and provided as input to the LLM \( g_\phi \) to obtain the output tokens \( \mathbf{T}_a \):</p> \[\mathbf{T}_a = g_{\phi}(\mathbf{VT} \oplus \mathit{TT}) \in \mathcal{V}^{J}.\] <p><small> \( ^{**} \) An crucial approximation, which we‚Äôll tackle in our blogpost <big>‚ÄúThe Hidden Inefficiency in Vision Language Modelling‚Äù</big> (coming soon).</small></p> <h2 id="conclusions">Conclusions</h2> <p>Through the pipeline we‚Äôve explored, we have witnessed a transformation: raw pixels, once just a grid of intensity values, have been flattened, projected, and semantically aligned to emerge as Visual Tokens. These tokens are the ‚Äúuniversal language‚Äù that allows an LLM to treat an image not as a foreign file type, but as a sequence of concepts‚Äîno different from the words in this sentence. By projecting visual data into the same \(D\)-dimensional embedding space as text, we have effectively given the LLM a pair of eyes.</p> <h2 id="whats-next-the-efficiency-bottleneck">What‚Äôs Next: The Efficiency Bottleneck</h2> <p>While we have successfully ‚Äúdigitized‚Äù sight for our models, a massive challenge remains. The impact of the amount Visual Tokens created by the vision encoding pipeline.</p> <p>In our next post, <big>‚ÄúThe Hidden Inefficiency in Vision Language Modelling‚Äù</big> (coming soon), we will dive deep into the cost of producing Visual Tokens on Inference Time &amp; Memory Requirements. We will break down how token count impacts self-attention \( O(N^2) \) and explore why reducing the visual token count is the secret to building faster, leaner, and more capable multimodal systems.</p> <h2 id="citation">Citation</h2> <p>If you use this work, please cite:</p> <pre><code class="language-bibtex">@misc{nulli2026enabling,
  title={De-mystifying Multimodal Learning: Enabiling Vision in Language Models},
  author={Nulli, Matteo},
  year={2026},
  url={https://huggingface.co/blog/MatteoNulli/de-mystifying-multimodal-learning-enabiling-vision},
  howpublished={Available at \url{https://matteonulli.github.io/blog/2026/demystifying0/} and \url{https://huggingface.co/blog/MatteoNulli/de-mystifying-multimodal-learning-enabiling-vision}},
  note={Hugging Face Blog}
}
</code></pre> <p><br/></p> <p><strong>References</strong></p> <div id="references-section"> <a id="conme-2024" class="bib-item">Huang Irene, Lin Wei, Mirza M. Jehanzeb, Hansen Jacob A., Doveh Sivan, Butoi Victor Ion, Herzig Roei, Arbelle Assaf, Kuehne Hilde, Darrell Trevor, Gan Chuang, Oliva Aude, Feris Rogerio, Karlinsky Leonid. (2024). Conme: Rethinking Evaluation of Compositional Reasoning for Modern VLMs. arXiv preprint arXiv:2406.08164.</a> <a id="eyes-wide-shut-2024" class="bib-item">Tong Shengbang, Liu Zhuang, Zhai Yuexiang, Ma Yi, LeCun Yann, Xie Saining. (2024). Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. arXiv preprint arXiv:2401.06209.</a> <a id="visual-instruction-tuning-2023" class="bib-item">Liu Haotian, Li Chunyuan, Wu Qingyang, Lee Yong Jae. (2023). Visual Instruction Tuning. arXiv preprint arXiv:2304.08485.</a> <a id="llavonevision-2024" class="bib-item">Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llava-onevision: Easy visual task transfer. Preprint, arXiv:2408.03326.</a> <a id="qwen2-5-vl-2025" class="bib-item">Bai Shuai, Chen Keqin, Liu Xuejing, Wang Jialin, Ge Wenbin, Song Sibo, Dang Kai, Wang Peng, Wang Shijie, Tang Jun, Zhong Humen, Zhu Yuanzhi, Yang Mingkun, Li Zhaohai, Wan Jianqiang, Wang Pengfei, Ding Wei, Fu Zheren, Xu Yiheng, Ye Jiabo, Zhang Xi, Xie Tianbao, Cheng Zesen, Zhang Hang, Yang Zhibo, Xu Haiyang, Lin Junyang. (2025). Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923.</a> <a id="qwen3-vl-2025" class="bib-item">QwenTeam. 2025. Qwen3-vl: Sharper vision, deeper thought, broader action.</a> <a id="qwen3-2025" class="bib-item">Yang An, et al. (2025). Qwen3 Technical Report. arXiv preprint arXiv:2505.09388.</a> <a id="internvl2-2024" class="bib-item">OpenGVLab-Team. (2024). InternVL2: Better Than the Best‚ÄîExpanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy. Blog post. URL https://internvl.github.io/blog/2024-07-02-InternVL-2.0/.</a> <a id="gemma-3-2025" class="bib-item">Gemma-Team. (2025). Gemma 3 Technical Report. arXiv preprint arXiv:2503.19786.</a> <a id="bags-of-words-vlms-2023" class="bib-item">Yuksekgonul Mert, Bianchi Federico, Kalluri Pratyusha, Jurafsky Dan, Zou James. (2023). When and Why Vision-Language Models Behave Like Bags-of-Words, and What to Do About It? arXiv preprint arXiv:2210.01936.</a> <a id="icl-compositional-vlms-2024" class="bib-item">Nulli Matteo, Ibrahimi Anesa, Pal Avik, Lee Hoshe, Najdenkoska Ivona. (2024). In-Context Learning Improves Compositional Understanding of Vision-Language Models. In ICML 2024 Workshop on Foundation Models in the Wild. arXiv preprint arXiv:2407.15487.</a> <a id="nulliobjectguided-2025" class="bib-item">Matteo Nulli, Ivona Najdenkoska, Mohammad Mahdi Derakhshani, and Yuki M Asano. 2025. Objectguided visual tokens: Eliciting compositional reasoning in multimodal language models. In EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)</a> <a id="vismin-2025" class="bib-item">Awal Rabiul, Ahmadi Saba, Zhang Le, Agrawal Aishwarya. (2025). Vismin: Visual Minimal-Change Understanding. arXiv preprint arXiv:2407.16772.</a> <a id="cambrian-1-2024" class="bib-item">Tong Shengbang, Brown Ellis, Wu Penghao, Woo Sanghyun, Middepogu Manoj, Akula Sai Charitha, Yang Jihan, Yang Shusheng, Iyer Adithya, Pan Xichen, Wang Austin, Fergus Rob, LeCun Yann, Xie Saining. (2024). Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs. arXiv preprint arXiv:2406.16860.</a> <a id="llava-next-2024" class="bib-item">Liu Haotian, Li Chunyuan, Li Yuheng, Li Bo, Zhang Yuanhan, Shen Sheng, Lee Yong Jae. (2024). LLaVA-NeXT: Improved Reasoning, OCR, and World Knowledge. Blog post (January 2024). URL https://llava-vl.github.io/blog/2024-01-30-llava-next/.</a> <a id="sam-2-2024" class="bib-item">Ravi Nikhila, Gabeur Valentin, Hu Yuan-Ting, Hu Ronghang, Ryali Chaitanya, Ma Tengyu, Khedr Haitham, R√§dle Roman, Rolland Chloe, Gustafson Laura, Mintun Eric, Pan Junting, Alwala Kalyan Vasudev, Carion Nicolas, Wu Chao-Yuan, Girshick Ross, Doll√°r Piotr, Feichtenhofer Christoph. (2024). SAM 2: Segment Anything in Images and Videos. arXiv preprint arXiv:2408.00714.</a> <a id="omg-seg-cvpr-2024" class="bib-item">Li Xiangtai, Yuan Haobo, Li Wei, Ding Henghui, Wu Size, Zhang Wenwei, Li Yining, Chen Kai, Loy Chen Change. (2024). OMG-Seg: Is One Model Good Enough for All Segmentation? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 27948‚Äì27959.</a> <a id="eagle-2-5-2025" class="bib-item">Chen Guo, Li Zhiqi, Wang Shihao, Jiang Jindong, Liu Yicheng, Lu Lidong, Huang De-An, Byeon Wonmin, Le Matthieu, Rintamaki Tuomas, Poon Tyler, Ehrlich Max, Lu Tong, Wang Limin, Catanzaro Bryan, Kautz Jan, Tao Andrew, Yu Zhiding, Liu Guilin. (2025). EAGLE 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models. arXiv preprint arXiv:2504.15271.</a> <a id="omg-llava-2024" class="bib-item">Zhang Tao, Li Xiangtai, Fei Hao, Yuan Haobo, Wu Shengqiong, Ji Shunping, Loy Chen Change, Yan Shuicheng. (2024). OMG-LLaVA: Bridging Image-Level, Object-Level, Pixel-Level Reasoning and Understanding. arXiv preprint arXiv:2406.19389.</a> <a id="sa2va-2025" class="bib-item">Yuan Haobo, Li Xiangtai, Zhang Tao, Huang Zilong, Xu Shilin, Ji Shunping, Tong Yunhai, Qi Lu, Feng Jiashi, Yang Ming-Hsuan. (2025). SA2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos. arXiv preprint arXiv:2501.04001.</a> <a id="clip-2021" class="bib-item">Radford Alec, Kim Jong Wook, Hallacy Chris, Ramesh Aditya, Goh Gabriel, Agarwal Sandhini, Sastry Girish, Askell Amanda, Mishkin Pamela, Clark Jack, Krueger Gretchen, Sutskever Ilya. (2021). Learning Transferable Visual Models from Natural Language Supervision. arXiv preprint arXiv:2103.00020.</a> <a id="improved-vit-baselines-2024" class="bib-item">Liu Haotian, Li Chunyuan, Li Yuheng, Lee Yong Jae. (2024). Improved Baselines with Visual Instruction Tuning. arXiv preprint arXiv:2310.03744.</a> <a id="vit-2021" class="bib-item">Dosovitskiy Alexey, Beyer Lucas, Kolesnikov Alexander, Weissenborn Dirk, Zhai Xiaohua, Unterthiner Thomas, Dehghani Mostafa, Minderer Matthias, Heigold Georg, Gelly Sylvain, Uszkoreit Jakob, Houlsby Neil. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010.11929.</a> <a id="llama-2-2023" class="bib-item">Touvron Hugo, Martin Louis, Stone Kevin, Albert Peter, Almahairi Amjad, Babaei Yasmine, Bashlykov Nikolay, Batra Soumya, Bhargava Prajjwal, Bhosale Shruti, Bikel Dan, Blecher Lukas, Canton Ferrer Cristian, Chen Moya, Cucurull Guillem, Esiobu David, Fernandes Jude, Fu Jeremy, Fu Wenyin, Fuller Brian, Gao Cynthia, Goswami Vedanuj, Goyal Naman, Hartshorn Anthony, Hosseini Saghar, Hou Rui, Inan Hakan, Kardas Marcin, Kerkez Viktor, Khabsa Madian, Kloumann Isabel, Korenev Artem, Koura Punit Singh, Lachaux Marie-Anne, Lavril Thibaut, Lee Jenya, Liskovich Diana, Lu Yinghai, Mao Yuning, Martinet Xavier, Mihaylov Todor, Mishra Pushkar, Molybog Igor, Nie Yixin, Poulton Andrew, Reizenstein Jeremy, Rungta Rashi, Saladi Kalyan, Schelten Alan, Silva Ruan, Smith Eric Michael, Subramanian Ranjan, Tan Xiao-qing Ellen, Tang Binh, Taylor Ross, Williams Adina, Kuan Jian Xiang, Xu Puxin, Yan Zheng, Zarov Iliyan, Zhang Yuchen, Fan Angela, Kambadur Melanie, Narang Sharan, Rodriguez Aurelien, Stojnic Robert, Edunov Sergey, Scialom Thomas. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models.</a> <a id="llama-3-2-2024" class="bib-item">Meta. (2024). Llama 3.2: Revolutionizing Edge AI and Vision with Open, Customizable Models. Blog post. URL https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.</a> <a id="lora-2021" class="bib-item">Hu Edward J., Shen Yelong, Wallis Phillip, Allen-Zhu Zeyuan, Li Yuanzhi, Wang Shean, Wang Lu, Chen Weizhu. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.</a> <a id="coco-2014" class="bib-item">Lin Tsung-Yi, Maire Michael, Belongie Serge, Hays James, Perona Pietro, Ramanan Deva, Doll√°r Piotr, Zitnick C. Lawrence. (2014). Microsoft COCO: Common Objects in Context. In Computer Vision ‚Äì ECCV 2014, pages 740‚Äì755. Springer.</a> <a id="image-descriptions-2014" class="bib-item">Young Peter, Lai Alice, Hodosh Micah, Hockenmaier Julia. (2014). From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference Over Event Descriptions. Transactions of the Association for Computational Linguistics, 2:67‚Äì78.</a> <a id="visual-genome-2017" class="bib-item">Krishna Ranjay, Zhu Yuke, Groth Oliver, Johnson Justin, Hata Kenji, Kravitz Joshua, Chen Stephanie, Kalantidis Yannis, Li Li-Jia, Shamma David A., et al. (2017). Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision, 123:32‚Äì73.</a> <a id="gqa-2019" class="bib-item">Hudson Drew A., Manning Christopher D. (2019). GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6700‚Äì6709.</a> <a id="sugar-crepe-2023" class="bib-item">Hsieh Cheng-Yu, Zhang Jieyu, Ma Zixian, Kembhavi Aniruddha, Krishna Ranjay. (2023). SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality. Advances in Neural Information Processing Systems, 36:31096‚Äì31116.</a> <a id="gpt-4-technical-report-2024" class="bib-item">OpenAI. (2024). GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.</a> <a id="scaling-instruction-finetuned-2024" class="bib-item">Chung Hyung Won, Hou Le, Longpre Shayne, Zoph Barret, Tay Yi, Fedus William, Li Yunxuan, Wang Xuezhi, Dehghani Mostafa, Brahma Siddhartha, et al. (2024). Scaling Instruction-Finetuned Language Models. Journal of Machine Learning Research, 25(70):1‚Äì53.</a> <a id="diagram-2016" class="bib-item">Kembhavi Aniruddha, Salvato Mike, Kolve Eric, Seo Minjoon, Hajishirzi Hannaneh, Farhadi Ali. (2016). A Diagram is Worth a Dozen Images. arXiv preprint arXiv:1603.07396.</a> <a id="mme-2024" class="bib-item">Fu Chaoyou, Bird Peixian, Shen Yunhang, Qin Yulei, Zhang Mengdan, Lin Xu, Yang Jinrui, Zheng Xiawu, Li Ke, Sun Xing, Wu Yunsheng, Ji Rongrong. (2024). MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv preprint arXiv:2306.13394.</a> <a id="evaluating-vlms-right-way-2024" class="bib-item">Chen Lin, Li Jinsong, Dong Xiaoyi, Zhang Pan, Zang Yuhang, Chen Zehui, Duan Haodong, Wang Jiaqi, Qiao Yu, Lin Dahua, Zhao Feng. (2024). Are We on the Right Way for Evaluating Large Vision-Language Models? arXiv preprint arXiv:2403.20330.</a> <a id="mmbench-2024" class="bib-item">Liu Yuan, Duan Haodong, Zhang Yuanhan, Li Bo, Zhang Songyang, Zhao Wangbo, Yuan Yike, Wang Jiaqi, He Conghui, Liu Ziwei, Chen Kai, Lin Dahua. (2024). MMBench: Is Your Multi-Modal Model an All-Around Player? arXiv preprint arXiv:2307.06281.</a> <a id="subobject-tokenization-2025" class="bib-item">Chen Delong, Cahyawijaya Samuel, Liu Jianfeng, Wang Baoyuan, Fung Pascale. (2025). Subobject-Level Image Tokenization. arXiv preprint arXiv:2402.14327.</a> <a id="deepspeed-2020" class="bib-item">Rasley Jeff, Rajbhandari Samyam, Ruwase Olatunji, He Yuxiong. (2020). DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD ‚Äô20), pages 3505‚Äì3506. doi:10.1145/3394486.3406703.</a> <a id="zero-2020" class="bib-item">Rajbhandari Samyam, Rasley Jeff, Ruwase Olatunji, He Yuxiong. (2020). ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1‚Äì16. doi:10.1109/SC41405.2020.00024.</a> <a id="adam-2017" class="bib-item">Kingma Diederik P., Ba Jimmy. (2017). Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980.</a> <a id="adamw-2019" class="bib-item">Loshchilov Ilya, Hutter Frank. (2019). Decoupled Weight Decay Regularization. arXiv preprint arXiv:1711.05101.</a> <a id="bert-2019" class="bib-item">Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina. (2019). BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT 2019, pages 4171‚Äì4186.</a> <a id="attention-is-all-you-need-2017" class="bib-item">Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob, Jones Llion, Gomez Aidan N., Kaiser ≈Åukasz, Polosukhin Illia. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems, 30.</a> <a id="pixtral-12b-2024" class="bib-item">Agrawal Pravesh, Antoniak Szymon, Bou Hanna Emma, Bout Baptiste, Chaplot Devendra, Chudnovsky Jessica, Costa Diogo, De Monicault Baudouin, Garg Saurabh, Gervet Theophile, Ghosh Soham, H√©liou Am√©lie, Jacob Paul, Jiang Albert Q., Khandelwal Kartik, Lacroix Timoth√©e, Lample Guillaume, Las Casas Diego, Lavril Thibaut, Le Scao Teven, Lo Andy, Marshall Louis, Martin Arthur, Mensch Arthur, Muddireddy Pavankumar, Nemychnikova Valera, Pellat Marie, Von Platen Patrick, Raghuraman Nikhil, Bout Rozi√®re Baptiste, Sablayrolles Alexandre, Saulnier Lucile, Sauvestre Romain, Rozi√®re Baptiste, Shang Wendy, Soletskyi Roman, Stewart Lawrence, Stock Pierre, Studnia Joachim, Subramanian Sandeep, Vaze Sagar, Wang Thomas, Yang Sophia. (2024). Pixtral 12B. arXiv preprint arXiv:2410.07073.</a> <a id="roformer-2023" class="bib-item">Su Jianlin, Lu Yu, Pan Shengfeng, Murtadha Ahmed, Wen Bo, Liu Yunfeng. (2023). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint arXiv:2104.09864.</a> <a id="blip2-2023" class="bib-item">Li J, Li D, Savarese S, Hoi S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. InInternational conference on machine learning 2023</a> <a id="llama-3-herd-2024" class="bib-item">Dubey Abhimanyu, et al. (2024). The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783.</a> <a id="reproducible-scaling-laws-2023" class="bib-item">Cherti Mehdi, Beaumont Romain, Wightman Ross, Wortsman Mitchell, Ilharco Gabriel, Gordon Cade, Schuhmann Christoph, Schmidt Ludwig, Jitsev Jenia. (2023). Reproducible Scaling Laws for Contrastive Language-Image Learning. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818‚Äì2829. doi:10.1109/CVPR52729.2023.00276.</a> <a id="sigmoid-loss-2023" class="bib-item">Zhai Xiaohua, Mustafa Basil, Kolesnikov Alexander, Beyer Lucas. (2023). Sigmoid Loss for Language Image Pre-Training. arXiv preprint arXiv:2303.15343.</a> <a id="dinov2-2024" class="bib-item">Oquab Maxime, Darcet Timoth√©e, Moutakanni Th√©o, Vo Huy, Szafraniec Marc, Khalidov Vasil, Fernandez Pierre, Haziza Daniel, Massa Francisco, El-Nouby Alaaeldin, Assran Mahmoud, Ballas Nicolas, Galuba Wojciech, Misra Ishan, Rabbat Michael, Sharma Vasu, Synnaeve Gabriel, Xu Hu, Jegou Herv√©, Mairal Julien, Labatut Patrick, Joulin Armand, Bojanowski Piotr. (2024). DINOv2: Learning Robust Visual Features Without Supervision. arXiv preprint arXiv:2304.07193.</a> <a id="internlm2-2024" class="bib-item">Cai Zheng, Cao Maosong, Chen Haojiong, Chen Kai, Chen Keyu, Chen Xin, Chen Xun, Chen Zehui, Chen Zhi, Chu Pei, Dong Xiaoyi, Duan Haodong, Fan Qi, Fei Zhaoye, Gao Yang, Ge Jiaye, Gu Chenya, Gu Yuzhe, Gui Tao, Guo Aijia, Guo Qipeng, He Conghui, Hu Yingfan, Huang Ting, Jiang Tao, Jiao Penglong, Jin Zhenjiang, Lei Zhikai, Li Jiaxing, Li Jingwen, Li Linyang, Li Shuaibin, Li Wei, Li Yining, Liu Hongwei, Liu Jiawei, Liu Kaiwen, Liu Kuikun, Liu Xiaoran, Lv Chengqi, Lv Haijun, Lv Kai, Ma Li, Ma Runyuan, Ma Zerun, Ning Wenchang, Ouyang Linke, Qiu Jiantao, Qu Yuan, Shang Fukai, Shao Yunfan, Song Demin, Song Zifan, Sui Zhihao, Sun Peng, Sun Yu, Tang Huanze, Wang Bin, Wang Guoteng, Wang Jiaqi, Wang Jiayu, Wang Rui, Wang Yudong, Wang Ziyi, Wei Xingjian, Weng Qizhen, Wu Fan, Xiong Yingtong, Xu Chao, Xu Ruiliang, Yan Hang, Yan Yirong, Yang Xiaogui, Ye Haochen, Ying Huaiyuan, Yu Jia, Yu Jing, Zang Yuhang, Zhang Chuyu, Zhang Li, Zhang Pan, Zhang Peng, Zhang Ruijie, Zhang Shuo, Zhang Songyang, Zhang Wenjian, Zhang Wenwei, Zhang Xingcheng, Zhang Xinyue, Zhao Hui, Zhao Qian, Zhao Xiaomeng, Zhao Fengzhe, Zhou Zaida, Zhou Jingming, Zhuo Jingming, Zou Yicheng, Qiu Xipeng, Qiao Yu, Lin Dahua. (2024). InternLM2 Technical Report. arXiv preprint arXiv:2403.17297.</a> <a id="omg-seg-arxiv-2024" class="bib-item">Li Xiangtai, Yuan Haobo, Li Wei, Ding Henghui, Wu Size, Zhang Wenwei, Li Yining, Chen Kai, Loy Chen Change. (2024). OMG-Seg: Is One Model Good Enough for All Segmentation? arXiv preprint arXiv:2401.10229.</a> <a id="seem-2023" class="bib-item">Zou Xueyan, Yang Jianwei, Zhang Hao, Li Feng, Li Linjie, Wang Jianfeng, Wang Lijuan, Gao Jianfeng, Lee Yong Jae. (2023). Segment Everything Everywhere All at Once. arXiv preprint arXiv:2304.06718.</a> <a id="siglip-2024" class="bib-item">Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer. Sigmoid Loss for Language Image Pre-Training, 2024. URL https://arxiv.org/abs/2303.15343.</a> <a id="fastvlms-2025" class="bib-item">Vasu, Pavan Kumar Anasosalu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam et al. "Fastvlm: Efficient vision encoding for vision language models." In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 19769-19780. 2025.</a> <a id="vicuna-2023" class="bib-item">Chiang Wei-Lin, Li Zhuohan, Lin Zi, Sheng Ying, Wu Zhanghao, Zhang Hao, Zheng Lianmin, Zhuang Siyuan, Zhuang Yonghao, Gonzalez Joseph E., Stoica Ion, Xing Eric P. (2023). Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90%* ChatGPT Quality. LMSYS Org Blog. https://lmsys.org/blog/2023-03-30-vicuna/</a> </div> <style>.bib-item{display:none}.bib-item.cited{display:block;margin-bottom:10px}</style> <script>
document.addEventListener("DOMContentLoaded", function() {
    // 1. Find all internal links in the post (usually starting with #)
    const links = document.querySelectorAll('a[href^="#"]');
    const citedIds = new Set();

    links.forEach(link => {
        // Get the ID being linked to (remove the # character)
        const id = link.getAttribute('href').substring(1);
        if (id) citedIds.add(id);
    });

    // 2. Loop through all reference items
    const refItems = document.querySelectorAll('.bib-item');
    refItems.forEach(item => {
        if (citedIds.has(item.id)) {
            item.classList.add('cited'); // This makes it visible via CSS
        }
    });
});
</script>]]></content><author><name></name></author><category term="Multimodal-Learning"/><category term="Vision-Language-Modelling"/><summary type="html"><![CDATA[A blogpost series on the nuts and bolts of Multimodal Learning]]></summary></entry><entry><title type="html">Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models</title><link href="https://matteonulli.github.io/blog/2025/ogllava/" rel="alternate" type="text/html" title="Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models"/><published>2025-09-05T16:24:00+02:00</published><updated>2025-09-05T16:24:00+02:00</updated><id>https://matteonulli.github.io/blog/2025/ogllava</id><content type="html" xml:base="https://matteonulli.github.io/blog/2025/ogllava/"><![CDATA[<h5 id="m-nulli-i-najdenkoska-m-m-derakhshani-v-orshulevich-m-dorkenwald-and-y-m-asano"><b>M. Nulli</b>, I. Najdenkoska, M. M. Derakhshani, V. Orshulevich, M. Dorkenwald and Y. M. Asano</h5> <h6 id="university-of-amsterdam-ebay-university-of-technology-nuremberg-ellis-unit-amsterdam">University of Amsterdam, eBay, University of Technology Nuremberg, ELLIS Unit Amsterdam</h6> <h6 id="------"><img src="https://upload.wikimedia.org/wikipedia/commons/1/17/Uva%C2%AEmerken_ENG.png" alt="University of Amsterdam" height="24"/> ¬†<img src="https://upload.wikimedia.org/wikipedia/commons/1/1b/EBay_logo.svg" alt="eBay" height="24"/> ¬† <img src="https://www.utn.de/files/2022/07/UTN-Website-icon-512.png" alt="UTN" height="24"/> ¬† <img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQhUVivV36BHYzHgtuCNL1TpI4jdOCG0W1h-A&amp;s" alt="UTN" height="24"/> ¬†</h6> <h6 id="-paper---full-thesis---blogpost---code">üìÑ <a href="https://openreview.net/forum?id=yvY1T3hHEQ">Paper</a> | üìú <a href="https://github.com/MatteoNulli/og_llava/blob/main/paper/LongPaper.pdf">Full Thesis</a> | üìù <a href="https://matteonulli.github.io/blog/2025/ogllava/">Blogpost</a> | üßë‚Äçüíª <a href="https://github.com/MatteoNulli/og_llava/tree/main">Code</a></h6> <h6 id="accepted-to-eurips-workshop-on-principles-of-generative-modelling"><em>Accepted to EurIPS, Workshop on Principles of Generative Modelling</em></h6> <p><br/></p> <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['\\[', '\\]']],
    },
    svg: { fontCache: 'global' }
  };
</script> <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/> <script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script> <script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, { delimiters: [ {left: '$$', right: '$$', display: true}, {left: '\\[', right: '\\]', display: true}, {left: '$', right: '$', display: false}, {left: '\\(', right: '\\)', display: false} ], // keep default ignore list so code/pre are skipped });"></script> <h2 id="motivation">Motivation</h2> <div class="row mt-3"> <a id="video2"></a> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/ogllava.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/eurips.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Matteo Nulli going through Figure 1 at the ELLIS Honours Presentation (left)</b>, <b>Matteo Nulli presenting the paper at EurIPS (right)</b> </div> <p>Most Multimodal Large Language Models (MLLMs) (<a href="#qwen2-5-vl-2025">2</a>, <a href="#internvl2-2024">3</a>, <a href="#gemma-3-2025">4</a>, <a href="#cambrian-1-2024">5</a>, <a href="#llava-next-2024">6</a>) use contrastively pre-trained vision encoders (<a href="#clip-2021">7</a>). They work well on many tasks, but often struggle when it comes to <strong>compositional understanding</strong> and <strong>reasoning</strong> on understanding interndependencies between objetcs, as highlighted in <a href="#bags-of-words-vlms-2023">8</a> and <a href="#icl-compositional-vlms-2024">9</a>. That‚Äôs because these encoders are mainly trained for image‚Äìcaption retrieval, not for truly breaking down and understanding all parts of a scene. Another issue is efficiency: state-of-the-art vision encoders generate <strong>2‚Äì3x more visual tokens</strong> (Any-Resolution in <a href="#llava-next-2024">6</a> and Spatial Visual Aggregator in <a href="#cambrian-1-2024">5</a>), which slow down both training and inference.</p> <p>To tackle these problems, we introduce <strong>OG-LLaVA (Object-Guided <a href="https://arxiv.org/pdf/2310.03744">LLaVA</a>)</strong>. With our new connector design, <strong><code>OG-Fusion</code></strong>, the model can reason about visual content more effectively‚Äîwithout adding lots of extra tokens or fine-tuning the vision encoder itself. At the core of <code>OG-Fusion</code> is a simple but powerful idea: combine <strong>CLIP representations</strong> with <strong>segmentation masks</strong>. This lets OG-LLaVA leverage the descriptive strength of segmentation models (<a href="#sam-2-2024">10</a>) to better capture <strong>object relationships</strong> and <strong>spatial arrangements</strong>. The result? <br/> <strong>OG-LLaVA outperforms comparable models on tasks demanding deep visual reasoning and grounding, while staying efficient.</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="mainfigure"></a> <figure> <picture> <img src="/assets/img/ogllava_3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: <b> OG-LLaVA </b> architecture with <code>OG-Fusion</code> internal proces </div> <h2 id="methodology">Methodology</h2> <p>Given a single input image $\mathbf{X} \in \mathbb{R}^{C \times H \times W}$, we denote by $\mathbf{M} = {\mathbf{m}_i \mid i = 1,\dots,N} \subset \mathbb{R}^{H \times W}$ the corresponding set of $N$ binary segmentation masks, where each mask satisfies $\mathbf{m}_i \in {0,1}^{H \times W}$. Our objective is to construct a set of segmentation-aware visual tokens, such that each variable-length token segment is explicitly associated with one object mask.</p> <p>For clarity, we describe the procedure assuming a batch size of one; a fully rigorous mathematical formulation is deferred to the Appendix of the <a href="https://openreview.net/forum?id=yvY1T3hHEQ">paper</a> (see Section Object-Guided Visual Tokens).</p> <h3 id="masks-and-features-extraction">Masks and Features Extraction</h3> <p>We begin by extracting object-level structure from each image through a segmentation model, which produces a set of binary masks $\mathbf{M}$. During training, visual features are obtained from a Vision Encoder, yielding representations $\mathbf{X‚Äô} \in \mathbb{R}^{V \times F}$. These features are aligned with the corresponding segmentation masks and processed through an ad-hoc downsampling operator designed to preserve object-centric information.</p> <h3 id="downsampling-operator">Downsampling Operator</h3> <p>We define a downsampling operator $\Phi_{\alpha}$ that maps a high-resolution binary mask to a lower-resolution representation. Each output bin aggregates neighboring pixels and is marked as foreground if it contains at least $\alpha$ foreground pixels. Applying this operator independently to all $N$ masks results in a set of downsampled binary masks:</p> <p align="center"> \[ \mathbf{M}' = \bigl\{ \Phi_{\alpha}(\mathbf{m}_i) \;\bigm|\; i = 1,\dots,N \bigr\} \subset \{0,1\}^{V} \] </p> <p>Further implementation details of $\Phi_{\alpha}$ are provided in Appendix A of the <a href="https://openreview.net/forum?id=yvY1T3hHEQ">paper</a>.</p> <h3 id="object-guided-visual-tokens">Object-Guided Visual Tokens</h3> <p>Following preprocessing, the downsampled masks $\mathbf{M}‚Äô$ are applied to the visual feature matrix $\mathbf{X}‚Äô$ through an index-based row-selection matrix $P_i$. This operation extracts object-specific visual fragments:</p> <p align="center"> \[ \mathbf{Y}_i = P_i\,\mathbf{X}' \;\in\; \mathbb{R}^{t_i \times F}. \] </p> <p>The resulting fragments are then projected into the language-model embedding space via a Multi-Layer Perceptron (MLP), producing Object-Guided Visual Tokens (OGVT):</p> <p align="center"> \[ \boxed{ \textbf{OGVT} \;:=\; MLP(\mathbf{Y}) \;\in\; \mathbb{R}^{T \times D} } \] </p> <p>Here, $T$ denotes the total number of object-bearing bins retained after downsampling. While $T$ varies per image, its expected value remains close to the original token count ($T \approx V$). When multiple masks overlap on the same ViT bin, that bin is duplicated across different $\mathbf{Y}_i$. Due to mask thresholding and the use of rotary positional embeddings (RoPE), these duplicated tokens yield distinct projections and therefore do not collapse into identical attention keys. As a result, attention is naturally biased toward regions with higher object density, effectively reintroducing spatial grounding that is otherwise weakened in standard Transformer architectures. A formal analysis of token duplication effects is provided in Appendix B of the <a href="https://openreview.net/forum?id=yvY1T3hHEQ">paper</a>.</p> <h3 id="model-architecture-training-and-inference">Model Architecture, Training, and Inference</h3> <p>To preserve architectural compatibility with LLaVA-1.5, we adopt CLIP ViT-L/14@336 (<a href="#clip-2021">7</a>) as the vision encoder. While CLIP is our primary choice, the proposed method is agnostic to the specific vision backbone.</p> <p>We experiment with two large language models: Llama 3.1-8B-Instruct (<a href="#llama-3-herd-2024">11</a>) and Llama 3.2-3B-Instruct (<a href="#llama-3-2-2024">12</a>), resulting in two variants OG-LLaVA-8B and OG-LLaVA-3B, respectively. An overview of the full OG-Fusion pipeline is shown in <a href="#mainfigure">Figure¬†1</a>. The architecture integrates a frozen Segment Anything Model 2 (SAM2) (<a href="#sam-2-2024">10</a>) backbone, followed by the object-guided token construction described above and a two-hidden-layer MLP with GeLU activations.</p> <p>Training follows the visual instruction tuning paradigm of <a href="#visual-instruction-tuning-2023">13</a> and proceeds in two stages:<br/> (i) Vision‚ÄìLanguage Alignment, where only OG-Fusion is unfrozen <br/> (ii) Supervised Fine-Tuning, where both the LLM and OG-Fusion are trained using LoRA (<a href="#lora-2021">14</a>).</p> <p>The <strong><em>OGVT</em></strong> are then given as input to a Large Language Model together with Textual Tokens to produce an output.<br/> The ‚ùÑÔ∏è (snowflake) and üî• (fire) symbols in <a href="#mainfigure">Figure¬†1</a> represent modules whose parameters are kept <strong>frozen</strong> or <strong>trained</strong>.<br/> LoRA emphasizes that not all parameters of the LLM are unfrozen, only the LoRA layers.</p> <p>Although <strong><em>OGVT</em></strong>s are constructed using segmentation masks during training, the model can be evaluated both with and without mask infusion‚Äîdemonstrating robustness by preserving the semantic structure of the original visual features $\mathbf{X}‚Äô$.</p> <h2 id="results">Results</h2> <p>Our results on compositional reasoning and vision-centric benchmarks <a href="#tab-main-res">Table¬†1</a>, show that <strong>OG-LLaVA</strong> consistently outperforms its baselines, across both <em><a href="https://arxiv.org/pdf/2310.03744">LLaVA-1.5</a></em> and <em><a href="https://arxiv.org/pdf/2406.16860">Cambrian-1</a></em> training setups. The improvements are not marginal‚Äîthey‚Äôre large and systematic.</p> <ul> <li><strong>Compositional understanding</strong> <ul> <li><strong>ARO</strong>: <ul> <li>+21% on <em>Coco-Order</em> (38.2 ‚Üí 82.6) and +16% on <em>Flickr-Order</em> (49.1 ‚Üí 84.0).</li> <li><em>Visual Genome Attribution</em> on average +10% across backbones and on <em>Visual Genome Relation</em> +20% across training data and model sizes.</li> </ul> </li> <li><strong>ConME</strong>: steady +2% gains, peaking at 65.2 in the 8B setting (+3.6 over the strongest baseline).</li> </ul> </li> <li><strong>Vision-centric reasoning</strong> <ul> <li><strong>MMVP</strong>: about +3 points on average (e.g. 32.0 ‚Üí 37.0 in 8B, 61.6 ‚Üí 66.0 with <em>Cambrian-1</em> data).</li> <li><strong>CVBench</strong>: stable performance, with only ¬±1 point fluctuations.</li> </ul> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="tab-main-res"></a> <figure> <picture> <img src="/assets/img/main-table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 1: <b> OG-LLaVA </b> performance on Compositional Reasoning and Vision Centric tasks compared with LLaVA baselines. </div> <p>In <a href="#fig-sit-vs-ours">Figure¬†7</a>, we compare <strong>OG-LLaVA-8B</strong> with SIT-8B, and LLaVA-1.5-8B under the same backbone. SIT-8B stands for <em><a href="https://arxiv.org/pdf/2402.14327">Subobject-level Image Tokenization (SIT)</a></em> a new study employing a comparable segmentation-infusion method. The results are clear: <strong>OG-LLaVA</strong> consistently outperforms SIT, with more than a 25% advantage on compositional reasoning and a 10% edge in visual grounding.</p> <p>There‚Äôs also a key difference in usability. <strong>OG-LLaVA</strong> works flexibly both with and without segmentation masks at inference, while SIT requires pre-computed masks every time. This not only adds non-trivial overhead‚Äîsince a separate segmentation model must run first‚Äîbut also makes the system less adaptable. In practice, the reduced token count doesn‚Äôt outweigh the complexity introduced, whereas <strong>OG-LLaVA</strong> preserves efficiency without imposing such constraints.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-sit-vs-ours"></a> <figure> <picture> <img src="/assets/img/og_llava_vs_rivals_weighted.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 7: <b> OG-LLaVA </b> vs Subobject Level Image Tokenization and LLaVA-1.5 on Compositional Reasoning and Vision Centric tasks. </div> <h2 id="qualitative-results">Qualitative Results</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/conme_visual.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: <b> OG-LLaVA </b> vs LLaVA-1.5 on Compositional Reasoning Benchmark ConMe. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/mmvp_visual.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 3: <b> OG-LLaVA </b> vs LLaVA-1.5 on Vision Grounding benchmark MMVP. </div> <p>The images we picked cover all kinds of tricky challenges‚Äîspotting tiny details, telling apart subtle colors, reading depth cues, recognizing materials, making sense of spatial layouts, and even detecting small objects. They‚Äôre designed to push visual‚Äìlanguage reasoning to its limits. What‚Äôs key is that these examples are tested at inference time with no extra fine-tuning, so any boost (or drop) in performance comes purely from the Object-Guided priors built into <strong>OG-LLaVA</strong>.</p> <p>In <a href="#fig-conme-rel-2">Figure¬†4</a>, <a href="#fig-conme-obj">¬†5</a> and <a href="#fig-conme-rel-1">¬†6</a> we highlight a range of cases where <strong>OG-LLaVA</strong> consistently demonstrates sharper perception and more grounded reasoning, from subtle posture cues to tricky color judgments and material recognition.</p> <ul> <li><strong>Fine-grained human pose</strong>: correctly reads a batter‚Äôs stance, placing the bat <em>up and behind</em> instead of <em>in front</em>. (<a href="#fig-conme-rel-2">Figure¬†4 - Picture1</a>)</li> <li><strong>Precise color &amp; reflection reasoning</strong>: rules out red in umbrellas, confining it to apples/plate, while the baseline gets misled, as well as captures realistic color reflections on materials and disambiguates different hues. (<a href="#fig-conme-rel-2">Figure¬†4 - Picture2</a>), (<a href="#fig-conme-obj">Figure¬†5 - Picture8</a>) &amp; (<a href="#fig-conme-rel-1">Figure¬†6 - Picture2</a>)</li> <li><strong>Depth-of-field understanding</strong>: detects focus fading <em>front-to-back</em> instead of mistakenly <em>left-to-right</em>. (<a href="#fig-conme-rel-2">Figure¬†4 - Picture3</a>)</li> <li><strong>Material recognition</strong>: identifies a skate-park surface as <em>concrete</em>, not asphalt and glass instead of curtains. (<a href="#fig-conme-rel-2">Figure¬†4 - Picture4</a>) &amp; (<a href="#fig-conme-rel-2">Figure¬†5 - Picture1</a>)</li> <li><strong>Font recognition</strong>: picks up subtle font characteristics, showing solid OCR ability. (<a href="#fig-conme-obj">Figure¬†5 - Picture2</a>)</li> <li><strong>Spatial reasoning</strong>: accurately locates people and objects in complex scenes. (<a href="#fig-conme-obj">Figure¬†5 - Picture3</a>) &amp; (<a href="#fig-conme-obj">Figure¬†5 - Picture6</a>)</li> <li><strong>Object counting &amp; detection</strong>: counts giraffes correctly where the baseline stumbles and spots a distant coffee maker amid clutter, avoiding confusion with a blender. (<a href="#fig-conme-obj">Figure¬†5 - Picture4</a>) &amp; (<a href="#fig-conme-rel-1">Figure¬†6 - Picture3</a>)</li> <li><strong>Fashion understanding</strong>: distinguishes between short sleeves and cap sleeves. (<a href="#fig-conme-obj">Figure¬†5 - Picture7</a>)</li> <li><strong>Dynamic cues</strong>: understands a distant sail is <em>inflated with strong breeze</em>, matching the surfing context. (<a href="#fig-conme-rel-1">Figure¬†6 - Picture1</a>)</li> <li><strong>Shape recognition</strong>: correctly identifies train tracks in the background. (<a href="#fig-conme-rel-1">Figure¬†6 - Picture4</a>)</li> </ul> <p>Together, these examples underline how <strong>OG-LLaVA</strong> moves beyond surface-level cues. It pays attention to fine details, adapts across diverse tasks, and reasons about entire scenes in a way that more closely reflects human understanding.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-conme-rel-2"></a> <figure> <picture> <img src="/assets/img/conme_additional_rel_2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Relation examples. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-conme-obj"></a> <figure> <picture> <img src="/assets/img/conme_additional_obj.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Object examples. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="fig-conme-rel-1"></a> <figure> <picture> <img src="/assets/img/conme_additional_rel_1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6: <b> OG-LLaVA </b> vs LLaVA-1.5 on ConMe Replace-Relation examples. </div> <h2 id="citation">Citation</h2> <p>If you use this work, please cite:</p> <pre><code class="language-bibtex">@inproceedings{nulli2025objectguided,
title={Object-Guided Visual Tokens: Eliciting Compositional Reasoning in Multimodal Language Models},
author={Matteo Nulli and Ivona Najdenkoska and Mohammad Mahdi Derakhshani and Yuki M Asano},
booktitle={EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)},
year={2025},
url={https://openreview.net/forum?id=yvY1T3hHEQ}
}
</code></pre> <p><strong>References</strong></p> <div id="references-section"> <a id="conme-2024" class="bib-item">Huang Irene, Lin Wei, Mirza M. Jehanzeb, Hansen Jacob A., Doveh Sivan, Butoi Victor Ion, Herzig Roei, Arbelle Assaf, Kuehne Hilde, Darrell Trevor, Gan Chuang, Oliva Aude, Feris Rogerio, Karlinsky Leonid. (2024). Conme: Rethinking Evaluation of Compositional Reasoning for Modern VLMs. arXiv preprint arXiv:2406.08164.</a> <a id="eyes-wide-shut-2024" class="bib-item">Tong Shengbang, Liu Zhuang, Zhai Yuexiang, Ma Yi, LeCun Yann, Xie Saining. (2024). Eyes Wide Shut? Exploring the Visual Shortcomings of Multimodal LLMs. arXiv preprint arXiv:2401.06209.</a> <a id="visual-instruction-tuning-2023" class="bib-item">Liu Haotian, Li Chunyuan, Wu Qingyang, Lee Yong Jae. (2023). Visual Instruction Tuning. arXiv preprint arXiv:2304.08485.</a> <a id="llavonevision-2024" class="bib-item">Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. 2024a. Llava-onevision: Easy visual task transfer. Preprint, arXiv:2408.03326.</a> <a id="qwen2-5-vl-2025" class="bib-item">Bai Shuai, Chen Keqin, Liu Xuejing, Wang Jialin, Ge Wenbin, Song Sibo, Dang Kai, Wang Peng, Wang Shijie, Tang Jun, Zhong Humen, Zhu Yuanzhi, Yang Mingkun, Li Zhaohai, Wan Jianqiang, Wang Pengfei, Ding Wei, Fu Zheren, Xu Yiheng, Ye Jiabo, Zhang Xi, Xie Tianbao, Cheng Zesen, Zhang Hang, Yang Zhibo, Xu Haiyang, Lin Junyang. (2025). Qwen2.5-VL Technical Report. arXiv preprint arXiv:2502.13923.</a> <a id="qwen3-vl-2025" class="bib-item">QwenTeam. 2025. Qwen3-vl: Sharper vision, deeper thought, broader action.</a> <a id="internvl2-2024" class="bib-item">OpenGVLab-Team. (2024). InternVL2: Better Than the Best‚ÄîExpanding Performance Boundaries of Open-Source Multimodal Models with the Progressive Scaling Strategy. Blog post. URL https://internvl.github.io/blog/2024-07-02-InternVL-2.0/.</a> <a id="gemma-3-2025" class="bib-item">Gemma-Team. (2025). Gemma 3 Technical Report. arXiv preprint arXiv:2503.19786.</a> <a id="bags-of-words-vlms-2023" class="bib-item">Yuksekgonul Mert, Bianchi Federico, Kalluri Pratyusha, Jurafsky Dan, Zou James. (2023). When and Why Vision-Language Models Behave Like Bags-of-Words, and What to Do About It? arXiv preprint arXiv:2210.01936.</a> <a id="icl-compositional-vlms-2024" class="bib-item">Nulli Matteo, Ibrahimi Anesa, Pal Avik, Lee Hoshe, Najdenkoska Ivona. (2024). In-Context Learning Improves Compositional Understanding of Vision-Language Models. In ICML 2024 Workshop on Foundation Models in the Wild. arXiv preprint arXiv:2407.15487.</a> <a id="nulliobjectguided-2025" class="bib-item">Matteo Nulli, Ivona Najdenkoska, Mohammad Mahdi Derakhshani, and Yuki M Asano. 2025. Objectguided visual tokens: Eliciting compositional reasoning in multimodal language models. In EurIPS 2025 Workshop on Principles of Generative Modeling (PriGM)</a> <a id="vismin-2025" class="bib-item">Awal Rabiul, Ahmadi Saba, Zhang Le, Agrawal Aishwarya. (2025). Vismin: Visual Minimal-Change Understanding. arXiv preprint arXiv:2407.16772.</a> <a id="cambrian-1-2024" class="bib-item">Tong Shengbang, Brown Ellis, Wu Penghao, Woo Sanghyun, Middepogu Manoj, Akula Sai Charitha, Yang Jihan, Yang Shusheng, Iyer Adithya, Pan Xichen, Wang Austin, Fergus Rob, LeCun Yann, Xie Saining. (2024). Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs. arXiv preprint arXiv:2406.16860.</a> <a id="llava-next-2024" class="bib-item">Liu Haotian, Li Chunyuan, Li Yuheng, Li Bo, Zhang Yuanhan, Shen Sheng, Lee Yong Jae. (2024). LLaVA-NeXT: Improved Reasoning, OCR, and World Knowledge. Blog post (January 2024). URL https://llava-vl.github.io/blog/2024-01-30-llava-next/.</a> <a id="sam-2-2024" class="bib-item">Ravi Nikhila, Gabeur Valentin, Hu Yuan-Ting, Hu Ronghang, Ryali Chaitanya, Ma Tengyu, Khedr Haitham, R√§dle Roman, Rolland Chloe, Gustafson Laura, Mintun Eric, Pan Junting, Alwala Kalyan Vasudev, Carion Nicolas, Wu Chao-Yuan, Girshick Ross, Doll√°r Piotr, Feichtenhofer Christoph. (2024). SAM 2: Segment Anything in Images and Videos. arXiv preprint arXiv:2408.00714.</a> <a id="omg-seg-cvpr-2024" class="bib-item">Li Xiangtai, Yuan Haobo, Li Wei, Ding Henghui, Wu Size, Zhang Wenwei, Li Yining, Chen Kai, Loy Chen Change. (2024). OMG-Seg: Is One Model Good Enough for All Segmentation? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 27948‚Äì27959.</a> <a id="eagle-2-5-2025" class="bib-item">Chen Guo, Li Zhiqi, Wang Shihao, Jiang Jindong, Liu Yicheng, Lu Lidong, Huang De-An, Byeon Wonmin, Le Matthieu, Rintamaki Tuomas, Poon Tyler, Ehrlich Max, Lu Tong, Wang Limin, Catanzaro Bryan, Kautz Jan, Tao Andrew, Yu Zhiding, Liu Guilin. (2025). EAGLE 2.5: Boosting Long-Context Post-Training for Frontier Vision-Language Models. arXiv preprint arXiv:2504.15271.</a> <a id="omg-llava-2024" class="bib-item">Zhang Tao, Li Xiangtai, Fei Hao, Yuan Haobo, Wu Shengqiong, Ji Shunping, Loy Chen Change, Yan Shuicheng. (2024). OMG-LLaVA: Bridging Image-Level, Object-Level, Pixel-Level Reasoning and Understanding. arXiv preprint arXiv:2406.19389.</a> <a id="sa2va-2025" class="bib-item">Yuan Haobo, Li Xiangtai, Zhang Tao, Huang Zilong, Xu Shilin, Ji Shunping, Tong Yunhai, Qi Lu, Feng Jiashi, Yang Ming-Hsuan. (2025). SA2VA: Marrying SAM2 with LLaVA for Dense Grounded Understanding of Images and Videos. arXiv preprint arXiv:2501.04001.</a> <a id="clip-2021" class="bib-item">Radford Alec, Kim Jong Wook, Hallacy Chris, Ramesh Aditya, Goh Gabriel, Agarwal Sandhini, Sastry Girish, Askell Amanda, Mishkin Pamela, Clark Jack, Krueger Gretchen, Sutskever Ilya. (2021). Learning Transferable Visual Models from Natural Language Supervision. arXiv preprint arXiv:2103.00020.</a> <a id="improved-vit-baselines-2024" class="bib-item">Liu Haotian, Li Chunyuan, Li Yuheng, Lee Yong Jae. (2024). Improved Baselines with Visual Instruction Tuning. arXiv preprint arXiv:2310.03744.</a> <a id="vit-2021" class="bib-item">Dosovitskiy Alexey, Beyer Lucas, Kolesnikov Alexander, Weissenborn Dirk, Zhai Xiaohua, Unterthiner Thomas, Dehghani Mostafa, Minderer Matthias, Heigold Georg, Gelly Sylvain, Uszkoreit Jakob, Houlsby Neil. (2021). An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. arXiv preprint arXiv:2010.11929.</a> <a id="llama-2-2023" class="bib-item">Touvron Hugo, Martin Louis, Stone Kevin, Albert Peter, Almahairi Amjad, Babaei Yasmine, Bashlykov Nikolay, Batra Soumya, Bhargava Prajjwal, Bhosale Shruti, Bikel Dan, Blecher Lukas, Canton Ferrer Cristian, Chen Moya, Cucurull Guillem, Esiobu David, Fernandes Jude, Fu Jeremy, Fu Wenyin, Fuller Brian, Gao Cynthia, Goswami Vedanuj, Goyal Naman, Hartshorn Anthony, Hosseini Saghar, Hou Rui, Inan Hakan, Kardas Marcin, Kerkez Viktor, Khabsa Madian, Kloumann Isabel, Korenev Artem, Koura Punit Singh, Lachaux Marie-Anne, Lavril Thibaut, Lee Jenya, Liskovich Diana, Lu Yinghai, Mao Yuning, Martinet Xavier, Mihaylov Todor, Mishra Pushkar, Molybog Igor, Nie Yixin, Poulton Andrew, Reizenstein Jeremy, Rungta Rashi, Saladi Kalyan, Schelten Alan, Silva Ruan, Smith Eric Michael, Subramanian Ranjan, Tan Xiao-qing Ellen, Tang Binh, Taylor Ross, Williams Adina, Kuan Jian Xiang, Xu Puxin, Yan Zheng, Zarov Iliyan, Zhang Yuchen, Fan Angela, Kambadur Melanie, Narang Sharan, Rodriguez Aurelien, Stojnic Robert, Edunov Sergey, Scialom Thomas. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models.</a> <a id="llama-3-2-2024" class="bib-item">Meta. (2024). Llama 3.2: Revolutionizing Edge AI and Vision with Open, Customizable Models. Blog post. URL https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/.</a> <a id="lora-2021" class="bib-item">Hu Edward J., Shen Yelong, Wallis Phillip, Allen-Zhu Zeyuan, Li Yuanzhi, Wang Shean, Wang Lu, Chen Weizhu. (2021). LoRA: Low-Rank Adaptation of Large Language Models. arXiv preprint arXiv:2106.09685.</a> <a id="coco-2014" class="bib-item">Lin Tsung-Yi, Maire Michael, Belongie Serge, Hays James, Perona Pietro, Ramanan Deva, Doll√°r Piotr, Zitnick C. Lawrence. (2014). Microsoft COCO: Common Objects in Context. In Computer Vision ‚Äì ECCV 2014, pages 740‚Äì755. Springer.</a> <a id="image-descriptions-2014" class="bib-item">Young Peter, Lai Alice, Hodosh Micah, Hockenmaier Julia. (2014). From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference Over Event Descriptions. Transactions of the Association for Computational Linguistics, 2:67‚Äì78.</a> <a id="visual-genome-2017" class="bib-item">Krishna Ranjay, Zhu Yuke, Groth Oliver, Johnson Justin, Hata Kenji, Kravitz Joshua, Chen Stephanie, Kalantidis Yannis, Li Li-Jia, Shamma David A., et al. (2017). Visual Genome: Connecting Language and Vision Using Crowdsourced Dense Image Annotations. International Journal of Computer Vision, 123:32‚Äì73.</a> <a id="gqa-2019" class="bib-item">Hudson Drew A., Manning Christopher D. (2019). GQA: A New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6700‚Äì6709.</a> <a id="sugar-crepe-2023" class="bib-item">Hsieh Cheng-Yu, Zhang Jieyu, Ma Zixian, Kembhavi Aniruddha, Krishna Ranjay. (2023). SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality. Advances in Neural Information Processing Systems, 36:31096‚Äì31116.</a> <a id="gpt-4-technical-report-2024" class="bib-item">OpenAI. (2024). GPT-4 Technical Report. arXiv preprint arXiv:2303.08774.</a> <a id="scaling-instruction-finetuned-2024" class="bib-item">Chung Hyung Won, Hou Le, Longpre Shayne, Zoph Barret, Tay Yi, Fedus William, Li Yunxuan, Wang Xuezhi, Dehghani Mostafa, Brahma Siddhartha, et al. (2024). Scaling Instruction-Finetuned Language Models. Journal of Machine Learning Research, 25(70):1‚Äì53.</a> <a id="diagram-2016" class="bib-item">Kembhavi Aniruddha, Salvato Mike, Kolve Eric, Seo Minjoon, Hajishirzi Hannaneh, Farhadi Ali. (2016). A Diagram is Worth a Dozen Images. arXiv preprint arXiv:1603.07396.</a> <a id="mme-2024" class="bib-item">Fu Chaoyou, Bird Peixian, Shen Yunhang, Qin Yulei, Zhang Mengdan, Lin Xu, Yang Jinrui, Zheng Xiawu, Li Ke, Sun Xing, Wu Yunsheng, Ji Rongrong. (2024). MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models. arXiv preprint arXiv:2306.13394.</a> <a id="evaluating-vlms-right-way-2024" class="bib-item">Chen Lin, Li Jinsong, Dong Xiaoyi, Zhang Pan, Zang Yuhang, Chen Zehui, Duan Haodong, Wang Jiaqi, Qiao Yu, Lin Dahua, Zhao Feng. (2024). Are We on the Right Way for Evaluating Large Vision-Language Models? arXiv preprint arXiv:2403.20330.</a> <a id="mmbench-2024" class="bib-item">Liu Yuan, Duan Haodong, Zhang Yuanhan, Li Bo, Zhang Songyang, Zhao Wangbo, Yuan Yike, Wang Jiaqi, He Conghui, Liu Ziwei, Chen Kai, Lin Dahua. (2024). MMBench: Is Your Multi-Modal Model an All-Around Player? arXiv preprint arXiv:2307.06281.</a> <a id="subobject-tokenization-2025" class="bib-item">Chen Delong, Cahyawijaya Samuel, Liu Jianfeng, Wang Baoyuan, Fung Pascale. (2025). Subobject-Level Image Tokenization. arXiv preprint arXiv:2402.14327.</a> <a id="deepspeed-2020" class="bib-item">Rasley Jeff, Rajbhandari Samyam, Ruwase Olatunji, He Yuxiong. (2020). DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining (KDD ‚Äô20), pages 3505‚Äì3506. doi:10.1145/3394486.3406703.</a> <a id="zero-2020" class="bib-item">Rajbhandari Samyam, Rasley Jeff, Ruwase Olatunji, He Yuxiong. (2020). ZeRO: Memory Optimizations Toward Training Trillion Parameter Models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1‚Äì16. doi:10.1109/SC41405.2020.00024.</a> <a id="adam-2017" class="bib-item">Kingma Diederik P., Ba Jimmy. (2017). Adam: A Method for Stochastic Optimization. arXiv preprint arXiv:1412.6980.</a> <a id="adamw-2019" class="bib-item">Loshchilov Ilya, Hutter Frank. (2019). Decoupled Weight Decay Regularization. arXiv preprint arXiv:1711.05101.</a> <a id="bert-2019" class="bib-item">Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina. (2019). BERT: Pre-Training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT 2019, pages 4171‚Äì4186.</a> <a id="attention-is-all-you-need-2017" class="bib-item">Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob, Jones Llion, Gomez Aidan N., Kaiser ≈Åukasz, Polosukhin Illia. (2017). Attention Is All You Need. Advances in Neural Information Processing Systems, 30.</a> <a id="pixtral-12b-2024" class="bib-item">Agrawal Pravesh, Antoniak Szymon, Bou Hanna Emma, Bout Baptiste, Chaplot Devendra, Chudnovsky Jessica, Costa Diogo, De Monicault Baudouin, Garg Saurabh, Gervet Theophile, Ghosh Soham, H√©liou Am√©lie, Jacob Paul, Jiang Albert Q., Khandelwal Kartik, Lacroix Timoth√©e, Lample Guillaume, Las Casas Diego, Lavril Thibaut, Le Scao Teven, Lo Andy, Marshall Louis, Martin Arthur, Mensch Arthur, Muddireddy Pavankumar, Nemychnikova Valera, Pellat Marie, Von Platen Patrick, Raghuraman Nikhil, Bout Rozi√®re Baptiste, Sablayrolles Alexandre, Saulnier Lucile, Sauvestre Romain, Rozi√®re Baptiste, Shang Wendy, Soletskyi Roman, Stewart Lawrence, Stock Pierre, Studnia Joachim, Subramanian Sandeep, Vaze Sagar, Wang Thomas, Yang Sophia. (2024). Pixtral 12B. arXiv preprint arXiv:2410.07073.</a> <a id="roformer-2023" class="bib-item">Su Jianlin, Lu Yu, Pan Shengfeng, Murtadha Ahmed, Wen Bo, Liu Yunfeng. (2023). RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv preprint arXiv:2104.09864.</a> <a id="blip2-2023" class="bib-item">Li J, Li D, Savarese S, Hoi S. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. InInternational conference on machine learning 2023</a> <a id="llama-3-herd-2024" class="bib-item">Dubey Abhimanyu, et al. (2024). The Llama 3 Herd of Models. arXiv preprint arXiv:2407.21783.</a> <a id="reproducible-scaling-laws-2023" class="bib-item">Cherti Mehdi, Beaumont Romain, Wightman Ross, Wortsman Mitchell, Ilharco Gabriel, Gordon Cade, Schuhmann Christoph, Schmidt Ludwig, Jitsev Jenia. (2023). Reproducible Scaling Laws for Contrastive Language-Image Learning. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818‚Äì2829. doi:10.1109/CVPR52729.2023.00276.</a> <a id="sigmoid-loss-2023" class="bib-item">Zhai Xiaohua, Mustafa Basil, Kolesnikov Alexander, Beyer Lucas. (2023). Sigmoid Loss for Language Image Pre-Training. arXiv preprint arXiv:2303.15343.</a> <a id="dinov2-2024" class="bib-item">Oquab Maxime, Darcet Timoth√©e, Moutakanni Th√©o, Vo Huy, Szafraniec Marc, Khalidov Vasil, Fernandez Pierre, Haziza Daniel, Massa Francisco, El-Nouby Alaaeldin, Assran Mahmoud, Ballas Nicolas, Galuba Wojciech, Misra Ishan, Rabbat Michael, Sharma Vasu, Synnaeve Gabriel, Xu Hu, Jegou Herv√©, Mairal Julien, Labatut Patrick, Joulin Armand, Bojanowski Piotr. (2024). DINOv2: Learning Robust Visual Features Without Supervision. arXiv preprint arXiv:2304.07193.</a> <a id="internlm2-2024" class="bib-item">Cai Zheng, Cao Maosong, Chen Haojiong, Chen Kai, Chen Keyu, Chen Xin, Chen Xun, Chen Zehui, Chen Zhi, Chu Pei, Dong Xiaoyi, Duan Haodong, Fan Qi, Fei Zhaoye, Gao Yang, Ge Jiaye, Gu Chenya, Gu Yuzhe, Gui Tao, Guo Aijia, Guo Qipeng, He Conghui, Hu Yingfan, Huang Ting, Jiang Tao, Jiao Penglong, Jin Zhenjiang, Lei Zhikai, Li Jiaxing, Li Jingwen, Li Linyang, Li Shuaibin, Li Wei, Li Yining, Liu Hongwei, Liu Jiawei, Liu Kaiwen, Liu Kuikun, Liu Xiaoran, Lv Chengqi, Lv Haijun, Lv Kai, Ma Li, Ma Runyuan, Ma Zerun, Ning Wenchang, Ouyang Linke, Qiu Jiantao, Qu Yuan, Shang Fukai, Shao Yunfan, Song Demin, Song Zifan, Sui Zhihao, Sun Peng, Sun Yu, Tang Huanze, Wang Bin, Wang Guoteng, Wang Jiaqi, Wang Jiayu, Wang Rui, Wang Yudong, Wang Ziyi, Wei Xingjian, Weng Qizhen, Wu Fan, Xiong Yingtong, Xu Chao, Xu Ruiliang, Yan Hang, Yan Yirong, Yang Xiaogui, Ye Haochen, Ying Huaiyuan, Yu Jia, Yu Jing, Zang Yuhang, Zhang Chuyu, Zhang Li, Zhang Pan, Zhang Peng, Zhang Ruijie, Zhang Shuo, Zhang Songyang, Zhang Wenjian, Zhang Wenwei, Zhang Xingcheng, Zhang Xinyue, Zhao Hui, Zhao Qian, Zhao Xiaomeng, Zhao Fengzhe, Zhou Zaida, Zhou Jingming, Zhuo Jingming, Zou Yicheng, Qiu Xipeng, Qiao Yu, Lin Dahua. (2024). InternLM2 Technical Report. arXiv preprint arXiv:2403.17297.</a> <a id="omg-seg-arxiv-2024" class="bib-item">Li Xiangtai, Yuan Haobo, Li Wei, Ding Henghui, Wu Size, Zhang Wenwei, Li Yining, Chen Kai, Loy Chen Change. (2024). OMG-Seg: Is One Model Good Enough for All Segmentation? arXiv preprint arXiv:2401.10229.</a> <a id="seem-2023" class="bib-item">Zou Xueyan, Yang Jianwei, Zhang Hao, Li Feng, Li Linjie, Wang Jianfeng, Wang Lijuan, Gao Jianfeng, Lee Yong Jae. (2023). Segment Everything Everywhere All at Once. arXiv preprint arXiv:2304.06718.</a> <a id="siglip-2024" class="bib-item">Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, Lucas Beyer. Sigmoid Loss for Language Image Pre-Training, 2024. URL https://arxiv.org/abs/2303.15343.</a> <a id="fastvlms-2025" class="bib-item">Vasu, Pavan Kumar Anasosalu, Fartash Faghri, Chun-Liang Li, Cem Koc, Nate True, Albert Antony, Gokula Santhanam et al. "Fastvlm: Efficient vision encoding for vision language models." In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 19769-19780. 2025.</a> </div> <style>.bib-item{display:none}.bib-item.cited{display:block;margin-bottom:10px}</style> <script>
document.addEventListener("DOMContentLoaded", function() {
    // 1. Find all internal links in the post (usually starting with #)
    const links = document.querySelectorAll('a[href^="#"]');
    const citedIds = new Set();

    links.forEach(link => {
        // Get the ID being linked to (remove the # character)
        const id = link.getAttribute('href').substring(1);
        if (id) citedIds.add(id);
    });

    // 2. Loop through all reference items
    const refItems = document.querySelectorAll('.bib-item');
    refItems.forEach(item => {
        if (citedIds.has(item.id)) {
            item.classList.add('cited'); // This makes it visible via CSS
        }
    });
});
</script>]]></content><author><name></name></author><category term="Multimodal-Learning"/><category term="Compositional-Reasoning"/><category term="Computer-Vision"/><summary type="html"><![CDATA[Addressing MLLMs shortcomings in Compositional Reasoning through CLIP-Segmentation fusion]]></summary></entry><entry><title type="html">Perception, Localization, Planning and Control on RAE Robots</title><link href="https://matteonulli.github.io/blog/2025/cvar/" rel="alternate" type="text/html" title="Perception, Localization, Planning and Control on RAE Robots"/><published>2025-01-20T15:24:00+01:00</published><updated>2025-01-20T15:24:00+01:00</updated><id>https://matteonulli.github.io/blog/2025/cvar</id><content type="html" xml:base="https://matteonulli.github.io/blog/2025/cvar/"><![CDATA[<h5 id="r-den-braber-m-nulli-d-zegveld">R. den Braber<em>, <b>M. Nulli*</b>, D. Zegveld</em></h5> <h6 id="intelligent-robotics-lab-university-of-amsterdam">Intelligent Robotics Lab, University of Amsterdam</h6> <h6 id="--"><img src="https://www.intelligentroboticslab.nl/wp-content/uploads/2022/10/IRL-logo-300x247-1.png" alt="University of Amsterdam" height="24"/>¬† <img src="https://upload.wikimedia.org/wikipedia/commons/1/17/Uva%C2%AEmerken_ENG.png" alt="eBay" height="24"/> ¬†</h6> <h6 id="-blogpost---code-1-2-3">üìù <a href="https://matteonulli.github.io/blog/2025/var/">Blogpost</a> | üßë‚Äçüíª <a href="https://github.com/rensdebee/lab3_ws">Code 1</a>, <a href="https://github.com/rensdebee/lab2_ws">2</a>, <a href="https://github.com/rensdebee/vafr_lab1_ws">3</a></h6> <h6 id="equal-contribution">*Equal Contribution</h6> <p><br/></p> <h2 id="introduction">Introduction</h2> <p>In this blogpost we go over our process of building an integrated perception‚Äìlocalization‚Äìplanning‚Äìcontrol pipeline on the RAE robot. Below we go over Camera Calibration and Line Following, Localization and Curlying Match playing (see <a href="#video1">Video 1</a>) and Mapping, Planning and Control, to allow our <a href="https://docs.luxonis.com/hardware/rae/get-started/">RAE Robot</a> to freely move across our environment.</p> <div class="row mt-3"> <a id="video1"></a> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <video src="/assets/video/IMG_6020.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> </div> <div class="caption"> Video 1: <b>RAE Robots playing a Curlying Match with our Algorithms.</b> </div> <h3 id="camera-calibration-and-line-following">Camera Calibration and Line Following</h3> <p>In this section we demonstrate the effectiveness of our line-following pipeline by using a RAE robot to move parallel to the lines spanned by ceiling lights.</p> <p>We began with camera calibration, which is shown to reduce distortion resulting in improved navigation accuracy. We do so comparing two algorithms of differing complexity, ultimately using Zhang‚Äôs [<a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr98-71.pdf">1</a>] camera calibration technique.</p> <p>Thereafter we developed a line‚Äëfollowing algorithm.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="figure1"></a> <figure> <picture> <img src="/assets/img/line_following_v2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Overview of the full pipeline, where the lower blue part specifically covers the Line Selection and Line Following procedure. With the output of the Line Following procedure a Movement Update is calculated to direct the robot along the direction of the ceiling lights. Additionally, when no lines are detected the RAE robot is instructed to spin around until it finds the wanted ceiling lines (here in red). </div> <p>First step is edege detection through an optimized version of the Canny edge detection algorithm. The next step is line detection, whose aim is to return a list of all lines that are spanned by the ceiling lights. It leverages a fine-tuned version of the Hough transform in addition to our triangular region of interest see blue line in <a href="#figure2">Figure 2</a>. Both the edge and line detection stages are refined by tuning the parameters and by adding additional steps such as image dilation to make the ceiling lines more prominent in the resulting image. This increases the robustness of our algorithm so that it can properly detect lines in difficult conditions, for example, at different angles, lighting conditions, and deal with gaps between the ceiling lights.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure2"></a> <figure> <picture> <img src="/assets/img/frame_601.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: Displays a visualization of the output of the full pipeline. Here the blue triangle indicates our triangular region of interest. Red lines are candidate lines, while the green line is the selected line to follow. This line is determined to the the closest line to the focus point (yellow). At the endpoint of this line do we find the lines vanishing point (cyan) towards which the robot is steered.. </div> <p>Lastly, the line-following algorithm implements a way to consistently select the closest line, which removes the distraction of multiple detected lines. Next, using the selected line, we calculate a vanishing point to move towards instead of blindly following the line as this proves to increase navigational consistency and accuracy see <a href="#figure2">Figure 2</a>. Additionally, in an ablation study we found that utilizing this vanishing point approach allows our robot to still perform well, even without camera calibration, with only a slight potential reduction in navigational accuracy. This robustness highlights the strengths and adaptability of our pipeline (see <a href="#figure1">Figure 1</a>) which results in a stable and accurate ceiling light line following algorithm for the RAE robot.</p> <h3 id="localization-and-curlying-match">Localization and Curlying Match</h3> <p>In this chapter we go over our setup for localization and curling match playing with the RAE robot, see <a href="#video2">Figure 3</a>.</p> <div class="row mt-3"> <a id="video2"></a> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/IMG_6020.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" autoplay="" controls=""/> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <video src="/assets/video/IMG_6018.mp4" class="img-fluid rounded z-depth-1" width="auto" height="auto" controls=""/> </figure> </div> </div> <div class="caption"> Figure 3: <b>Curlying Match with our Algorithms.</b> In these two videos we show the accuracy of our Localization Algorithm over multiple runs and multiple points of references. </div> <p>We start by performing marker detection on QR-style code markers from APRILTAG. After improving our marker detection algorithm, we localize ourselves in real-world coordinates using the center of the field as the (0, 0) point. We consider different distance measurements and locate ourselves based on two, three, or more markers. Our marker-based localization algorithm allows us to obtain an average localization error of less than 13 cm, see <a href="#figure4">Figure 4</a>.</p> <p>We utilize an Extended Kalman Filter [<a href="https://ieeexplore.ieee.org/document/1271397">2</a>] to merge our marker-based localization with the odometry motor-based estimated position to achieve a more accurate and robust location estimate during the match, see <a href="#video2">Figure 3</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure4"></a> <figure> <picture> <img src="/assets/img/measures-apriltag.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 4: <b>Results on distance measurements from RAE Robot to Markers</b>. We report the results of measurements for three different positions on the field. For each, we note the Marker ID, the three estimated distances, and the errors with the correct distance. The 2D Distance || Error represents the error between the 2D Distance from the camera of the robot and the marker and the corresponding error with the 2D correct distance. The 3D Distance || Error are the result of the 3D distance estimated through the cv2.solvePnP function and its error with the actual 3D distance to the marker. Lastly, the version we employ is the (Ours) 2D dp Distance || Error, which stands for the 2-dimensional down-projected distance of the above 3D and its error compared to the 2D correct distance. </div> <p>Lastly, we design an algorithm which combines (<a href="#figure5">Figure 5</a>. ) a feature-based and position-based solution to ensure accurate and robust localization during the curling match. Furthermore, we incorporate object avoidance to navigate around other RAE robots that may block the path to the target location. Our algorithm achieves effective results through the coordination of three key nodes:</p> <ul> <li><b>Localization node:</b> Estimates the robot‚Äôs position using the detected markers.</li> <li><b>Driving node:</b> Uses motor-based odometry to estimate the robot‚Äôs position and combines it with marker-based localization in order to navigate toward the target location.</li> <li><b>Object avoidance node:</b> Signals when an object is in the way and identifies where in the image this object is to ensure that it is avoided (see the next section for more details).</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="figure5"></a> <figure> <picture> <img src="/assets/img/Lab2_pipeline.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 5: <b> Full flow.</b> The full pipeline for playing curling, with each node represented in a distinct color. Nodes subscribe to various RAE robot topics, shown as squares with rounded right edges, and also subscribe to topics published by other nodes, depicted as circles. </div> <p>The visual marked-based localization works as described above and publishes the current location estimate of the robot as a point on a ROS topic. The driving node picks up this topic and combines it with its motor-based odometry using an EKF. For the driving node, we implemented the motor-based odometry by hand as we ran into some curious issues when trying to center the robot‚Äôs position using the built-in odometry implementation. This built-in implementation used an EKF as well between the motor-based odometry and the Inertial Measurement Unit (IMU), but upon resetting the starting location of the robot to (0,0) the IMU would behave erratically and report weird headings with a large error from the actual heading. For our implementation, we make use of the heading (yaw) reported by the IMU. However, before every run, we automatically reset the heading to 0 (north) by reading the initial IMU yaw value and subtracting this value from all further IMU readings. Now we can rely on the reported fixed headings, we also need to localize in cases where no visual localization is provided due to not detecting markers. We do this by subscribing to the motor odometry topic. This topic publishes the robot‚Äôs location as (x,y) coordinates from its internal reference frame based on the motor odometry. However, we want the (x,y) position in our real-world coordinate frame. To obtain this we calculate the distance that the robot drives by calculating the Euclidean distance between two (x,y) coordinate points from the last 2 consecutive motor odometry messages. We then use our heading theta to calculate the robot‚Äôs (x,y) position in our real-world coordinate frame using</p> <pre><code class="language-latex">x += \cos(\theta) \cdot (distance \ driven) and y += \sin(\theta) \cdot (distance \ driven)
</code></pre> <p>Since we now have both the motor-based odometry location and the marker-based visual location, we combine these two with an EKF to create a more accurate, reliable, and robust prediction of our robot‚Äôs real-world location. This process, partially outlined in Section 2.5, is further clarified below. For both the odometry- and marker-based locations we use a different uncertainty matrix. For the marker location, we use an uncertainty matrix with a 13 cm standard deviation, this value corresponds to the average marker-based localization error that we found over a wide range of field positions, with different numbers of markers detected at different distances. The uncertainty matrix for the motor-based odometry uses a standard deviation of 20 cm, expressing the average error we measured after driving to different target locations relying solely on the motor odometry.</p> <p>Given the reliable prediction of our world location and our known (improved) heading readings, we now have the information to navigate toward the real-world coordinates of the target location. We achieve this by calculating the required heading to drive from our current estimated location to the target location. Then we calculate the smallest angle between the current heading and the required heading and determine the direction in which to turn our robot by looking at the sign of this resulting angle. Furthermore, the rotation speed is determined by scaling this angle (in radians) with a gain factor, resulting in slower turns for smaller corrections. Lastly, the forward driving speed is calculated based on the distance to the target location, with a capped maximum speed limit. This similarly to the angle case, ensures more precise movements as the robot approaches the target location.</p> <p>With the full pipeline active, the robot is able to drive towards all 5 target locations reliably, and can accurately localize itself from any starting position on the field using the detected markers, see <a href="#table1">Table 1</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="table1"></a> <figure> <picture> <img src="/assets/img/table-curling.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 1: <b>Curling Match Results</b>: We report the results of the curling match performed on 4 different runs. For each run we had a different starting order, meaning that in most runs there already were robots positioned on the field. We show the spot that we are driving to, its coordinates (X-position, Y-position), together with the absolute distance from our finishing position to the target point Localization error (cm) in centimeters. </div> <h3 id="mapping-planning-and-control">Mapping, Planning and Control</h3> <p>Finally we report the process of mapping and finding a path through a maze using only the video stream as captured with the RGB camera off the RAE robot.</p> <p>First we create a 3D point cloud is created through Structure from Motion (SfM) and optimize it with COLMAP [<a href="https://ieeexplore.ieee.org/document/7780814">3</a>], see <a href="#figure6">Figure 6</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="table1"></a> <figure> <picture> <img src="/assets/img/colmap.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 6: <b>VisualSFM + COLMAP</b>: Results of using the VisualSFM algorithm to do offline Structure from Motion given our sequence of images extracted with additional improved reconstruction results utilizing COLMAP. </div> <p>Thereafter, we filter the point cloud through a quantiles-based filtering method (<a href="#figure7">Figure 7</a>). This filtered cloud is converted into a topology mapping using approximate cell decomposition (<a href="#figure8">Figure 8</a>).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure7"></a> <figure> <picture> <img src="/assets/img/quantile-based-filtering.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 7: <b>Reconstruction results on the 3D point clouds</b>. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure8"></a> <figure> <picture> <img src="/assets/img/cell-decomposition.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 8: <b>Topological mapping of the reconstructed point clouds</b>. We report (left) the original grid along with the binary approximation through cell decomposition (center), as well as our further improved map (right), filling the edges and removing non-connected points. </div> <p>This topology mapping is then used to plan a path from the start to the end of the maze using Dijkstra‚Äôs algorithm. We first use vanilla Dijkstra, soon realizing the infeasibility of the algorithm with edges weighted by one, see <a href="#figure9">Figure 9</a>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure9"></a> <figure> <picture> <img src="/assets/img/not_optimized_planned_path.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 9: <b>Vanilla Dijkstra with edges weighted by one.</b> The orange pixels indicate occupied cells, black pixels indicate free cells, and white pixels indicate the planned path. </div> <p>We thus focused on improving the route found using the default Dijkstra a lgorithm by changing it so that we prioritize movement along the center of the roads spanned by the maze. To this effect, we devised a different way to compute the cost for each edge. Instead of setting the cost to one, we added a distance to the occupied cells penalty. This distance penalty should be high if an edge is close to a wall and low if it is far away.</p> <p>We used <code>scipy.ndimage.distance\_transform\_edt</code> to calculate for each free cell the shortest distance to each occupied cell and scale this by a factor of a hundred. However, adding this distance to each edge would give the opposite effect, as free cells in the middle have a larger distance to occupied cells. We could invert the distance by multiplying it by minus one. However, as Dijkstra tries to find the route with the lowest cost, this leads to negative loops and no solution. Therefore, instead, we took the cell with the largest distance and added this to the distance multiplied by minus one. This resulted in the cell with the largest distance to a free cell not having a penalty and cells close to the wall having the most significant penalty. As our graph is directed, we added the penalty of the outgoing node to each of its edges. For example, if we have the graph A ‚Üí B. Edge ‚Üí would get a cost of 1+ penalty(A). The path found using this cost function can be found in <a href="#figure10">Figure 10 (left)</a>; this figure clearly shows that the new route follows the middle of the maze, making it considerably easier, more robust, and safer for a robot to follow. Additionally, <a href="#figure10">Figure 10 (right)</a> shows the computed penalty map, showing that our method works since cells in the middle of the path have a low penalty (white) and cells closer to the walls have an increasingly higher penalty (red).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure10"></a> <figure> <picture> <img src="/assets/img/optimized-planning.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 10: <b>Our optimized planned path (a) that makes use of penalty map (b) in order to calculate the cost of each edge.</b> The orange pixels in (a) indicate occupied cells, the black pixels indicate free cells, and the white pixels indicate the planned path. In (b), the shade of red indicates the applied penalty amount, which increases the closer a cell is to a wall. </div> <h2 id="citation">Citation</h2> <p>If you use this work, please cite:</p> <pre><code class="language-bibtex">@misc{denbraber2025cvar,
  author  = {den Braber, R., and Nulli, M., and Zegveld, D.},
  title   = {Perception, Localization, Planning and Control on RAE Robots},
  howpublished  = {https://matteonulli.github.io/blog/2025/cvar/},
  year    = {2025},
}
</code></pre>]]></content><author><name></name></author><category term="Computer-Vision"/><category term="Robotics"/><summary type="html"><![CDATA[Computer Vision for Autonomour Robots]]></summary></entry><entry><title type="html">Optimizing Predictions: Vocabulary Reduction and Contrastive Decoding in LLMs</title><link href="https://matteonulli.github.io/blog/2024/earlyexit/" rel="alternate" type="text/html" title="Optimizing Predictions: Vocabulary Reduction and Contrastive Decoding in LLMs"/><published>2024-11-26T10:10:45+01:00</published><updated>2024-11-26T10:10:45+01:00</updated><id>https://matteonulli.github.io/blog/2024/earlyexit</id><content type="html" xml:base="https://matteonulli.github.io/blog/2024/earlyexit/"><![CDATA[<h5 id="ka-abdel-sadek--m-nulli--j-velja--j-vincenti--g-desimini">K.A. Abdel Sadek *, <strong>M. Nulli</strong> *, J. Velja *, J. Vincenti *, G. Desimini</h5> <h6 id="university-of-amsterdam-uva-bosch-delta-lab-krueger-ai-safety-lab-kasl">University of Amsterdam, UvA-Bosch Delta Lab, Krueger AI Safety Lab (KASL)</h6> <h6 id="----"><img src="https://upload.wikimedia.org/wikipedia/commons/1/17/Uva%C2%AEmerken_ENG.png" alt="University of Amsterdam" height="24"/> ¬† <img src="https://ivi.fnwi.uva.nl/uvaboschdeltalab/images/logo.png" alt="UvA-Bosch Delta Lab" height="24"/>¬† <img src="https://www.kasl.ai/wp-content/uploads/2023/10/kasl-main-1.png" alt="KASL" height="24"/>¬†</h6> <h6 id="-paper---blogpost---code">üìÑ <a href="https://arxiv.org/abs/2410.18952">Paper</a> | üìù <a href="https://matteonulli.github.io/blog/2024/earlyexit/">Blogpost</a> | üßë‚Äçüíª <a href="https://github.com/joanvelja/Confidently_Exiting/blob/main/">Code</a></h6> <h6 id="accepted-to-neurips-efficient-natural-language-and-speech-processing"><em>Accepted to NeurIPS, Efficient Natural Language and Speech Processing</em></h6> <h6 id="equal-contribution">*Equal Contribution</h6> <hr/> <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['\\[', '\\]']],
    },
    svg: { fontCache: 'global' }
  };
</script> <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css"/> <script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js"></script> <script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" onload="renderMathInElement(document.body, { delimiters: [ {left: '$$', right: '$$', display: true}, {left: '\\[', right: '\\]', display: true}, {left: '$', right: '$', display: false}, {left: '\\(', right: '\\)', display: false} ], // keep default ignore list so code/pre are skipped });"></script> <h2 id="introduction">Introduction</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="mainfigure"></a> <figure> <picture> <img src="/assets/img/nips_earlyexit2.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: <b> Presenting the paper at NeurIPS.</b> From left to right, Karim Abdel Sadek, Jort Vincenti, Joan Velja and Matteo Nulli. </div> <p>Recent advancements in Large Language Models (LLMs) have significantly improved performance across various Natural Language Processing (NLP) tasks (<a href="#bert-pre-training-2019">Devlin et al., 2019</a>; <a href="#language-models-few-shot-2020">Brown et al., 2020</a>; <a href="#scaling-language-models-2021">Rae et al., 2021</a>; <a href="#using-deepspeed-megatron-2022">Smith et al., 2022</a>; <a href="#palm-scaling-2023">Chowdhery et al., 2023</a>). Efforts at improving the capabilities of these models have revolved around scaling the number of parameters and data (<a href="#scaling-laws-2020">Kaplan et al., 2020</a>; <a href="#traning-compute-optimal-2022">Hoffmann et al., 2022</a>). However, the substantial computational load presents a practical challenge during inference, particularly in resource-constrained applications. To address this issue, Early-Exiting mechanisms (<a href="#branchynet-2016">Teerapittayanon et al., 2016</a>; <a href="#right-tool-2020">Schwartz et al., 2020</a>; <a href="#leebert-2021">Zhu, 2021</a>; <a href="#model-depth-analysis-2021">Simoulin &amp; Crabb√©, 2021</a>; <a href="#fast-robust-early-exiting-2023">Bae et al., 2023</a>) have been proposed, thus reducing the inference time without significantly compromising performance. This approach is crucial because while scaling model architectures is beneficial during training, the same amount of compute may not be necessary at inference time for every input, especially for simpler tasks (<a href="#transformer-key-value-memories-2021">Geva et al., 2021</a>; <a href="#transformer-promoting-concepts-2022">2022</a>). By enabling intermediate layer decoding, Early Exiting offers a promising solution to balance computational efficiency and model accuracy, ensuring that LLMs remain practical and effective in diverse application scenarios.</p> <p>We analyze the early exiting paradigm for LLMs, conducting a preliminary analysis covering challenges associated with this framework. First, we study a phenomenon of non-finetuned LMs, where the confidence at early layers is deemed to be high, but where accuracy is not satisfactory, thus resulting in poor calibration (<a href="#reducing-overconfidence-2022">Mielke et al., 2022</a>; <a href="#linguistic-calibration-2024">Band et al., 2024</a>). This gives us grounds to implement some heuristics for the minimum exit layer in our experiments. We repeat the same analysis for fine-tuned models, observing that the phenomena is not as prominent.</p> <p>We first present a method (<strong>Vocabulary Pruning</strong>) for increasing model efficiency while remaining confident in the quality of the resulting predictions. Specifically, drawing from <a href="#confident-adaptive-language-modeling-2022">Schuster et al. (2022)</a>, we modify their Softmax approach, by pruning the vocabulary size across layers. This allows us to speed-up the inference time of our predictions, with a negligible loss in performance. However, in order to offset the decrease in performance, we thus propose within-model <strong>Contrastive Decoding</strong> (<a href="#contrastive-decoding-2023">Li et al., 2023</a>) as an alternative means for confidence.</p> <p>Our methods are validated empirically on different NLP tasks, including text summarization and question answering. Our experiments demonstrate that, combining the two aforementioned approaches, we attain a Pareto improvement with respect to FLOPS efficiency and performance.</p> <h2 id="related-works">Related Works</h2> <p>While the semantic nature of Natural Language is rich, some parts of a sentence often lack variance. In such cases, the number of layers the model has to potentially go through to return the right token is relatively low. Following this intuition, there have been a large number of studies introducing different Early-Exiting frameworks (<a href="#branchynet-2016">Teerapittayanon et al., 2016</a>; <a href="#right-tool-2020">Schwartz et al., 2020</a>; <a href="#model-depth-analysis-2021">Simoulin &amp; Crabb√©, 2021</a>; <a href="#leebert-2021">Zhu, 2021</a>; <a href="#fast-robust-early-exiting-2023">Bae et al., 2023</a>; <a href="#transformer-key-value-memories-2021">Geva et al., 2021</a>; <a href="#transformer-promoting-concepts-2022">2022</a>).</p> <p><a href="#confident-adaptive-language-modeling-2022">Schuster et al., 2022</a> investigates Softmax-based confidence measures. Here, the challenge of early-exiting is addressed by introducing a framework that dynamically allocates computational resources per input and generation time-step. The exiting criterium is based on the difference in probits between the two most probable predicted tokens. This ensures a gain in computational efficiency, without excessive performance degradation.</p> <p><a href="#fast-robust-early-exiting-2023">Bae et al. (2023)</a> introduces the (FREE) Fast and Robust Early Exiting framework. FREE uses a shallow-deep module to compute the computational path, hence determining the portion of layers used. Alternatively, <a href="#class-exclusion-2024">Wang et al. (2024)</a> propose a class-based early-exiting strategy. This method leverages the use of intermediate layer features to exclude part of the tokens, allowing later layers to focus on a reduced subset of potential tokens.</p> <p>Contrastive Decoding (<a href="#contrastive-decoding-2023">Li et al., 2023</a>) is a technique proposed to reduce unwanted behaviors in LLMs such as hallucinations, repetition, and incoherence. The method employs two models, a smaller one called <i>amateur</i>, and a larger one called <i>expert</i>. They both perform auto-regressive text generation on the same data, and the final predicted token is selected based on the output difference between the predictions of the expert and amateur. However, employing two LLMs is highly inefficient, both in terms of memory and compute. Alternative methods have been proposed, which employ the contrastive decoding scheme, without the necessity of using two models. An example of such work is the idea of Auto-Contrastive Decoding (<a href="#auto-contrastive-decoding-2023">Gera et al., 2023</a>). The authors show how contrasting outputs of different layers within the same model can benefit text generation outputs. The study shows that predictions of shallow layers can help those of deeper ones to attain better results. Other studies have adapted this technique to different tasks such as reducing hallucination in LLMs (<a href="#dola-contrasting-layers-2024">Chuang et al., 2024</a>). Our proposed contrastive decoding techniques are based on both <a href="#auto-contrastive-decoding-2023">Gera et al. (2023)</a> and <a href="#dola-contrasting-layers-2024">Chuang et al. (2024)</a> and adapted to the aforementioned early-exiting framework of <a href="#confident-adaptive-language-modeling-2022">Schuster et al. (2022)</a>.</p> <h2 id="preliminaries-and-experimental-setting">Preliminaries and Experimental Setting</h2> <h3 id="transformer-architecture">Transformer Architecture</h3> <p>The Transformer network, introduced by <a href="#attention-is-all-you-need-2017">Vaswani et al. (2017)</a>, is structured into $L$ layers, each comprising two distinct sublayers: the Multi-Head Attention (MHA) layer and the Feed-Forward Network (FFN) layer. Within this framework, updates to the residual stream for a subsequent prediction are carried out via the following recursive formula:</p> <p align="center"> \[ h^\ell_t = \text{Transformer}^\ell(h^{\ell-1}_t) \] </p> <p>where $\ell$ represents each layer from 1 to $L$, and $h_t^0$ denotes the output of the embedding layer $\mathbf{W_E}$. The embedding layer $\mathbf{W_E} \in \mathbb{R}^{d_{\text{vocab}} \times d_{\text{model}}}$, transforms the tokens $y_{1:t}$ having size $d_{\text{vocab}}$, into dense vector representations sized $d_{\text{model}}$.</p> <p>After processing through the $L$-th layer, the final prediction for the next token, $\hat{x}_{t+1}$, is produced by</p> <p align="center"> \[ p(\hat{x}_{t+1} \mid x_{&lt; t+1}) = \text{softmax}(\textbf{W}_L h^L_{t}) \] </p> <p>where</p> <p align="center"> \[ \textbf{W}_L \in \mathbb{R}^{d_{\text{model}} \times d_{\text{vocab}}} \] </p> <p>is the linear classifier of block L responsible for mapping back the output of the FNN at that block from $d_{\text{model}}$ to $d_{\text{vocab}}$.</p> <p>Our approach incorporates an early-exiting strategy, wherein the generation of the next token can occur at any layer $\ell$ if the computed confidence score $c_\ell$ exceeds a specified threshold $\tau$. When an early exit is triggered at layer $\ell$, it necessitates updating the key and value pairs in subsequent layers to ensure proper attention mechanisms for future tokens. To efficiently manage this, a state copying technique is employed, where the hidden states from the early-exited layer i.e. $h_{t+1}^{\ell}$ are duplicated across subsequent layers ($h_{t+1}^i = h_{t+1}^{\ell}$ for every $i$ such that $i = \ell + 1, ‚Ä¶ , L$). This process maintains computational efficiency and model performance, even in compact - for today‚Äôs standards - model configurations like T5.</p> <h3 id="experimental-setting">Experimental Setting</h3> <p>In this section, we introduce the experimental setting used in both <a href="#methodology">‚ÄúMethodology‚Äù</a> and <a href="#experiments">‚ÄúExperiments‚Äù</a>. We evaluate the encoder-decoder T5 model (<a href="#exploring-limits-2020">Raffel et al., 2020</a>) on two different datasets and two different downstream tasks:</p> <ul> <li>Stanford Question Answering Dataset (SQuAD) (<a href="#squad-2016">Rajpurkar et al., 2016</a>): over 100k annotated data, 10k of which used for evaluation.</li> <li>SamSum (<a href="#samsum-corpus-2019">Gliwa et al., 2019</a>): a human-annotated dataset for abstractive Summarization with more than 800 samples in the Validation set.</li> </ul> <p>Each dataset has its own evaluation metric. Question Answering on SQuAD and Summarization on SamSum will be evaluated via F1 and Rouge-L scores respectively.</p> <p>Additionally, we compare the performance and effects of our proposed methods on:</p> <ul> <li>Pre-trained-only version of T5, from <a href="https://huggingface.co/google-t5/t5-large" target="_blank" rel="noopener noreferrer">t5-large</a>,</li> <li>Fine-tuned version of T5, <a href="https://huggingface.co/jvelja/t5-squad" target="_blank" rel="noopener noreferrer">t5-squad</a>, <a href="https://huggingface.co/jvelja/t5-samsum" target="_blank" rel="noopener noreferrer">t5-samsum</a>. Both are fine-tuned on the corresponding training dataset.</li> </ul> <p>Our code is heavily based and builds on top of the publicly available codebase <a href="https://github.com/raymin0223/fast_robust_early_exit" target="_blank" rel="noopener noreferrer"> fast-robust-early-exit</a> (<a href="#fast-robust-early-exiting-2023">Bae et al., 2023</a>).</p> <h2 id="methodology">Methodology</h2> <h3 id="early-exiting-via-the-softmax-approach">Early Exiting via the Softmax Approach</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="figure-1"></a> <figure> <picture> <img src="/assets/img/early_exit/softmax_shrink.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 1:</b> Softmax Pruning approaches: illustration of (1) <i>fixed</i> and (2) <i>decaying</i> pruning methods for token importance preservation. (3) <i>Adaptive</i> decaying not illustrated. </div> <p>Our first approach (<a href="#figure-1">Figure 1</a>) aims to improve a limitation of the Softmax response method introduced by <a href="#confident-adaptive-language-modeling-2022">Schuster et al. (2022)</a>. We denote the final output of layer $\ell$ as</p> <p align="center"> $\textbf{v}^\ell = \text{Softmax}(\textbf{W}_\ell h^{\ell}_{t})$ </p> <p>The so-called confidence measure is computed as the difference between the top two values of the probits vector $\textbf{v}$, at each layer $\ell$. We denote this measure as $c_{t+1}^{\ell}$. Let us define an early-exit threshold $\tau_{t+1}^{\ell}$ at each layer. If our confidence measure exceeds the early exit-threshold,</p> \[c_{t+1}^{\ell} \geq \tau_{t+1}^{\ell}\] <p>the model exits early, providing us with the prediction for the next token computed at layer $\ell$. Otherwise, it continues by going into the next Transformer block. However, the matrix multiplication inside Softmax, i.e., $\mathbf{W_\ell} h_{t}^{\ell}$ is computationally expensive, especially when iterated over multiple layers. The exact number of Floating Point Operations (FLOPs) for the above corresponds to $d_{\text{model}} \times d_{\text{vocab}} \times L$. Hence, by pruning the vocabulary size at the first layer from $d_{\text{vocab}}$ to $k$, the number of computations required will reduce to $d_{\text{model}} \times k \times L$.</p> <p>Recall that $\mathbf{W_\ell} \in \mathbb{R}^{d_{\text{vocab}} \times d_{\text{model}}}$, where $d_{\text{vocab}} \approx 32000$ is our vocabulary size, and $d_{\text{model}}$ is equal to the size of the last hidden representation of our FFN. Both parameters are on a scaling upward trend in SOTA architectures. We argue that most of these computations are redundant, and potentially not necessary for some tasks. In <a href="#figure-2">Figure 2</a>, we show the boxplots for the rank of the final predicted token at each layer, across not fine-tuned and fine-tuned models, for two different datasets. The main takeaway from these images is that the final predicted token is often already highly ranked from the first few layers of our model. This behavior is more explicit in Figures <a href="#figure-2b">2b</a> and <a href="#figure-2d">2d</a>, where we use fine-tuned models on our downstream tasks. On the other hand, confidence alone can be a deceiving measure. LLMs can be overconfident in the first layers, causing the model to exit prematurely. Our desiderata is for the model to be confident at the same time when its prediction has a high accuracy, that is, to be calibrated. However, we interestingly note that such behavior is rarely observed at early layers. In Figures <a href="#figure-3">3</a> and <a href="#figure-4">4</a>, we see the accuracy and the confidence across each layer. The model in the first layers presents an anomalously high confidence, while its performance is still poor. Early exiting only based on the Softmax response would result in bad performance. We decide to set a minimum exit layer parameter $j$, which forces the model to consider exiting only after this layer. Note that this parameter is highly dependent on the model and dataset one experiments on. For fine-tuned models for example, one expects this parameter to be smaller.</p> <p>Motivated by these findings, we introduce three additional modifications to the Softmax response approach.</p> <p><strong>Softmax response via fixed vocabulary pruning</strong> After the minimum early exit layer $j$, we prune $\textbf{W}_{j+1}$, retaining its top-k tokens in the new unembedding matrix. We define the size of the new pruned matrix as</p> <p align="center"> $$ \large \tilde{\textbf{W}}_{j+i} \in \mathbb{R}^{d_{\text{model}} \times k}, \quad \textrm{for} \quad i = 1, \ldots, L-j \quad \textrm{and} \quad k \ll d_{\text{vocab}} $$ </p> <p>The size is kept fixed to $k$ for all subsequent layers. Theoretically, calculating the ratio between the original number of computations required in the original approach and ours, we get</p> <p align="center"> $$ \large \frac{d_{\text{model}} \times d_{\text{vocab}} \times L}{d_{\text{model}} \times k \times (L - j) + d_{\text{model}}\times d_{\text{vocab}} \times j} $$ </p> <p>which corresponds to an approximate efficiency gain in the order of</p> <p align="center"> $$ \large \mathcal{O}\left(\frac{d_{\text{vocab}}}{k} \times (L-j)\right) $$ </p> <p><strong>Softmax response via decaying vocabulary pruning</strong></p> <p>As one can note from <a href="#figure-2">Figure 2</a>, the rank of the predicted token smoothly decreases across layers. Similarly, we start by pruning the $\textbf{W}_{j+1}$ matrix, given a minimum early exit layer $j$. We retain its top $k$-tokens, obtaining</p> <p align="center"> $$ \large \tilde{\textbf{W}}_{j+i} \in \mathbb{R}^{k \times d_{\text{model}}} $$ </p> <p>Now, instead of keeping the reduced matrix size fixed, we further prune it at every successive layer. Given $\tilde{\textbf{W}}_{j+i}$ of size $k_1$, we prune it at layer $j+i+1$ to a reduced matrix of size $k_2$, where</p> <p align="center"> $$ \large k_2 = \max\left( k^*, \frac{k_1}{1 + \frac{k_1 - k^*}{k^*} \cdot \frac{j + i}{\text{num\_layers}}} \right) $$ </p> <p>$k^*$ here indicates a lower bound on the size $\tilde{\textbf{W}}_{j+i+1}$ can reach. The function we define has been chosen based on <a href="#figure-2a">Figure 2a</a>, hence to be robust against the worst-case scenario. It approximates the decay in the ranking of the top-k token in that case. The efficiency gain is even more prominent than in the case of fixed pruning.</p> <p><strong>Softmax response via adaptive vocabulary pruning</strong></p> <p>It can be seen in Figures <a href="#figure-3">3</a> and <a href="#figure-4">4</a> that, after a few blocks, the confidence and the F1 score of each layer are highly correlated. Together with <a href="#figure-2">Figure 2</a>, this poses a basis for an approach where the amount of retained top-k tokens at each layer is adapted to the confidence at the previous one. We propose the following:</p> <p align="center"> $$ \large k^\ell = \text{vocab\_size} \times (1 - \text{confidence}^{\ell - 1} \times \text{scaling_factor}) $$ </p> <p>Where $k^{\ell}$ is the amount of retained tokens at layer $\ell$, $vocab_size$ is the size of the full vocabulary, $\text{confidence}^{\ell - 1}$ is the confidence at layer $\ell - 1$, <strong><em>scaling_factor</em></strong> is a coefficient that is introduced to avoid retaining 0 tokens in case of confidence = 1. For simplicity, this has been set to 0.9 during our experiments.</p> <p>To summarize, our final predicted token is often highly ranked across all layers. Due to this, pruning the vocabulary matrix allows us to reduce the amount of computations at each block, discarding only irrelevant tokens. While we may potentially trade-off some performance, this further speeds up the runtime of our model, allowing us to obtain considerable efficiency gains.</p> <div id="figure-2" class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure-2a"></a> <figure> <picture> <img src="/assets/img/early_exit/boxplot_topk_rank_evalsquad_google-t5_t5-large.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>Figure 2a:</b> Non fine-tuned T5-Large model, SQuAD Dataset</div> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure-2b"></a> <figure> <picture> <img src="/assets/img/early_exit/boxplot_topk_rank_evalsquad_jvelja_t5-squad.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>Figure 2b:</b> Fine-tuned T5-Large model, SQuAD Dataset</div> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure-2c"></a> <figure> <picture> <img src="/assets/img/early_exit/boxplot_top1_rank_evalsamsum_google-t5_t5-large.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>Figure 2c:</b> Non fine-tuned T5-Large model, SamSum Dataset</div> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure-2d"></a> <figure> <picture> <img src="/assets/img/early_exit/boxplot_topk_rank_evalsquad_jvelja_t5-squad.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>Figure 2d:</b> Fine-tuned T5-Large model, SamSum Dataset</div> </div> </div> <div class="caption"> <b>Figure 2:</b> Boxplots of the rank of final predicted token at each layer, across 2 different models and 2 different datasets. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure-3"></a> <figure> <picture> <img src="/assets/img/early_exit/conf_metric_squad_google_t5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>Figure 3:</b> Confidence vs F1 accuracy. T5-base model, SQuAD dataset</div> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure-4"></a> <figure> <picture> <img src="/assets/img/early_exit/conf_metric_squad_tuned.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>Figure 4:</b> Confidence vs F1 accuracy. Fine-Tuned model, SQuAD dataset</div> </div> </div> <h3 id="measuring-confidence-via-contrastive-decoding">Measuring Confidence Via Contrastive Decoding</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="figure-5"></a> <figure> <picture> <img src="/assets/img/early_exit/IMG_0311.JPG" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 5:</b> Dynamic Contrastive Decoding: illustration of how we leverage <i>Contrastive Decoding</i> within model layers. </div> <p>The second approach (<a href="#figure-5">Figure 5</a>) is inspired by <a href="#contrastive-decoding-2023">Li et al. (2023)</a>. We propose to apply their framework to address one of the limitations of <a href="#confident-adaptive-language-modeling-2022">Schuster et al., 2022</a>. Namely, as introduced in previous sections, the Softmax response approach relies on a static notion of confidence, which depends only on the probability distribution computed at the current layer. Such approach may ignore the evolution of the probits‚Äô magnitudes across the model.</p> <p>By contrasting the outputs of smaller LMs with larger ones, Contrastive Decoding (CD) accounts for the difference in representations computed by amateur and mature layers. The core goal of this method is to refine the output distribution by filtering through the lens of larger models, retaining only their superior linguistic predictions. The original implementation involves the use of two models in parallel, returning the log-ratio between the probits $p_{\text{EXP}}$ of a large LM - called the expert - and the probits $p_{\text{AMA}}$ of a small LM - called the amateur.</p> <p>Naturally, this captures the dynamical change of the token‚Äôs distribution when computed at different heights of the Attention‚Äôs stack.</p> <p>Following <a href="#contrastive-decoding-2023">Li et al. (2023)</a>, we first implement the CD adaptive plausibility constraint, $\nu_{\text{head}}(x_{&lt; t})$, defined as:</p> <p align="center"> $$ \large \nu_{\text{head}}(x_{&lt; t}) = \{x_t \in V : p_{\text{EXP}}(x_t|x_{&lt; t}) \geq \alpha \cdot \underset{x'_t \in V}theme (p_{\text{EXP}}(x'_t|x_{&lt; t}))\} $$ </p> <p> where $V$ is our vocabulary. </p> <p>It‚Äôs important to recognize that smaller LMs, despite their limitations, do reliably capture basic elements of English grammar, such as subject-verb agreement. Applying the CD objective indiscriminately could penalize these correct linguistic behaviors, leading to false negatives. It might also erroneously reward implausible token choices, resulting in false positives. To address these potential pitfalls, we incorporate the plausibility constraint $\nu_{\text{head}}$ into our framework. Given a preceding context $x_{&lt; t}$, this constraint selects a subset of plausible next tokens, out of the vocabulary $V$, whose probabilities are above a threshold. The threshold is a fraction $\alpha$ of the max probability token in the vocabulary. We set the hyperparameter $\alpha \in[0, 1]$ to 0.1, as done by <a href="#contrastive-decoding-2023">Li et al. (2023)</a>. Borrowing from <a href="#auto-contrastive-decoding-2023">Gera et al. (2023)</a>, the contrastive objective, called Log Contrastive Difference (LCD), is defined as:</p> \[\large p_{\text{LCD}}(x_t | x_{&lt; t}) = \text{Softmax}\left(\log \frac{p_{\text{EXP}}(x_t | x_{&lt; t})}{p_{\text{AMA}}(x_t | x_{&lt; t})}\right) \sum_{x_t \in V_{head}(x_{&lt; t})} p_{EXP}(x_t | x_{&lt; t})\] <p>The LCD objective is designed to promote text patterns that are preferred by the expert LMs and discourage those that are typically produced by the amateur LMs. It works in tandem with the plausibility constraint, to ensure that the penalization of amateur behaviors does not disregard grammatically correct and sensible language constructs. The final distribution will be:</p> \[\large p_{\text{DCD}}(x_t | x_{&lt; t}) = \begin{cases} p_{\text{LCD}}(x_t | x_{&lt; t}) &amp; \text{if} \ x_t \in V_{\text{head}}(x_{&lt; t}) \\ p_{\text{EXP}}(x_t | x_{&lt; t}) &amp; \text{otherwise} \end{cases}\] <p>We utilize this defined distribution to compute the new confidence $c^{\ell}_t$. By doing so, we overcome the static nature of the confidence measure usually considered in the Early-Exiting literature.</p> <p>On the other hand, we remind that our approach is based on the use of one single model.</p> <p>Building upon <a href="#auto-contrastive-decoding-2023">Gera et al., 2023</a>, we include their variant of auto-contrastive decoding into our early-exiting framework. Here, $p_{\text{EXP}}$ and $p_{\text{AMA}}$ are respectively proxied by the current layer $\ell$ and by the layer $\lfloor{\frac{\ell}{2}}\rfloor$. This intuition is aligned with findings by <a href="#contrastive-decoding-2019">Elbayad et al. (2019)</a> and <a href="#transformer-key-value-memories-2021">Geva et al. (2021)</a>; <a href="#transformer-promoting-concepts-2022">Geva et al. (2022)</a>. We will refer to this auto-contrastive decoding strategy as ‚ÄúWeighted Contrastive Decoding‚Äù. One question that arises from this idea is the previous layer selection. Clearly, this choice of the amateur layer is very arbitrary.</p> <p>We tackle this problem drawing from <a href="#dola-contrasting-layers-2024">Chuang et al., 2024</a>. The authors suggest selection via distance-in-distribution through Jensen-Shannon Divergence. This way, they claim, it is possible to find the most fit amateur layer. They do so by contrasting the final distribution against a set of candidate possible premature layers. The layer selected as the one with highest JSD w.r.t. the expert one. They also divide the layers into 2 to 4 buckets of $J$ based on the total number of layers, relying on a validation set to choose the best bucket for each task. Our claim is that the bucketing strategy is suboptimal for several reasons. First, it requires task-specific selection, which is undesirable since these models are utilized by end users for open-ended generation. Second, bucketing does not address the implicit bias JSD will have towards the lower layers of the distribution. Earlier representations are necessarily more diverse, since the set of plausible tokens for autoregressive generation gets narrower as one goes deeper into the stack. For this reason, we discount the JSD value between two distributions $i, j$ by the layer distance $\ell_j - \ell_i$. The discounting allows us to capture the layers at which there is a more significant distribution change w.r.t. the one we find ourselves at, thus obtaining a meaningful signal from the chosen contrastive distribution.</p> <p>Consider the current expert layer $\ell$, and set of plausible amateur layer $J = { 2, ‚Ä¶, L-1 }$. The selected layer $m$ is obtained as</p> <p> $$ \large m = \underset{j\in J}{\text{argmax}} \frac{1}{\ell - j} \text{JSD} (p_{\ell}(x_t | x_{&lt; t}),p_{j}(x_t | x_{&lt; t})) $$ </p> <p>To illustrate the above method, in (<a href="#figure-6">Figure 6</a>) we show the JSD contrast on a given sample.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="figure-6"></a> <figure> <picture> <img src="/assets/img/early_exit/jsds.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b>Figure 6:</b> Evolution in <i>JSD</i> distribution. T5-model, SQuAD Dataset </div> <p>We will call this technique ‚ÄúJensen-Shannon Divergence (JSD) Contrastive Decoding‚Äù.</p> <p>Finally, to get the best of both worlds, we experiment with a mixed approach between Contrastive Decoding and Softmax pruning. The rationale here is that we can combine the CD confidence measure together with the relevant top-k tokens in the logits we find with the pruning done for the Softmax vocabulary pruning approach.</p> <h2 id="experiments">Experiments</h2> <h3 id="softmax-speed-up">Softmax Speed-Up</h3> <p>In this section, we report the results of the different Softmax vocabulary reductions applied to the $\textbf{W}_{j}$ matrix. The aim is to achieve similar performance with regards to the evaluation metrics, while significantly reducing the amount of FLOPs required. We implement the previously proposed approaches and perform our experiments by building on the available <a href="https://github.com/raymin0223/fast_robust_early_exit" target="_blank" rel="noopener noreferrer"> codebase implementation</a>. The minimum exit layer is based on the lowest confidence level found in <a href="#figure-3">Figure 3</a> and <a href="#figure-4">Figure 4</a>. We run experiments, either with the Fixed or Decaying approaches, as presented in <a href="#early-exiting-via-the-softmax-approach">Section ‚ÄúEarly Exiting via the Softmax Approach‚Äù</a>. We evaluate the models based on their respective performance metrics and the number of floating point operations (FLOPs). The evaluation is conducted for both the Question-Answering (see <a href="#figure-7">Figure 7</a>) and Summarization task (see <a href="#figure-8">Figure 8</a>).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/Final%20Plot%20Squad%20F1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>F1 Metric</b> for T5-Large and T5-Finetuned with <i>No</i>, <i>Fixed</i>, or <i>Decaying</i> reduction applied to the matrix</div> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/Final%20Plot%20Squad%20Flops.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>FLOPs per sample during confidence estimation</b> for T5-Large and T5-Finetuned with <i>No</i>, <i>Fixed</i>, or <i>Decaying</i> reduction applied to the matrix</div> </div> </div> <div class="caption"> <a id="figure-7"></a><b>Figure 7:</b> Performance on Question-Answering Task: Comparison of model performance in terms of F1 score and the number of FLOPs generated per sample during confidence estimation. The minimum exit layer was set to 7 for T5-Large (which sets <i>k=842</i> for fixed) and 2 for T5-Large Finetuned (which sets <i>k=2781</i> for fixed), with the confidence set to 0.9. The amount of FLOPs represents the average from 100 samples and is only calculated during confidence estimation. <a href="#fast-robust-early-exiting-2023">Bae et al. (2023)</a>. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/Final%20Plot%20SamSum%20Rougl.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>RougeL Metric</b> for T5-Large and T5-Finetuned with <i>No</i>, <i>Fixed</i>, or <i>Decaying</i> reduction applied to the matrix</div> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/Final%20Plot%20Samsum%20flops.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>FLOPs per sample during confidence estimation</b> for T5-Large and T5-Finetuned with <i>No</i>, <i>Fixed</i>, or <i>Decaying</i> reduction applied to the matrix</div> </div> </div> <div class="caption"> <a id="figure-8"></a><b>Figure 8:</b> Performance on Summarization Task. Comparison of model performance in terms of ROUGE-L score and the number of FLOPs generated per sample. The minimum exit layer was set to 7 for T5-Large (which sets <i>k=842</i> for fixed) and 2 for T5-Large Finetuned (which sets <i>k=2781</i> for fixed), with the confidence set at 0.9 for both. The amount of FLOPs represents the average from 100 samples and is only calculated during confidence estimation. <a href="fast-robust-early-exiting-2023">Bae et al. (2023)</a>. </div> <p> Both plots display the following trend: similar performance is achieved across the evaluation metrics, but the amount of FLOPs decreases by a factor of 100x. Additionally, comparing Fixed and Decaying reduction, half of the FLOPs are utilized by the latter, which however incurs a 2% loss in performance. This illustrates the trade-off: choosing a smaller $k$ reduces the number of FLOPs but at the cost of a degrading performance. Additionally, due to fine-tuned models exiting at earlier stages, fewer FLOPs are computed overall. However, the same trade-off remains. We set the threshold $\tau^{\ell}_{t}$ required for exiting at 0.9 across all layers. It is important to note that if this value were lowered, our model would exit earlier, hence producing faster but more inaccurate and less confident predictions. </p> <h3 id="contrastive-decoding">Contrastive Decoding</h3> <p>In this section, we analyze the behavior of the two implemented versions of Contrastive Decoding confidence measures, Weighted and Jensen-Shannon Divergence (JSD). The goal of this section is to illustrate the impact of CD on the performance and average early exit of the model.</p> <p>Results from <a href="#figure-9">Figure 9</a> show Weighted contrastive decoding achieving comparable average exit layer with Softmax baseline by (<a href="#confident-adaptive-language-modeling-2022">Schuster et al., 2022</a>), while still retaining almost all the performance. More interesting is the behaviour of JSD, which consistently beats the Softmax baseline. The method is exiting earlier with an average gain of 2.5 blocks, while also achieving higher performance with a 2\% increase over the no-exiting baseline (green dot).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/squadexit.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>(a)</b> Average exit block across different minimum exit layers</div> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/squadf1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>(b)</b> F1 score across different minimum exit layers</div> </div> </div> <div class="caption"> <a id="figure-9"></a><b>Figure 9:</b> SQuAD Average Exit and F1. Results are reported on t5-large non-finetuned model on SQuAD dataset. Due to time and compute constraints, the results displayed are computed on 100 samples. </div> <p>Evaluation on SamSum dataset, <a href="#figure-10">Figure 10</a>, shows notable results. While Weighted Contrastive Decoding is on par with the Softmax baseline, the JSD confidence measure is exiting earlier on average, with a 2.9 block gain against Softmax (red line). Additionally, JSD is attaining an approximate 10\% increase in Rouge-L performance if setting the minimum exit-layer to 17.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/sam_avg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>(a)</b> Average exit block across different minimum exit layers</div> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/samsum_intermediate.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>(b)</b> Rouge-L score across different minimum exit layers</div> </div> </div> <div class="caption"> <a id="figure-10"></a><b>Figure 10:</b> SamSum Average Exit and Rouge-L. Results are reported on t5-large non-finetuned model on SamSum dataset. Due to time and compute constraints, the results displayed are computed on 100 samples. </div> <h3 id="speedup-and-contrastive-decoding">Speedup and Contrastive Decoding</h3> <p>JSD has shown significant performance gains with respect to Softmax and Weighted Contrastive Decoding. In this chapter, we merge JSD with Softmax vocabulary pruning. We then compare the best Softmax vocabulary pruning with Contrastive Decoding against the previously analyzed individual baselines. We show that combining JSD technique with a compute efficient pruning mechanism positively impacts results. We will report the average exit block, the performance score, and the computed FLOPs.</p> <h4 id="best-jsd-pruning-combination">Best JSD Pruning Combination</h4> <p>We perform a series of experiments aimed at understanding the best possible vocabulary pruning method for the best CD confidence measure.</p> <p>Following the argument in <a href="#contrastive-decoding">‚ÄúContrastive Decoding‚Äù</a>, we observe that the model is most performant when the minimum exit layer is selected to be among the latest ones. Keeping this in mind, <a href="#table-1">Table 1</a> shows the average exit layer and score of the model. Both are averaged across these sensible minimum exit layers. We note that combining Adaptive Pruning with JSD beats the performance of JSD combined either with Fixed or Decaying pruning. It also obtains an average gain of 1.2 blocks against Fixed pruning on SamSum. However, JSD+Fixed achieves the highest Rouge-L score in SamSum. Given the considerations above, we choose Adaptive to be the most fitting pruning method to combine with the JSD confidence measure. We defer to Appendix A a series of detailed plots indicating all minimum exit layers in this setting.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/early_exit/table1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> <b id="table-1">Table 1:</b> JSD with pruning. Comparison across different pruning methodologies applied to Jensen-Shannon Divergence (JSD) confidence measure. The values in the table are the mean of exit/performance over 15, 17, 18, 19, 20 as early minimum exit layer ¬± their standard deviation. Due to time and compute constraints, the results displayed are computed on 100 samples. </div> <p>Comparison Across different pruning methodologies applied to Jensen-Shannon Divergence (JSD) confidence measure. The values in the table are the mean of exit/performance over 15, 17, 18, 19, 20 as early minimum exit layer ¬± their standard deviation. Due to time and compute constraints, the results displayed are computed on 100 samples.</p> <h4 id="comparison-with-baseline-models">Comparison with Baseline Models</h4> <p>Given the results of <a href="#speedup-and-contrastive-decoding">‚ÄúSpeedup and Contrastive Decoding‚Äù</a>, together with our analysis of the best minimum exit layer to use in CD, we now compare the most performing pruning method of <a href="#best-jsd-pruning-combination">‚ÄúBest JSD Pruning Combination‚Äù</a> with the baselines from <a href="#contrastive-decoding">‚ÄúContrastive Decoding‚Äù</a> and <a href="#softmax-speedup">‚ÄúSoftmax Speed-Up‚Äù</a>. We set the minimum exit layer at 19 for all the experiments below.</p> <p>In <a href="#softmax-speed-up">‚ÄúSoftmax Speed-Up‚Äù</a> we show the considerable impact the pruning approach has on FLOPs. Similarly, Figures <a href="#figure-10">10</a> and <a href="#figure-11">11</a> show that removing a large number of tokens has a notable effect on compute, reducing it by almost 100 times on SQuAD and 10 on SamSum between JSD baseline and JSD with adaptive pruning. This gap is also more noteworthy when looking at the amount of performance retained. On both fine-tuned and non-finetuned models the decrease in performance between the downstream tasks is never more than 1.5%, with JSD. Lastly, we highlight the difference in results between Figure <a href="#figure-7">7</a>, <a href="#figure-8">8</a>, and Figure <a href="#figure-10">10</a>, <a href="#figure-11">11</a>, due to a higher minimum exit layer selected for the former experiments. However, in both cases, our results are consistent both in trend of terms and performance and FLOPs reduction.</p> <p>In conclusion, combining a vocabulary reduction approach, together with a confidence measure method, allows us to compute considerably fewer FLOPs, while retaining the performance with respect to Softmax and JSD baselines.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/squad_f1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>F1 Metric</b> for T5-Large and T5-Finetuned with <i>No</i>, <i>Fixed</i>, or <i>Decaying</i> reduction applied to the matrix</div> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/squad_flops.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>FLOPs per sample during confidence estimation</b> for T5-Large and T5-Finetuned with <i>No</i>, <i>Fixed</i>, or <i>Decaying</i> reduction applied to the matrix</div> </div> </div> <div class="caption"> <a id="figure-10"></a><b>Figure 10:</b> <b>Performance on Question-Answering Task</b>: Comparison of model performance in terms of F1 score and the amount of FLOPs generated per sample. The minimum exit layer was set to 19 for both T5-Large and T5-Large Finetuned, with the confidence set at 0.9 for both. </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/rougesamsam.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>F1 Metric</b> for T5-Large and T5-Finetuned with <i>No</i>, <i>Fixed</i>, or <i>Decaying</i> reduction applied to the matrix</div> </div> <div class="col-sm mt-3 mt-md-0 text-center"> <figure> <picture> <img src="/assets/img/early_exit/sam_flops.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <div class="caption"><b>FLOPs per sample during confidence estimation</b> for T5-Large and T5-Finetuned with <i>No</i>, <i>Fixed</i>, or <i>Decaying</i> reduction applied to the matrix</div> </div> </div> <div class="caption"> <a id="figure-11"></a><b>Figure 11:</b> Performance on Summarization Task: Comparison of model performance in terms of ROUGE-L score and the amount of FLOPs generated per sample. The minimum exit layer was set to 19 for both T5-Large and T5-Large Finetuned, with the confidence set at 0.9 for both. </div> <h2 id="conclusion-and-future-work">Conclusion and Future Work</h2> <p>In this study, we have explored the application of early exiting strategies within Large Language Models (LLMs) to address the challenge of computational efficiency during inference. Our research integrates traditional early exiting mechanisms with concrete gains in efficiency obtained from vocabulary pruning. Additionally, we apply the idea of Contrastive Decoding to the early exiting setting, showing how this approach can be used as a confidence measure, by imposing a clever heuristic on the choice of the layer to contrast to. Lastly, we combine the aforementioned techniques and demonstrate that we can both retain almost all performance, while still carrying out a considerably lower number of FLOPs during inference. This results in a solution that satisfies both the efficiency and score performances we aimed for.</p> <p>In future work, a natural follow-up is the use of the Contrastive Decoding output as the resulting output to perform the prediction on. Moreover, sensible investigations about the distributional distance and specific interventions on the computation of the contrastive distribution can be considered.</p> <p>On an empirical note, we aim to expand our analysis to include a wider array of tasks - machine translation, open-ended generation, and long context tasks - and evaluation on larger datasets to further validate our proposal. Another limitation is that the overall runtime performance does not always match the improvements seen in FLOPs. This discrepancy is largely due to the hyper-optimization of the PyTorch library, which optimizes matrix multiplications, thereby reducing overall runtime, though it is worth noting that our gains in FLOPs should increase as a function of model scale. Additionally, since we are working with pre-trained tokenizers, reducing the $W_j$ matrix leads to incorrect predictions, necessitating a remapping back to the original vocabulary size. This process introduces an overhead that further worsens runtime, as we are forced to apply the same operation twice (reducing first and then expanding again). Several engineering-reliant optimizations are still possible in this direction, which were not explored due to the previously mentioned constraints. With regards to vocabulary reduction, the function that shrinks the $k$ values is based on the worst-case scenario observed in the data (see <a href="#figure-3">Figure 3</a>). This function could be adjusted depending on both the problem type and the LM employed. For instance, a finetuned model as depicted in <a href="#figure-2b">Figure 2b</a> might benefit from more aggressive shrinkage compared to its non-finetuned counterpart. Additionally, we plan on further refining the integration of the vocabulary pruning method with Contrastive Decoding. We hypothesize that, by leveraging the list of top-k tokens within Contrastive Decoding, we can get rid of the plausibility constraint, overall reducing further reliance on hyperparameter settings and tuning. All these reasons inspire us to further work in this promising direction, and we hope the same applies to the reader.</p> <h3 id="citation">Citation</h3> <p>If you use this work, please cite:</p> <pre><code class="language-bibtex">@misc{vincenti2024dynamicvocabularypruningearlyexit,
      title={Dynamic Vocabulary Pruning in Early-Exit LLMs}, 
      author={Jort Vincenti and Karim Abdel Sadek and Joan Velja and Matteo Nulli and Metod Jazbec},
      year={2024},
      eprint={2410.18952},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.18952}, 
}
@misc{abdelsadek2024optimizing,
  title={Optimizing Predictions: Vocabulary Reduction and Contrastive Decoding in LLMs},
  author={K. A. Abdel Sadek and M. Nulli and J. Velja and J. Vincenti and G. Desimini},
  year={2024},
  url={https://matteonulli.github.io/blog/2025/earlyexit/},
}
</code></pre> <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['\\[', '\\]']]
    },
    svg: {
      fontCache: 'global'
    }
  };
</script> <script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"> </script> <p><strong>References</strong></p> <div id="references-section"> <a id="fast-robust-early-exiting-2023">Bae Sangmin, Ko Jongwoo, Yun Se-Young. (2023). Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding. arXiv preprint arXiv:2310.05424.</a> <a id="linguistic-calibration-2024">Band Neil, Li Xuechen, Ma Tengyu, Hashimoto Tatsunori. (2024). Linguistic Calibration of Language Models. arXiv preprint arXiv:2404.00474.</a> <a id="language-models-few-shot-2020">Brown Tom B, Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan Jared, Dhariwal Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry Girish, Askell Amanda, Agarwal Sandhini, Herbert-Voss Ariel, Krueger Gretchen, Henighan Tom, Child Rewon, Ramesh Aditya, Ziegler Daniel M, Wu Jeffrey, Winter Clemens, Hesse Christopher, Chen Mark, Sigler Eric, Litwin Mateusz, Gray Scott, Chess Benjamin, Clark Jack, Berner Christopher, McCandlish Sam, Radford Alec, Sutskever Ilya, Amodei Dario. (2020). Language Models are Few-Shot Learners. arXiv preprint arXiv:2005.14165.</a> <a id="palm-scaling-2023">Chowdhery Aakanksha, Narang Sharan, Devlin Jacob, Bosma Maarten, Mishra Gaurav, Roberts Adam, Barham Paul, Chung Hyung Won, Sutton Charles, Gehrmann Sebastian, and others. (2023). Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240), 1-113.</a> <a id="dola-contrasting-layers-2024">Chuang Yung-Sung, Xie Yujia, Luo Hongyin, Kim Yoon, Glass James, He Pengcheng. (2024). DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. arXiv preprint arXiv:2309.03883.</a> <a id="bert-pre-training-2019">Devlin Jacob, Chang Ming-Wei, Lee Kenton, Toutanova Kristina. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers) (pp. 4171-4186). Minneapolis, Minnesota: Association for Computational Linguistics.</a> <a id="multi-news-2019">Fabbri Alexander R, Li Irene, She Tianwei, Li Suyi, Radev Dragomir R. (2019). Multi-news: A large-scale multi-document summarization dataset and abstractive hierarchical model. arXiv preprint arXiv:1906.01749.</a> <a id="transformer-key-value-memories-2021">Geva Mor, Schuster Roei, Berant Jonathan, Levy Omer. (2021). Transformer Feed-Forward Layers Are Key-Value Memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (pp. 5484-5495). Online and Punta Cana, Dominican Republic: Association for Computational Linguistics.</a> <a id="transformer-promoting-concepts-2022">Geva Mor, Caciularu Avi, Wang Kevin, Goldberg Yoav. (2022). Transformer Feed-Forward Layers Build Predictions by Promoting Concepts in the Vocabulary Space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing (pp. 30-45). Abu Dhabi, United Arab Emirates: Association for Computational Linguistics.</a> <a id="samsum-corpus-2019">Gliwa Bogdan, Mochol Iwona, Biesek Maciej, Wawer Aleksander. (2019). SAMSum corpus: A human-annotated dialogue dataset for abstractive summarization. arXiv preprint arXiv:1911.12237.</a> <a id="training-compute-optimal-2022">Hoffmann Jordan, Borgeaud Sebastian, Mensch Arthur, Buchatskaya Elena, Cai Trevor, Rutherford Eliza, Casas Diego de Las, Hendricks Lisa Anne, Welbl Johannes, Clark Aidan, and others. (2022). Training compute-optimal large language models. arXiv preprint arXiv:2203.15556.</a> <a id="class-exclusion-2024">Jingcun Wang, Bring Li, Grace Li Zhang. (2024). Early-Exit with Class Exclusion for Efficient Inference of Neural Networks.</a> <a id="scaling-laws-2020">Kaplan Jared, McCandlish Sam, Henighan Tom, Brown Tom B, Chess Benjamin, Child Rewon, Gray Scott, Radford Alec, Wu Jeffrey, Amodei Dario. (2020). Scaling laws for neural language models. arXiv preprint arXiv:2001.08361.</a> <a id="contrastive-decoding-2023">Li Xiang Lisa, Holtzman Ari, Fried Daniel, Liang Percy, Eisner Jason, Hashimoto Tatsunori, Zettlemoyer Luke, Lewis Mike. (2023). Contrastive Decoding: Open-ended Text Generation as Optimization. arXiv preprint arXiv:2210.15097.</a> <a id="faster-depth-adaptive-transformers-2021">Liu Yijin, Meng Fandong, Zhou Jie, Chen Yufeng, Xu Jinan. (2021). Faster depth-adaptive transformers. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 15, pp. 13424-13432).</a> <a id="reducing-overconfidence-2022">Mielke Sabrina J, Szlam Arthur, Dinan Emily, Boureau Y-Lan. (2022). Reducing conversational agents' overconfidence through linguistic calibration. arXiv preprint arXiv:2012.14983.</a> <a id="contrastive-decoding-improves-reasoning-2023">O'Brien Sean, Lewis Mike. (2023). Contrastive decoding improves reasoning in large language models. arXiv preprint arXiv:2309.09117.</a> <a id="scaling-language-models-2021">Rae Jack W, Borgeaud Sebastian, Cai Trevor, Millican Katie, Hoffmann Jordan, Song Francis, Aslanides John, Henderson Sarah, Ring Roman, Young Susannah, and others. (2021). Scaling language models: Methods, analysis &amp; insights from training gopher. arXiv preprint arXiv:2112.11446.</a> <a id="language-models-unsupervised-multitask-2019">Radford Alec, Wu Jeffrey, Child Rewon, Luan David, Amodei Dario, Sutskever Ilya. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8), 9.</a> <a id="squad-2016">Rajpurkar Pranav, Zhang Jian, Lopyrev Konstantin, Liang Percy. (2016). Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.</a> <a id="exploring-limits-2020">Raffel Colin, Shazeer Noam, Roberts Adam, Lee Katherine, Narang Sharan, Matena Michael, Zhou Yanqi, Li Wei, Liu Peter J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140), 1-67.</a> <a id="confident-adaptive-transformers-2021">Schuster Tal, Fisch Adam, Jaakkola Tommi, Barzilay Regina. (2021). Consistent accelerated inference via confident adaptive transformers. arXiv preprint arXiv:2104.08803.</a> <a id="confident-adaptive-language-modeling-2022">Schuster Tal, Fisch Adam, Gupta Jai, Dehghani Mostafa, Bahri Dara, Tran Vinh Q, Tay Yi, Metzler Donald. (2022). Confident Adaptive Language Modeling. arXiv preprint arXiv:2207.07061.</a> <a id="right-tool-2020">Schwartz Roy, Stanovsky Gabriel, Swayamdipta Swabha, Dodge Jesse, Smith Noah A. (2020). The right tool for the job: Matching model and instance complexities. arXiv preprint arXiv:2004.07453.</a> <a id="get-to-the-point-2017">See Abigail, Liu Peter J, Manning Christopher D. (2017). Get to the point: Summarization with pointer-generator networks. arXiv preprint arXiv:1704.04368.</a> <a id="model-depth-analysis-2021">Simoulin Antoine, Crabb√© Benoit. (2021). How many layers and why? An analysis of the model depth in transformers. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop (pp. 221-228).</a> <a id="using-deepspeed-megatron-2022">Smith Shaden, Patwary Mostofa, Norick Brandon, LeGresley Patrick, Rajbhandari Samyam, Casper Jared, Liu Zhun, Prabhumoye Shrimai, Zerveas George, Korthikanti Vijay, and others. (2022). Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model. arXiv preprint arXiv:2201.11990.</a> <a id="branchynet-2016">Teerapittayanon Surat, McDanel Bradley, Kung Hsiang-Tsung. (2016). Branchynet: Fast inference via early exiting from deep neural networks. In 2016 23rd international conference on pattern recognition (ICPR) (pp. 2464-2469). IEEE.</a> <a id="llama2-open-foundation-2023">Touvron Hugo, Martin Louis, Stone Kevin, Albert Peter, Almahairi Amjad, Babaei Yasmine, Bashlykov Nikolay, Batra Soumya, Bhargava Prajjwal, Bhosale Shruti, Bikel Dan, Blecher Lukas, Ferrer Cristian Canton, Chen Moya, Cucurull Guillem, Esiobu David, Fernandes Jude, and others. (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models. arXiv preprint arXiv:2307.09288.</a> <a id="accelerating-llm-inference-2023">Varshney Neeraj, Chatterjee Agneet, Parmar Mihir, Baral Chitta. (2023). Accelerating llm inference by enabling intermediate layer decoding. arXiv preprint arXiv:2310.18581.</a> <a id="attention-is-all-you-need-2017">Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob, Jones Llion, Gomez Aidan N, Kaiser ≈Åukasz, Polosukhin Illia. (2017). Attention is all you need. Advances in neural information processing systems.</a> <a id="leebert-2021">Wei Zhu. (2021). LeeBERT: Learned early exit for BERT with cross-level optimization.</a> </div>]]></content><author><name></name></author><category term="Inference-Optimisation"/><category term="Efficiency"/><category term="LLMs"/><category term="Early-Exiting"/><category term="Contrastive-Decoding"/><summary type="html"><![CDATA[Efficiency-focused early exiting, vocabulary pruning, and contrastive decoding for LLM inference]]></summary></entry><entry><title type="html">Model Compression for Machine Translation in Large Language Models</title><link href="https://matteonulli.github.io/blog/2024/alma/" rel="alternate" type="text/html" title="Model Compression for Machine Translation in Large Language Models"/><published>2024-11-05T15:24:00+01:00</published><updated>2024-11-05T15:24:00+01:00</updated><id>https://matteonulli.github.io/blog/2024/alma</id><content type="html" xml:base="https://matteonulli.github.io/blog/2024/alma/"><![CDATA[<h5 id="m-nulli-j-vincenti-a-tragoudaras-z-tzifa-kratira-and-a-b-gomez"><b>M. Nulli</b>, J. Vincenti, A. Tragoudaras, Z. Tzifa-Kratira, and A. B. Gomez</h5> <h6 id="university-of-amsterdam">University of Amsterdam</h6> <h6 id="-"><img src="https://upload.wikimedia.org/wikipedia/commons/1/17/Uva%C2%AEmerken_ENG.png" alt="eBay" height="24"/> ¬†</h6> <h6 id="-paper---blogpost---code">üìÑ <a href="https://www.overleaf.com/read/dkkcbhpycncf#bec4c7">Paper</a> | üìù <a href="https://matteonulli.github.io/blog/2024/alma/">Blogpost</a> | üßë‚Äçüíª <a href="https://github.com/JortVincenti/ALMA">Code</a></h6> <p><br/></p> <h2 id="motivation">Motivation</h2> <p>Small traditional machine translation models can produce high-quality translations in a one-to-one language setting [<a href="https://arxiv.org/pdf/2207.04672">1</a>]. However, when scaling these models to handle multiple languages, they often struggle to perform well due to the limited amount of parallel data and the complex architectures required to fully understand the task. In contrast, Large Language Models (LLMs) [<a href="https://arxiv.org/pdf/2005.14165">2</a>] can handle complex settings due to their large architectures and overcome smaller datasets by leveraging their large English training data. As a result, ALMA is a fine-tuned LLM designed for machine translation across multiple language directions from and to English and is the first LLM to become competitive with traditional machine translation models [<a href="https://arxiv.org/pdf/2309.11674">3</a>].</p> <p>Despite performing remarkably well in translation, ALMA incurs significant computational and memory costs, making even inference prohibitively expensive to scale to consumer hardwar [<a href="https://arxiv.org/pdf/2308.07633">4</a>]. To address these challenges, several compression techniques have been proposed for LLMs. Among many others, these include quantization [<a href="https://scholar.google.com/scholar_url?url=https://ieeexplore.ieee.org/abstract/document/720541/&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=4619659333798904151&amp;ei=4Pn9aLLLGNyOieoP1t-zoQM&amp;scisig=ABGrvjJbo0ZiWOaWn3QOaSjwLpbc">5</a>, <a href="https://scholar.google.com/scholar_url?url=http://proceedings.mlr.press/v202/xiao23c.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=17814731842610185667&amp;ei=-vn9aImlILmAieoPlr2tqAg&amp;scisig=ABGrvjIl4KLQN2x_Kith27oHWlKF">6</a>] and pruning [<a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper/1989/hash/6c9882bbac1c7093bd25041881277658-Abstract.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=4588992562541514941&amp;ei=Efr9aNeCM_rUieoPvMbB8Qc&amp;scisig=ABGrvjKlhw7h_hc2pm1eDaB_57Jp">7</a>, <a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper/2015/hash/ae0eb3eed39d2bcef4622b2499a05fe6-Abstract.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=6338930303179684776&amp;ei=I_r9aIX5F77WieoPn4GZgQE&amp;scisig=ABGrvjJDL40cEEu6_3RiqtrpmeNY">8</a>, <a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/1510.00149&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=7860777411990654691&amp;ei=Mfr9aKfnEv2lieoP1Jf4uAw&amp;scisig=ABGrvjKoCGzYeTPmCdCI6xO9Hgxo">9</a>] methods. However, none of these techniques have been applied to ALMA and evaluated in terms of translation quality.</p> <p>In this work, we explore applying compression techniques to ALMA to preserve its translation quality. We experiment with five distinct compression methods: GPTQ [<a href="https://scholar.google.com/scholar_url?url=https://proceedings.mlr.press/v202/frantar23a&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=2009267211619482754&amp;ei=Rfr9aKK0DYGpieoPyfaRyA4&amp;scisig=ABGrvjJl-us59RdHA3e_MXFWxM8X">10</a>], Q-LoRA , [<a href="https://scholar.google.com/scholar_url?url=https://proceedings.neurips.cc/paper_files/paper/2023/hash/1feb87871436031bdc0f2beaa62a049b-Abstract-Conference.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=8079217396100949644&amp;ei=gvr9aPPOK_2lieoP1Jf4uAw&amp;scisig=ABGrvjIqYBQ-eh1JEPCyzNEGJAtt">11</a>], SmoothQuant [<a href="https://scholar.google.com/scholar_url?url=http://proceedings.mlr.press/v202/xiao23c.html&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=17814731842610185667&amp;ei=jfr9aLrzIbmAieoPlr2tqAg&amp;scisig=ABGrvjIl4KLQN2x_Kith27oHWlKF">12</a>], Wanda [<a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2306.11695&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=8181806601256870843&amp;ei=p_r9aLD8JNrZzwKX7-SYDw&amp;scisig=ABGrvjJYsdHWsU9e_-K7BUqRU9W8">13</a>], and DSnot [<a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2310.08915&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=6652760928495592428&amp;ei=v_r9aOaYCe2ZieoP7ueJyQk&amp;scisig=ABGrvjLWrVul2-STCvxcctLhFb5d">14</a>]. To evaluate the effectiveness of these techniques, we assess their translation quality, memory usage, and inference time, providing a comprehensive analysis of the trade-offs involved in compressing LLMs fine-tuned for translation.</p> <h2 id="results">Results</h2> <p>Table 1 presents the results for the five different compression techniques. A noticeable performance gap can be observed between the <code>en‚Üíxx</code> and <code>xx‚Üíen</code> translation directions. As noted in [<a href="https://arxiv.org/pdf/2309.11674">3</a>], this disparity is likely due to the large amount of English data used during the training of LLAMA 2 [<a href="https://scholar.google.com/scholar_url?url=https://arxiv.org/abs/2307.09288&amp;hl=en&amp;sa=T&amp;oi=gsr-r&amp;ct=res&amp;cd=0&amp;d=10722545090167785628&amp;ei=Jfv9aPKCF-2ZieoP7ueJyQk&amp;scisig=ABGrvjIKbmSm-weoKpHx13AzXNT6">15</a>], resulting in a bias towards better performance when translating into English.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <a id="tab-main-res"></a> <figure> <picture> <img src="/assets/img/alma-table.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Table 1: Summary of calculated metrics across all analyzed model compression techniques. All results are calculated on ALMA-7B and the Method column highlights the method used along with a precision/sparsity level (NA= Not Applicable, NF4=4-bit NormalFloat). We report BLEU (‚Üë) and COMET (‚Üë) scores for <code>en ‚Üî x</code>. Additionally, we cover the average time per token in seconds (‚Üì) and the memory allocation in megabytes (MB) (‚Üì) after each compression technique is applied. </div> <h4 id="quantization">Quantization</h4> <p>All quantization methods preserved strong BLEU and COMET performance‚Äîonly slightly below the baseline‚Äîwhile reducing memory use by about 70%. This stability likely stems from ALMA‚Äôs already low BLEU score and highly variable multilingual weights, which allow better generalization after quantization. However, 4-bit quantization increased inference time, likely because GPU operations still rely on FP16 multiplications, which are more efficient on current hardware despite the smaller memory footprint of integer formats.</p> <p>The GPTQ study in <a href="#figure1">Figure 1</a> found that 4-bit quantization offers the best balance between performance and memory efficiency, while 2-bit causes major degradation due to limited representational capacity. However, GPTQ also led to increased inference time, likely because the fused kernel optimization from the original paper was not implemented (<a href="#tab-main-res">2nd Row Table 1</a>).</p> <p>The Q-LoRA-like quantization method showed minimal translation performance loss and faster inference than GPTQ (<a href="#tab-main-res">3rd Row Table 1</a>), thanks to its efficient block-wise floating-point quantization and de-quantization to FP16.</p> <p>Finally, combining SmoothQuant with Q-LoRA by smoothing activations and applying block-wise weight quantization‚Äîsurprisingly improved performance beyond the baseline ALMA-7B (<a href="#tab-main-res">4th Row Table 1</a>), suggesting that quantization can also act as a noise-suppressing mechanism that enhances translation quality.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure1"></a> <figure> <picture> <img src="/assets/img/alma-figure1.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1: Top grah shows average BLEU Score for both directions of <code>en ‚Üî x</code> across different GPTQ bitquantization levels. The bottom bar chart represents the memory allocation across different precision values. Both results are based on ALMA-7B. </div> <h4 id="pruning">Pruning</h4> <p>Wanda pruning achieved lower BLEU scores but offered faster inference and reduced memory use compared to the full ALMA-7B model (<a href="#tab-main-res">5th Row Table 1</a>). However, its memory savings were less substantial than quantization, since pruning zeros out weights rather than compressing them. The resulting sparsity speeds up computation but does not reduce the number of operations.</p> <p>Wanda + DSnoT improved perplexity (predictive accuracy) but had negligible gains in BLEU and COMET, indicating that it enhances word-level prediction rather than overall translation quality. Thus, DSnoT appears better suited for general LLM tasks and was excluded from the main results.</p> <p>Wanda + GPTQ, inspired by prior LLaMA experiments, combined pruning (50% sparsity) with 4-bit quantization, yielding slightly better performance and memory usage comparable to pure quantization, though with longer inference times (<a href="#tab-main-res">6th Row Table 1</a>). The method‚Äôs suitability ultimately depends on the hardware and performance trade-offs of the translation setup.</p> <h4 id="effects-of-calibration">Effects of Calibration</h4> <p>In <a href="#figure2">Figure 2</a> we report our expeirments on calibration data. These showed minimal performance differences across varying sample sizes, suggesting that translation models depend mainly on pattern recognition and word mappings, which smaller datasets can capture effectively. Since calibration is a one-time post-training step, using the best-performing setup (512 samples per language) is acceptable despite higher latency. However, as model size increases, calibration becomes computationally expensive, highlighting the advantage of methods like Q-LoRA, which bypass the calibration step entirely.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0 text-center"> <a id="figure2"></a> <figure> <picture> <img src="/assets/img/alma-figure2.png" class="img-fluid rounded z-depth-1 w-50" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 2: The top and bottom graphs display the BLEU and COMET scores, respectively, for both translation directions <code>en ‚Üî x</code> across varying calibration sizes and different compression methods. All results are based on the ALMA-7B model. </div> <h2 id="citation">Citation</h2> <p>If you use this work, please cite:</p> <pre><code class="language-bibtex">@misc{nulli2024modelcompression,
  author = {Nulli, M. and Vincenti, J. and Tragoudaras, A. and Tzifa-Kratira, Z. and Gomez, A.},
  title  = {Model Compression for Machine Translation in Large Language Models},
  url    = {https://matteonulli.github.io/blog/2024/alma/},
  year   = {2024}
}
</code></pre>]]></content><author><name></name></author><category term="Inference-Optimisation"/><category term="LLMs"/><summary type="html"><![CDATA[Model Compression for Machine Translation in Large Language Models]]></summary></entry></feed>